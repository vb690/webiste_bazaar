{
  "hash": "9828e973623f611e5a92a2ed758530e7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Analyzing A/B Test Data using Gaussian Processes\ndescription: 'This post illustrates how to analyze longitudinal A/B test data using Gaussian Process.'\ndate: '2025-03-29'\ncategories:\n  - JAX\n  - Numpyro\n  - Bayesian Statistics\n  - A/B Test\n  - Gaussian Process\nbibliography: bibliographies/ab_test_gp.bib\njupyter: python3\n---\n\n::: {#36de3278 .cell vscode='{\"languageId\":\"python\"}' execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\n%load_ext watermark\n\nNUM_CHAINS = 4\n\nfrom typing import Dict, Any, Callable, Tuple\n\nfrom functools import partial\n\nimport pandas as pd\nimport numpy as np\n\nimport scipy\nfrom scipy.stats import lognorm, median_abs_deviation\n\nimport jax\nfrom jax.typing import ArrayLike\nimport jax.random as random\nfrom jax import jit\nfrom jax import numpy as jnp\nfrom jax import vmap\n\nimport numpyro\nnumpyro.set_host_device_count(NUM_CHAINS)\nimport numpyro.distributions as dist\nfrom numpyro.infer import (\n    MCMC,\n    NUTS,\n    Predictive\n)\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.figure import Figure\nfrom matplotlib import gridspec\n\nimport seaborn as sns\n\ndef plot_univariate_series(\n        series_data: Dict[Any, Any], \n        ax: plt.Axes, \n        **plot_kwargs: Any\n    ) -> plt.Axes:\n    ax.scatter(\n        series_data[\"x\"][\"data\"],\n        series_data[\"y\"][\"data\"],\n        **plot_kwargs\n    )\n    ax.plot(\n        series_data[\"x\"][\"data\"],\n        series_data[\"y\"][\"data\"],\n        **plot_kwargs\n    )\n    ax.set_xlabel(series_data[\"x\"][\"label\"])\n    ax.set_ylabel(series_data[\"y\"][\"label\"])\n\n    ax.tick_params(\n        direction=\"in\",\n        top=True, axis=\"x\",\n        rotation=45,\n    )\n    ax.grid(\n        visible=True,\n        which=\"major\",\n        axis=\"x\",\n        color=\"k\",\n        alpha=0.25,\n        linestyle=\"--\",\n    )\n    return ax\n```\n:::\n\n\n# Premise\n\nWhat will be illustrated in this post is strongly in spired by the content of the books \"Statistical Rethinking\" [@mcelreath2018statistical] and \"Bayesian Data Analysis 3rd Edition\" [@gelman1995bayesian]. In particular, the idea to separately code and illustrate the behavior of different covariance kernel functions comes from the amazing [\"Kernel Cookbook\"](https://www.cs.toronto.edu/~duvenaud/cookbook/) and PhD thesis of David Duvenaud [@duvenaud2014automatic].\n\nThis post assumes some level of knowledge in bayesian statistics and probabilistic programming.\n\n## What we will cover\n\n1. Very brief illustration of longitudinal A/B test within and observational paradigm.\n2. Very brief illustration of gaussian processes and their application to analyzing A/B test data.\n3. Overview of how to implement a gaussian process model using [Numpyro](https://num.pyro.ai/en/stable/index.html) and [JAX](https://docs.jax.dev/en/latest/index.html).\n4. Simulating A/B test data.\n5. Analyzing A/B test data within an modelling setting.\n\n## What we will **not** cover\n\n1. Detailed coverage of A/B test (e.g., sampling, randomization etc...).\n2. Hypothesis testing (we will focus on modelling).\n3. Fundamentals of bayesian statistics.\n4. Detailed overview of Gaussian processes.\n5. Probabilistic programming and sampling algorithms.\n\n# Introduction\n\n## Longitudinal A/B tests in observational settings\n\nWhen we talk about A/B test we usually refer to a research method used for evaluating if a given intervention is having an impact on a pre-defined outcome variable measured inside a sample. For doing so, we can draw two distinct samples (the A and B group) from a population of interest, subject one of the two to the intervention and then measure observed differences in the outcome variable. Subject to **several** assumptions and pre-conditions (we suggest reading part V of \"Regression and Other Stories\" [@gelman2021regression]), if we observe a difference between the two groups when can conclude that our intervention might have had an impact on our outcome variable.\n\n## Gaussian Process\n\nThe Gaussian Process $GP$ can be thought as the continuous generalization of basis function regression (see Chapter 20 of [@gelman1995bayesian]). We can think of it as a stochastic process where any point drawn from it, $x_1, \\dots, x_n$, comes from a multi-dimensional gaussian. In other words, it is as a prior distribution over an un-known function $\\mu(x)$ defined as\n\n$$\n\\mu(x_1), \\dots, \\mu(x_n) \\sim \\mathcal{N}((m(x_1), \\dots, m(x_n)), k(x, \\dots, x_n))\n$$\n\nor more compactly\n\n$$\n\\mu(x) \\sim GP(m, k)\n$$\n\nwhere $m$ is a mean function and $k$ is a covariance function. We can already have an intuition of how defining the $GP$ in terms mean and covariance **functions** gives us quite some flexibility as it allows us to produce a model than can interpolate for all the value of $x$.\n\nThe $m$ function provides the most likely guess for the $GP$ like the mean vector of a multi-dimensional Gaussian, deviation from this expected model are then handled by the covariance function $k$.\n\nThe $k$ function (often called Kernel) allows to structurally define the $GP$ behavior at any two points by producing an $n \\times n$ covariance function given by evaluating $k(x, x')$ for every $x_1, \\dots, x_n$.\n\nOne convenient property of $GP$ is that the sum and multiplication of two or more $GP$ is itself a $GP$, this allows to combine different types of Kernels for imposing specific structural constrains.\n\n## Gaussian Process for A/B test\n\nAlthough there aren't many papers illustrating how $GP$ can be used for analyzing A/B test data we found this interesting work by [@benavoli2015gaussian] from [IDSIA](https://www.idsia.usi-supsi.ch/) that we decided to adapt to our use-case.\n\n\n\n# Implementing a Gaussian Process Model in Numpyro\n\nIn this section we will illustrate how we can implement a $GP$ model using Numpyro. Numpyro offers a Numpy-like Backend for [Pyro](https://pyro.ai/) a Probabilistic Programming Language (PPL). Other than offering the flexibility of specifying models using the familiar Numpy interface, Numpyro is perfectly integrated with JAX allowing us to tap into its JIT compilation capabilities.\n\n::: {#624e8c8e .cell execution_count=2}\n``` {.python .cell-code}\nBLUE = (0, 83, 159)\nBLUE = tuple(value / 255. for value in BLUE)\n\nRED = (238, 28, 46)\nRED = tuple(value / 255. for value in RED)\n\nTIME_IDX = np.arange(7*8)\nDATES = pd.date_range(\n    start=\"01-01-2023\",\n    periods=len(TIME_IDX)\n).values\nTIME = \"date\"\nMODELS = [\n    \"Branch A\",\n    \"Branch B\",\n    \"Branch C\",\n    \"Branch D\",\n    \"Branch E\",\n]\n```\n:::\n\n\n## Kernel Functions\n\n::: {#b815bf85 .cell vscode='{\"languageId\":\"python\"}' execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\ndef simulate_kernel(\n    X: ArrayLike, \n    X_prime:ArrayLike, \n    kernel_function: Callable, \n    samples: int,\n    mean: float = 0.\n) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:\n    generated_covariance = kernel_function(\n        X=X,\n        X_prime=X_prime,\n    )\n    distance = generated_covariance[:, len(X) // 2]\n    sampled_functions = np.random.multivariate_normal(\n        mean=np.zeros(\n            shape=generated_covariance.shape[0]\n        ) + mean,\n        cov=generated_covariance,\n        size=samples\n    )\n    return generated_covariance, distance, sampled_functions\n\ndef visualize_kernel(\n    X: ArrayLike, \n    generated_covariance: ArrayLike, \n    distance: ArrayLike, \n    sampled_functions: ArrayLike, \n    kernel_name: str\n) -> Figure:\n\n    fig = plt.figure(\n        figsize=(8, 8),\n        tight_layout=True\n    )\n    grid = gridspec.GridSpec(\n        nrows=2,\n        ncols=2\n    )\n    ax_functions = fig.add_subplot(grid[0, :])\n    ax_distance = fig.add_subplot(grid[1, 0])\n    ax_covariance = fig.add_subplot(grid[1, 1])\n\n    for index in range(sampled_functions.shape[0]):\n\n        ax_functions = plot_univariate_series(\n            series_data={\n                \"x\": {\n                    \"data\": X,\n                    \"label\": \"x\"\n                },\n                \"y\": {\n                    \"data\": sampled_functions[index, :],\n                    \"label\": \"y\"\n                },\n\n            },\n            ax=ax_functions,\n            alpha=0.5\n        )\n\n    ax_functions.set_title(\"Sampled Functions \\n from MvNormal\")\n    ax_functions.axhline(0, linestyle=\"--\", c=\"k\")\n\n    ax_distance.plot(\n        X - (len(X) // 2),\n        distance\n    )\n    ax_distance.grid(alpha=0.5)\n    ax_distance.set_title(f\"Distance Function \\n Determined by {kernel_name} Kernel\")\n    ax_covariance.set_ylabel(\"Similarity\")\n    ax_covariance.set_xlabel(\"Distance\")\n\n    ax_covariance.imshow(\n        generated_covariance\n    )\n    ax_covariance.set_ylabel(\"x'\")\n    ax_covariance.set_xlabel(\"x\")\n    ax_covariance.set_title(f\"Covariance \\n Determined by {kernel_name} Kernel\")\n\n\n    plt.suptitle(f\"{kernel_name} Kernel\")\n    return fig\n```\n:::\n\n\n### Radial Basis Function Kernel\n\n::: {#9a61bdb9 .cell execution_count=4}\n``` {.python .cell-code}\n@jit\ndef RBF_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    variance: float, \n    length: float, \n    noise: float,\n    jitter: float =1.0e-6, \n    include_noise: bool =True\n) -> ArrayLike:\n    squared_differences = jnp.power(\n        (X[:, None] - X_prime),\n        2.0\n    )\n    squared_length_scale = 2 * jnp.power(\n        length,\n        2.0\n    )\n    covariance_matrix = jnp.exp(\n        - (squared_differences / squared_length_scale)\n    )\n    scaled_covariance_matrix = variance * covariance_matrix\n\n    if include_noise:\n        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])\n\n    return scaled_covariance_matrix\n```\n:::\n\n\n::: {#0439077b .cell execution_count=5}\n``` {.python .cell-code}\nkernel_function = partial(\n    RBF_kernel,\n    variance=1,\n    length=10,\n    noise=0.001\n)\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"RBF\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\n### Matern Kernel\n\n::: {#f8f22542 .cell execution_count=6}\n``` {.python .cell-code}\n@jit\ndef rational_quadratic_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    variance: float, \n    length: float, \n    noise: float, \n    alpha: float, \n    jitter: float =1.0e-6, \n    include_noise: bool =True\n) -> ArrayLike:\n\n    squared_differences = jnp.power(\n        (X[:, None] - X_prime),\n        2.0\n    )\n    squared_length_scale = 2 * alpha * jnp.power(\n        length,\n        2.0\n    )\n    covariance_matrix = jnp.power(\n        1 + (squared_differences / squared_length_scale),\n        - alpha\n    )\n    scaled_covariance_matrix = variance * covariance_matrix\n\n    if include_noise:\n        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])\n\n    return scaled_covariance_matrix\n```\n:::\n\n\n::: {#ecabc82b .cell execution_count=7}\n``` {.python .cell-code}\nkernel_function = partial(\n    rational_quadratic_kernel,\n    variance=1,\n    length=10,\n    alpha=3,\n    noise=0.001\n)\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3\n\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"Rational Quadratic\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\n### Periodic Kernel\n\n::: {#32fe8dcb .cell execution_count=8}\n``` {.python .cell-code}\n@jit\ndef periodic_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    variance: float, \n    length: float, \n    noise: float, \n    period: float, \n    jitter: float = 1.0e-6, \n    include_noise: bool =True\n) -> ArrayLike:\n\n    periodic_difference = jnp.pi * jnp.abs(X[:, None] - X_prime) / period\n    sine_squared_difference = 2 * jnp.power(\n        jnp.sin(periodic_difference),\n        2.0\n    )\n    squared_length_scale = jnp.power(\n        length,\n        2.0\n    )\n    covariance_matrix = jnp.exp(\n        - (sine_squared_difference / squared_length_scale)\n    )\n    scaled_covariance_matrix = variance * covariance_matrix\n\n    if include_noise:\n        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])\n\n    return scaled_covariance_matrix\n```\n:::\n\n\n::: {#6864f907 .cell execution_count=9}\n``` {.python .cell-code}\nkernel_function = partial(\n    periodic_kernel,\n    variance=1,\n    length=5,\n    period=10,\n    noise=0.001\n)\n\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"Periodic\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\n### Linear Kernel\n\n::: {#746f3046 .cell execution_count=10}\n``` {.python .cell-code}\n@jit\ndef linear_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    variance: float, \n    noise: float, \n    jitter: float = 1.0e-6, \n    include_noise: bool = True\n) -> ArrayLike:\n    scaled_covariance_matrix = variance + (X[:, None] * X_prime)\n    if include_noise:\n        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])\n\n    return scaled_covariance_matrix\n```\n:::\n\n\n::: {#addd4bcb .cell execution_count=11}\n``` {.python .cell-code}\nkernel_function = partial(\n    linear_kernel,\n    variance=0.01,\n    noise=0.001\n)\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"Linear\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-12-output-1.png){}\n:::\n:::\n\n\n### White Noise Kernel\n\n::: {#5115bf3f .cell execution_count=12}\n``` {.python .cell-code}\n@jit\ndef white_noise_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    variance: float\n) -> ArrayLike:\n    covariance_matrix = variance * jnp.eye(X.shape[0])\n    return covariance_matrix\n```\n:::\n\n\n::: {#659342e2 .cell execution_count=13}\n``` {.python .cell-code}\nkernel_function = partial(\n    white_noise_kernel,\n    variance=0.01,\n)\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"White Noise\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-14-output-1.png){}\n:::\n:::\n\n\n### Combining Kernels\n\n::: {#65080c1d .cell execution_count=14}\n``` {.python .cell-code}\n@partial(jit, static_argnums=(2,3))\ndef additive_combined_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    first_kernel: Callable, \n    second_kernel: Callable\n) -> ArrayLike:\n    return first_kernel(X=X, X_prime=X_prime) + second_kernel(X=X, X_prime=X_prime)\n\n@partial(jit, static_argnums=(2,3))\ndef multiplicative_combined_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    first_kernel: Callable, \n    second_kernel: Callable\n) -> ArrayLike:\n    return first_kernel(X=X, X_prime=X_prime) * second_kernel(X=X, X_prime=X_prime)\n```\n:::\n\n\n::: {#3a350c48 .cell execution_count=15}\n``` {.python .cell-code}\nkernel_function_linear = partial(\n    linear_kernel,\n    variance=0.0001,\n    noise=0.000\n)\nkernel_function_rbf = partial(\n    RBF_kernel,\n    variance=30,\n    length=2,\n    noise=10\n)\n\nkernel_function = partial(\n    additive_combined_kernel,\n    first_kernel=kernel_function_linear,\n    second_kernel=kernel_function_rbf\n)\n\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"Additive Linear+RBF\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-16-output-1.png){}\n:::\n:::\n\n\n::: {#cc2b3a2a .cell execution_count=16}\n``` {.python .cell-code}\nkernel_function_periodic = partial(\n    periodic_kernel,\n    variance=5,\n    length=5,\n    period=10,\n    noise=0.0001\n)\nkernel_function_rbf = partial(\n    RBF_kernel,\n    variance=.1,\n    length=50,\n    noise=0.0001\n)\n\nkernel_function = partial(\n    multiplicative_combined_kernel,\n    first_kernel=kernel_function_periodic,\n    second_kernel=kernel_function_rbf\n)\n\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"Multiplicative Periodic RBF\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-17-output-1.png){}\n:::\n:::\n\n\n## The Gaussian Process Model\n\n::: {#de37289b .cell execution_count=17}\n``` {.python .cell-code}\ndef RBF_gaussian_process(\n        trend_kernel_variance_dist,\n        trend_kernel_noise_dist,\n        trend_kernel_length_dist,\n\n        seasonal_kernel_variance_dist,\n        seasonal_kernel_noise_dist,\n        seasonal_kernel_length_dist,\n        seasonal_kernel_period_dist,\n\n\n        day_dist,\n        effect_mean_dist,\n        effect_mu_dist,\n        effect_sigma_dist,\n    ):\n\n    def get_model(X, y, days, arm_indexes):\n        # The parameters of the kernel function are sampled from their\n        # associated prior distributions\n        trend_kernel_variance = numpyro.sample(\n            \"trend_kernel_variance\",\n            trend_kernel_variance_dist\n        )\n        trend_kernel_noise = numpyro.sample(\n            \"trend_kernel_noise\",\n            trend_kernel_noise_dist\n        )\n        trend_kernel_length = numpyro.sample(\n            \"trend_kernel_length\",\n            trend_kernel_length_dist\n        )\n\n        seasonal_kernel_variance = numpyro.sample(\n            \"seasonal_kernel_variance\",\n            seasonal_kernel_variance_dist\n        )\n        seasonal_kernel_noise = numpyro.sample(\n            \"seasonal_kernel_noise\",\n            seasonal_kernel_noise_dist\n        )\n        seasonal_kernel_length = numpyro.sample(\n            \"seasonal_kernel_length\",\n            seasonal_kernel_length_dist\n        )\n        seasonal_kernel_period = numpyro.sample(\n            \"seasonal_kernel_length_periodic\",\n            seasonal_kernel_period_dist\n        )\n\n        coefficients_days = numpyro.sample(\n            \"coefficients_days\",\n            day_dist\n        )\n        noise = numpyro.sample(\n            \"noise_dist\",\n            noise_dist\n        )\n\n        effect_mu = numpyro.sample(\n            \"effect_mu\",\n            effect_mu_dist,\n        )\n        effect_sigma = numpyro.sample(\n            \"effect_sigma\",\n            effect_sigma_dist,\n        )\n        with numpyro.plate(\"number_arms\", X.shape[1]):\n            effect = numpyro.sample(\n                \"effect_dist\",\n                dist.Normal(\n                    effect_mu,\n                    effect_sigma,\n                )\n            )\n\n        # Compute the covariance matrix using the RBF kernel\n        covariance_matrix_trend = RBF_kernel(\n            X=X,\n            X_prime=X,\n            variance=trend_kernel_variance,\n            length=trend_kernel_length,\n            noise=trend_kernel_noise\n        )\n        covariance_matrix_seasonal = periodic_kernel(\n            X=X,\n            X_prime=X,\n            variance=seasonal_kernel_variance,\n            length=seasonal_kernel_length,\n            period=seasonal_kernel_period,\n            noise=seasonal_kernel_noise\n        )\n        trend_intercept =  numpyro.sample(\n            \"trend_intercept\",\n            dist.MultivariateNormal(\n                loc=jnp.zeros(X.shape[0]),\n                covariance_matrix=covariance_matrix_trend\n            )\n        )\n        seasonal_intercept =  numpyro.sample(\n            \"seasonal_intercept\",\n            dist.MultivariateNormal(\n                loc=jnp.zeros(X.shape[0]),\n                covariance_matrix=covariance_matrix_seasonal,\n            )\n        )\n\n        global_intercept = numpyro.deterministic(\n            \"global_intercept\", \n            trend_intercept + seasonal_intercept,\n        )\n        days_effect = numpyro.deterministic(\n            \"days_effect\", \n            jnp.dot(days, coefficients_days),\n        )\n\n        # Sample y from a MV Normal distribution of mean 0\n        # and covariance determined by the kernel function\n        numpyro.sample(\n            \"y\",\n            dist.Normal(\n                global_intercept + days_effect + effect[arm_indexes],\n                noise\n            ),\n            obs=y,\n        )\n\n    return get_model\n```\n:::\n\n\n# Simulating the data\n\n## Generating the underlying process\n\n::: {#f170fa98 .cell execution_count=18}\n``` {.python .cell-code}\ndef generate_underlying(\n    time_index, \n    periodic_variance, \n    periodic_length, \n    period, \n    rbf_variance, \n    rbf_length\n):\n    kernel_function_seasonality = partial(\n        periodic_kernel,\n        variance=periodic_variance,\n        length=periodic_length,\n        period=period, \n        noise=0.0001\n    )\n    kernel_function_slow_variation = partial(\n        RBF_kernel,\n        variance=rbf_variance,\n        length=rbf_length,\n        noise=0.0001\n    )\n\n    kernel_function_underlying = partial(\n        additive_combined_kernel,\n        first_kernel=kernel_function_seasonality,\n        second_kernel=kernel_function_slow_variation\n    )\n\n    generated_covariance, distance, sampled_function_underlying = simulate_kernel(\n        X=time_index,\n        X_prime=time_index,\n        kernel_function=kernel_function_underlying,\n        samples=1,\n    )\n    return generated_covariance, distance, sampled_function_underlying\n\n\ngenerated_covariance, distance, sampled_based_function = generate_underlying(\n    time_index=TIME_IDX, \n    periodic_variance=0.25, \n    periodic_length=14, \n    period=7, # every seven days we have weekly variations\n    rbf_variance=0.125, \n    rbf_length=14,  # slow variation with 21 days decay\n)\n\nsampled_based_function = (\n    (sampled_based_function - sampled_based_function.min()) / \n    (sampled_based_function.max() - sampled_based_function.min())\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_based_function,\n    kernel_name=\"Simulated Underlying Process\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-19-output-1.png){}\n:::\n:::\n\n\n## Adding the experimental effects\n\n::: {#08c3d3ed .cell execution_count=19}\n``` {.python .cell-code}\n# We assume constant effect as a fraction of standard deviation\n# This is somehow unrealistic\nmodels_parameters = {\n    \"Branch B\": {\n        \"experimental_delta\": sampled_based_function.std() * .5, \n        \"periodic_length\": 6, \n        \"period\": 3, \n        \"rbf_length\": 5,\n    }, \n    \"Branch C\": {\n        \"experimental_delta\": sampled_based_function.std() * .75, \n        \"periodic_length\": 10, \n        \"period\": 5, \n        \"rbf_length\": 14,\n    }, \n    \"Branch D\": {\n        \"experimental_delta\": sampled_based_function.std() * 1., \n        \"periodic_length\": 10, \n        \"period\": 14, \n        \"rbf_length\": 28,\n    }, \n    \"Branch E\": {\n        \"experimental_delta\": sampled_based_function.std() * 1.25, \n        \"periodic_length\": 14, \n        \"period\": 7, \n        \"rbf_length\": 3,\n    }, \n}\n\nsampled_functions_underlying = [sampled_based_function.flatten()]\nfor arm in [\"Branch B\", \"Branch C\", \"Branch D\", \"Branch E\"]:\n\n    kernel_function = partial(\n        white_noise_kernel,\n        variance=.01,\n\n    )\n    _, _, sampled_function_underlying = generate_underlying(\n        time_index=TIME_IDX, \n        periodic_variance=0.25, \n        periodic_length=models_parameters[arm][\"periodic_length\"], \n        period=models_parameters[arm][\"period\"],\n        rbf_variance=0.125, \n        rbf_length=models_parameters[arm][\"rbf_length\"],\n    )\n    _, _, sampled_experimental_effect_function = simulate_kernel(\n        X=TIME_IDX,\n        X_prime=TIME_IDX,\n        kernel_function=kernel_function,\n        mean=models_parameters[arm][\"experimental_delta\"],\n        samples=1\n    )\n    \n    sampled_functions_underlying.append(\n        (\n            sampled_function_underlying + sampled_experimental_effect_function\n        ).flatten()\n    )\n\nsampled_functions_underlying = np.array(sampled_functions_underlying)\n```\n:::\n\n\n::: {#a3254e9b .cell execution_count=20}\n``` {.python .cell-code}\nfig, ax = plt.subplots(1, 1, figsize=(8, 4))\nfor idx, series in enumerate(range(sampled_functions_underlying.shape[0])):\n\n    ax = plot_univariate_series(\n        series_data={\n            \"x\": {\n                \"data\": TIME_IDX,\n                \"label\": \"x\"\n            },\n            \"y\": {\n                \"data\":  sampled_functions_underlying[series, :],\n                \"label\": \"y\"\n            },\n\n        },\n        ax=ax,\n        alpha=0.25 if idx != 0 else 1.\n    )\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-21-output-1.png){}\n:::\n:::\n\n\n## Adding WeekDay Effects\n\n::: {#eacbd2d5 .cell execution_count=21}\n``` {.python .cell-code}\nweekday_effect = np.array(\n    [\n        0.,\n        0.,\n        0.,\n        0.,\n        0.15,\n        -0.1,\n        -0.05\n    ]\n)\nweekday_effect = np.hstack([weekday_effect for _ in range(TIME_IDX.shape[0] // 6)])\nweekday_effect = weekday_effect[:TIME_IDX.shape[0]]\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 4))\nax = plot_univariate_series(\n    series_data={\n        \"x\": {\n            \"data\": TIME_IDX,\n            \"label\": \"x\"\n        },\n        \"y\": {\n            \"data\":  np.ones(shape=(TIME_IDX.shape[0])) * weekday_effect,\n            \"label\": \"y\"\n        },\n\n    },\n    ax=ax,\n    alpha=0.5\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-22-output-1.png){}\n:::\n:::\n\n\n::: {#cc230fba .cell execution_count=22}\n``` {.python .cell-code}\nweekday_deltas = (sampled_functions_underlying) * weekday_effect\nsimulated_series = sampled_functions_underlying + weekday_deltas\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 4))\nfor idx, series in enumerate(range(simulated_series.shape[0])):\n\n    ax = plot_univariate_series(\n        series_data={\n            \"x\": {\n                \"data\": TIME_IDX,\n                \"label\": \"x\"\n            },\n            \"y\": {\n                \"data\":  simulated_series[series, :],\n                \"label\": \"y\"\n            },\n\n        },\n        ax=ax,\n        alpha=0.25 if idx != 0 else 1.\n    )\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-23-output-1.png){}\n:::\n:::\n\n\n# Fitting Gaussian Process Models to A/B test data\n\n::: {#4132362a .cell execution_count=23}\n``` {.python .cell-code}\ndf = pd.DataFrame(\n    simulated_series.T,\n    # the names here indicates the 5 branches of an AB test.\n    columns=MODELS\n)\ndf[\"date\"] = DATES\ndf[\"day_week\"] = df[\"date\"].dt.day_name()\ndf.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Branch A</th>\n      <th>Branch B</th>\n      <th>Branch C</th>\n      <th>Branch D</th>\n      <th>Branch E</th>\n      <th>date</th>\n      <th>day_week</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.321283</td>\n      <td>0.036905</td>\n      <td>0.478498</td>\n      <td>1.151216</td>\n      <td>0.897527</td>\n      <td>2023-01-01</td>\n      <td>Sunday</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.302074</td>\n      <td>0.251725</td>\n      <td>0.429615</td>\n      <td>1.319666</td>\n      <td>0.660875</td>\n      <td>2023-01-02</td>\n      <td>Monday</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.358393</td>\n      <td>-0.135311</td>\n      <td>0.593296</td>\n      <td>1.103609</td>\n      <td>0.606442</td>\n      <td>2023-01-03</td>\n      <td>Tuesday</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.260785</td>\n      <td>0.479307</td>\n      <td>0.394926</td>\n      <td>1.117882</td>\n      <td>0.185718</td>\n      <td>2023-01-04</td>\n      <td>Wednesday</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.267859</td>\n      <td>0.200669</td>\n      <td>0.520351</td>\n      <td>1.349445</td>\n      <td>0.032881</td>\n      <td>2023-01-05</td>\n      <td>Thursday</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#b9538790 .cell execution_count=24}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\ndef _set_x_axis_grid(ax):\n    ax.tick_params(\n        direction=\"in\",\n        top=True,\n        axis=\"x\"\n    )\n    ax.grid(\n        visible=True,\n        which=\"major\",\n        axis=\"x\",\n        color=\"k\",\n        alpha=0.25,\n        linestyle=\"--\"\n    )\n    return ax\n\ndef plot_arms_time_series(time, arm_a, arm_b, df, ax, y_label, title):\n    ax.scatter(\n        df[time].values,\n        df[arm_a].values,\n        marker=\"x\",\n        c=\"blue\",\n        label=arm_a\n    )\n    ax.scatter(\n        df[time].values,\n        df[arm_b].values,\n        marker=\"*\",\n        c=\"red\",\n        label=arm_b\n    )\n    ax = _set_x_axis_grid(ax=ax)\n    ax.set_title(title)\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(y_label)\n    return ax\n```\n:::\n\n\n::: {#94502df2 .cell execution_count=25}\n``` {.python .cell-code}\nplt.figure(figsize=(15, 5))\nsns.violinplot(\n    data=df.melt(\n        id_vars=(\"date\"),\n        value_vars=MODELS,\n        var_name=\"Arm\",\n        value_name=\"Dependent Variable\"\n    ),\n    x=\"Arm\",\n    y=\"Dependent Variable\",\n    hue=\"Arm\",\n)\nplt.xlabel(\"AB Test Arms\")\nplt.grid(alpha=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-26-output-1.png){}\n:::\n:::\n\n\n::: {#885cfb2a .cell execution_count=26}\n``` {.python .cell-code}\nplt.figure(figsize=(8, 4))\nsns.violinplot(\n    data=df.melt(\n        id_vars=(\"date\", \"day_week\"),\n        value_vars=MODELS,\n        var_name=\"Arm\",\n        value_name=\"Dependent Variable\"\n    ),\n    x=\"day_week\",\n    y=\"Dependent Variable\",\n    order=[\n        \"Monday\",\n        \"Tuesday\",\n        \"Wednesday\",\n        \"Thursday\",\n        \"Friday\",\n        \"Saturday\",\n        \"Sunday\"\n    ],\n    hue=\"day_week\"\n)\nplt.xlabel(\"Day of the Week\")\nplt.grid(alpha=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-27-output-1.png){}\n:::\n:::\n\n\n::: {#1cf631d3 .cell execution_count=27}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(8, 4))\nax = sns.violinplot(\n    data=df.melt(\n        id_vars=(\"date\", \"day_week\"),\n        value_vars=MODELS,\n        var_name=\"Arm\",\n        value_name=\"Dependent Variable\"\n    ),\n    x=\"day_week\",\n    y=\"Dependent Variable\",\n    hue=\"Arm\",\n    order=[\n        \"Monday\",\n        \"Tuesday\",\n        \"Wednesday\",\n        \"Thursday\",\n        \"Friday\",\n        \"Saturday\",\n        \"Sunday\"\n    ]\n)\nsns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\nplt.xlabel(\"Day of the Week\")\nplt.grid(alpha=0.5)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-28-output-1.png){}\n:::\n:::\n\n\n::: {#0ec8d5b3 .cell execution_count=28}\n``` {.python .cell-code}\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 6), sharex=True, sharey=True)\narms = MODELS.copy()\narms.remove(\"Branch A\")\n\nfor arm_b, ax in zip(arms, axs.flatten()):\n\n    ax = plot_arms_time_series(\n        time=\"date\",\n        arm_a=\"Branch A\",\n        arm_b=arm_b,\n        df=df,\n        ax=ax,\n        y_label=\"Dependent Variable\",\n        title=\"AB Test\"\n    )\n    ax.legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-29-output-1.png){}\n:::\n:::\n\n\n::: {#696fa6f2 .cell execution_count=29}\n``` {.python .cell-code}\narms = MODELS.copy()\narms.remove(\"Branch A\")\n\nfig, axs = plt.subplots(\n    nrows=2,\n    ncols=2,\n    figsize=(12, 6),\n    sharex=True,\n    sharey=True\n)\nfor arm_b, ax in zip(arms, axs.flatten()):\n\n    delta = df[arm_b] - df[\"Branch A\"]\n    time = df[TIME]\n\n    semed = median_abs_deviation(delta) / np.sqrt(len(time))\n\n    ax.scatter(\n        time,\n        delta,\n        marker=\"o\",\n        facecolors='none',\n        edgecolors='r',\n        linestyle=\":\"\n    )\n    ax.set_title(arm_b)\n    ax.plot(\n        time,\n        [np.median(delta)] * len(time),\n        c='r',\n    )\n    ax.fill_between(\n        time,\n        [np.median(delta) - (1.96 * semed)] * len(time),\n        [np.median(delta) + (1.96 * semed)] * len(time),\n        color='r',\n        alpha=0.1\n    )\n    ax.axhline(\n        0,\n        linestyle=\":\",\n        c=\"k\"\n    )\n    ax.set_xlabel(\"Date\")\n    ax.set_ylabel(\"$ \\Delta $\\nDependent Variable\")\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:45: SyntaxWarning: invalid escape sequence '\\D'\n<>:45: SyntaxWarning: invalid escape sequence '\\D'\n/var/folders/h_/lq2hvf816xs9ffng6570sblh0000gn/T/ipykernel_20044/2513872932.py:45: SyntaxWarning: invalid escape sequence '\\D'\n  ax.set_ylabel(\"$ \\Delta $\\nDependent Variable\")\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-30-output-2.png){}\n:::\n:::\n\n\n# Visualize the results\n\n# Conclusion\n\n# Hardware and Requirements\nHere you can find the hardware and python requirements used for building this post.\n\n::: {#5cd3b867 .cell execution_count=30}\n``` {.python .cell-code}\n%watermark\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLast updated: 2025-04-04T08:17:03.779521+01:00\n\nPython implementation: CPython\nPython version       : 3.13.2\nIPython version      : 9.0.2\n\nCompiler    : Clang 18.1.8 \nOS          : Darwin\nRelease     : 24.3.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 14\nArchitecture: 64bit\n\n```\n:::\n:::\n\n\n::: {#c8c9968e .cell execution_count=31}\n``` {.python .cell-code}\n%watermark --iversions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npandas    : 2.2.3\nnumpyro   : 0.18.0\nseaborn   : 0.13.2\nmatplotlib: 3.10.1\nscipy     : 1.15.2\nnumpy     : 2.2.4\njax       : 0.5.2\n\n```\n:::\n:::\n\n\n",
    "supporting": [
      "ab_test_gp_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}