{
  "hash": "2e7cdb7f80a2b7b5cbb66481a48efa5d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Analyzing A/B Test Data using Gaussian Processes\ndescription: 'This post illustrates how to analyze longitudinal A/B test data using Gaussian Process.'\ndate: '2025-03-29'\ncategories:\n  - JAX\n  - Numpyro\n  - Bayesian Statistics\n  - A/B Test\n  - Gaussian Process\nbibliography: bibliographies/ab_test_gp.bib\njupyter: python3\n---\n\n::: {#509af8c8 .cell vscode='{\"languageId\":\"python\"}' execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\n%load_ext watermark\n\nNUM_CHAINS = 4\n\nfrom typing import Dict, Any, Callable, Tuple\n\nfrom functools import partial\n\nimport pandas as pd\nimport numpy as np\n\nimport scipy\nfrom scipy.stats import lognorm, median_abs_deviation\n\nimport jax\nfrom jax.typing import ArrayLike\nimport jax.random as random\nfrom jax import jit\nfrom jax import numpy as jnp\nfrom jax import vmap\n\nimport numpyro\nnumpyro.set_host_device_count(NUM_CHAINS)\nimport numpyro.distributions as dist\nfrom numpyro.infer import (\n    MCMC,\n    NUTS,\n    Predictive\n)\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.figure import Figure\nfrom matplotlib import gridspec\n\nimport seaborn as sns\n\ndef plot_univariate_series(\n        series_data: Dict[Any, Any], \n        ax: plt.Axes, \n        **plot_kwargs: Any\n    ) -> plt.Axes:\n    ax.scatter(\n        series_data[\"x\"][\"data\"],\n        series_data[\"y\"][\"data\"],\n        **plot_kwargs\n    )\n    ax.plot(\n        series_data[\"x\"][\"data\"],\n        series_data[\"y\"][\"data\"],\n        **plot_kwargs\n    )\n    ax.set_xlabel(series_data[\"x\"][\"label\"])\n    ax.set_ylabel(series_data[\"y\"][\"label\"])\n\n    ax.tick_params(\n        direction=\"in\",\n        top=True, axis=\"x\",\n        rotation=45,\n    )\n    ax.grid(\n        visible=True,\n        which=\"major\",\n        axis=\"x\",\n        color=\"k\",\n        alpha=0.25,\n        linestyle=\"--\",\n    )\n    return ax\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe watermark extension is already loaded. To reload it, use:\n  %reload_ext watermark\n```\n:::\n:::\n\n\n# Premise\n\nWhat will be illustrated in this post is strongly in spired by the content of the books \"Statistical Rethinking\" [@mcelreath2018statistical] and \"Bayesian Data Analysis 3rd Edition\" [@gelman1995bayesian]. In particular, the idea to separately code and illustrate the behavior of different covariance kernel functions comes from the amazing [\"Kernel Cookbook\"](https://www.cs.toronto.edu/~duvenaud/cookbook/) and PhD thesis of David Duvenaud [@duvenaud2014automatic].\n\nThis post assumes some level of knowledge in bayesian statistics and probabilistic programming.\n\n## What we will cover\n\n1. Very brief illustration of longitudinal A/B test within and observational paradigm.\n2. Very brief illustration of gaussian processes and their application to analyzing A/B test data.\n3. Overview of how to implement a gaussian process model using [Numpyro](https://num.pyro.ai/en/stable/index.html) and [JAX](https://docs.jax.dev/en/latest/index.html).\n4. Simulating A/B test data.\n5. Analyzing A/B test data within an modelling setting.\n\n## What we will **not** cover\n\n1. Detailed coverage of A/B test (e.g., sampling, randomization etc...).\n2. Hypothesis testing (we will focus on modelling).\n3. Fundamentals of bayesian statistics.\n4. Detailed overview of Gaussian processes.\n5. Probabilistic programming and sampling algorithms.\n\n# Introduction\n\n## Longitudinal A/B tests in observational settings\n\nWhen we talk about A/B test we usually refer to a research method used for evaluating if a given intervention is having an impact on a pre-defined outcome variable measured inside a sample. For doing so, we can draw two distinct samples (the A and B group) from a population of interest, subject one of the two to the intervention and then measure observed differences in the outcome variable. Subject to **several** assumptions and pre-conditions (we suggest reading part V of \"Regression and Other Stories\" [@gelman2021regression]), if we observe a difference between the two groups when can conclude that our intervention might have had an impact on our outcome variable.\n\n## Gaussian Process\n\nThe Gaussian Process $GP$ can be thought as the continuous generalization of basis function regression (see Chapter 20 of [@gelman1995bayesian]). We can think of it as a stochastic process where any point drawn from it, $x_1, \\dots, x_n$, comes from a multi-dimensional gaussian. In other words, it is as a prior distribution over an un-known function $\\mu(x)$ defined as\n\n$$\n\\mu(x_1), \\dots, \\mu(x_n) \\sim \\mathcal{N}((m(x_1), \\dots, m(x_n)), k(x, \\dots, x_n))\n$$\n\nor more compactly\n\n$$\n\\mu(x) \\sim GP(m, k)\n$$\n\nwhere $m$ is a mean function and $k$ is a covariance function. We can already have an intuition of how defining the $GP$ in terms mean and covariance **functions** gives us quite some flexibility.\n\n## Gaussian Process for A/B test\n\n# Implementing a Gaussian Process Model in Numpyro\n\n::: {#06d86f24 .cell execution_count=2}\n``` {.python .cell-code}\nBLUE = (0, 83, 159)\nBLUE = tuple(value / 255. for value in BLUE)\n\nRED = (238, 28, 46)\nRED = tuple(value / 255. for value in RED)\n\nTIME_IDX = np.arange(7*6)\nDATES = pd.date_range(\n    start=\"01-01-2023\",\n    periods=len(TIME_IDX)\n).values\nTIME = \"date\"\nMODELS = [\n    \"Branch A\",\n    \"Branch B\",\n    \"Branch C\",\n    \"Branch D\",\n    \"Branch E\",\n]\n```\n:::\n\n\n## Kernel Functions\n\n::: {#589f0463 .cell vscode='{\"languageId\":\"python\"}' execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\ndef simulate_kernel(\n    X: ArrayLike, \n    X_prime:ArrayLike, \n    kernel_function: Callable, \n    samples: int,\n    mean: float = 0.\n) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:\n    generated_covariance = kernel_function(\n        X=X,\n        X_prime=X_prime,\n    )\n    distance = generated_covariance[:, len(X) // 2]\n    sampled_functions = np.random.multivariate_normal(\n        mean=np.zeros(\n            shape=generated_covariance.shape[0]\n        ) + mean,\n        cov=generated_covariance,\n        size=samples\n    )\n    return generated_covariance, distance, sampled_functions\n\ndef visualize_kernel(\n    X: ArrayLike, \n    generated_covariance: ArrayLike, \n    distance: ArrayLike, \n    sampled_functions: ArrayLike, \n    kernel_name: str\n) -> Figure:\n\n    fig = plt.figure(\n        figsize=(8, 8),\n        tight_layout=True\n    )\n    grid = gridspec.GridSpec(\n        nrows=2,\n        ncols=2\n    )\n    ax_functions = fig.add_subplot(grid[0, :])\n    ax_distance = fig.add_subplot(grid[1, 0])\n    ax_covariance = fig.add_subplot(grid[1, 1])\n\n    for index in range(sampled_functions.shape[0]):\n\n        ax_functions = plot_univariate_series(\n            series_data={\n                \"x\": {\n                    \"data\": X,\n                    \"label\": \"x\"\n                },\n                \"y\": {\n                    \"data\": sampled_functions[index, :],\n                    \"label\": \"y\"\n                },\n\n            },\n            ax=ax_functions,\n            alpha=0.5\n        )\n\n    ax_functions.set_title(\"Sampled Functions \\n from MvNormal\")\n    ax_functions.axhline(0, linestyle=\"--\", c=\"k\")\n\n    ax_distance.plot(\n        X - (len(X) // 2),\n        distance\n    )\n    ax_distance.grid(alpha=0.5)\n    ax_distance.set_title(f\"Distance Function \\n Determined by {kernel_name} Kernel\")\n    ax_covariance.set_ylabel(\"Similarity\")\n    ax_covariance.set_xlabel(\"Distance\")\n\n    ax_covariance.imshow(\n        generated_covariance\n    )\n    ax_covariance.set_ylabel(\"x'\")\n    ax_covariance.set_xlabel(\"x\")\n    ax_covariance.set_title(f\"Covariance \\n Determined by {kernel_name} Kernel\")\n\n\n    plt.suptitle(f\"{kernel_name} Kernel\")\n    return fig\n```\n:::\n\n\n### Radial Basis Function Kernel\n\n::: {#d0d91d01 .cell execution_count=4}\n``` {.python .cell-code}\n@jit\ndef RBF_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    variance: float, \n    length: float, \n    noise: float,\n    jitter: float =1.0e-6, \n    include_noise: bool =True\n) -> ArrayLike:\n    squared_differences = jnp.power(\n        (X[:, None] - X_prime),\n        2.0\n    )\n    squared_length_scale = 2 * jnp.power(\n        length,\n        2.0\n    )\n    covariance_matrix = jnp.exp(\n        - (squared_differences / squared_length_scale)\n    )\n    scaled_covariance_matrix = variance * covariance_matrix\n\n    if include_noise:\n        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])\n\n    return scaled_covariance_matrix\n```\n:::\n\n\n::: {#c2849687 .cell execution_count=5}\n``` {.python .cell-code}\nkernel_function = partial(\n    RBF_kernel,\n    variance=1,\n    length=10,\n    noise=0.001\n)\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"RBF\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\n### Matern Kernel\n\n::: {#e8926e84 .cell execution_count=6}\n``` {.python .cell-code}\n@jit\ndef rational_quadratic_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    variance: float, \n    length: float, \n    noise: float, \n    alpha: float, \n    jitter: float =1.0e-6, \n    include_noise: bool =True\n) -> ArrayLike:\n\n    squared_differences = jnp.power(\n        (X[:, None] - X_prime),\n        2.0\n    )\n    squared_length_scale = 2 * alpha * jnp.power(\n        length,\n        2.0\n    )\n    covariance_matrix = jnp.power(\n        1 + (squared_differences / squared_length_scale),\n        - alpha\n    )\n    scaled_covariance_matrix = variance * covariance_matrix\n\n    if include_noise:\n        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])\n\n    return scaled_covariance_matrix\n```\n:::\n\n\n::: {#75a73244 .cell execution_count=7}\n``` {.python .cell-code}\nkernel_function = partial(\n    rational_quadratic_kernel,\n    variance=1,\n    length=10,\n    alpha=3,\n    noise=0.001\n)\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3\n\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"Rational Quadratic\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\n### Periodic Kernel\n\n::: {#9b54f966 .cell execution_count=8}\n``` {.python .cell-code}\n@jit\ndef periodic_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    variance: float, \n    length: float, \n    noise: float, \n    period: float, \n    jitter: float = 1.0e-6, \n    include_noise: bool =True\n) -> ArrayLike:\n\n    periodic_difference = jnp.pi * jnp.abs(X[:, None] - X_prime) / period\n    sine_squared_difference = 2 * jnp.power(\n        jnp.sin(periodic_difference),\n        2.0\n    )\n    squared_length_scale = jnp.power(\n        length,\n        2.0\n    )\n    covariance_matrix = jnp.exp(\n        - (sine_squared_difference / squared_length_scale)\n    )\n    scaled_covariance_matrix = variance * covariance_matrix\n\n    if include_noise:\n        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])\n\n    return scaled_covariance_matrix\n```\n:::\n\n\n::: {#8ae0caf3 .cell execution_count=9}\n``` {.python .cell-code}\nkernel_function = partial(\n    periodic_kernel,\n    variance=1,\n    length=5,\n    period=10,\n    noise=0.001\n)\n\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"Periodic\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\n### Linear Kernel\n\n::: {#a6873fbc .cell execution_count=10}\n``` {.python .cell-code}\n@jit\ndef linear_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    variance: float, \n    noise: float, \n    jitter: float = 1.0e-6, \n    include_noise: bool = True\n) -> ArrayLike:\n    scaled_covariance_matrix = variance + (X[:, None] * X_prime)\n    if include_noise:\n        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])\n\n    return scaled_covariance_matrix\n```\n:::\n\n\n::: {#a293b262 .cell execution_count=11}\n``` {.python .cell-code}\nkernel_function = partial(\n    linear_kernel,\n    variance=0.01,\n    noise=0.001\n)\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"Linear\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-12-output-1.png){}\n:::\n:::\n\n\n### White Noise Kernel\n\n::: {#86720352 .cell execution_count=12}\n``` {.python .cell-code}\n@jit\ndef white_noise_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    variance: float\n) -> ArrayLike:\n    covariance_matrix = variance * jnp.eye(X.shape[0])\n    return covariance_matrix\n```\n:::\n\n\n::: {#6b579dc5 .cell execution_count=13}\n``` {.python .cell-code}\nkernel_function = partial(\n    white_noise_kernel,\n    variance=0.01,\n)\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"White Noise\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-14-output-1.png){}\n:::\n:::\n\n\n### Combining Kernels\n\n::: {#4901a7bb .cell execution_count=14}\n``` {.python .cell-code}\n@partial(jit, static_argnums=(2,3))\ndef additive_combined_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    first_kernel: Callable, \n    second_kernel: Callable\n) -> ArrayLike:\n    return first_kernel(X=X, X_prime=X_prime) + second_kernel(X=X, X_prime=X_prime)\n\n@partial(jit, static_argnums=(2,3))\ndef multiplicative_combined_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    first_kernel: Callable, \n    second_kernel: Callable\n) -> ArrayLike:\n    return first_kernel(X=X, X_prime=X_prime) * second_kernel(X=X, X_prime=X_prime)\n```\n:::\n\n\n::: {#2378beeb .cell execution_count=15}\n``` {.python .cell-code}\nkernel_function_linear = partial(\n    linear_kernel,\n    variance=0.0001,\n    noise=0.000\n)\nkernel_function_rbf = partial(\n    RBF_kernel,\n    variance=30,\n    length=2,\n    noise=10\n)\n\nkernel_function = partial(\n    additive_combined_kernel,\n    first_kernel=kernel_function_linear,\n    second_kernel=kernel_function_rbf\n)\n\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"Additive Linear+RBF\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-16-output-1.png){}\n:::\n:::\n\n\n::: {#f969234f .cell execution_count=16}\n``` {.python .cell-code}\nkernel_function_periodic = partial(\n    periodic_kernel,\n    variance=5,\n    length=5,\n    period=10,\n    noise=0.0001\n)\nkernel_function_rbf = partial(\n    RBF_kernel,\n    variance=.1,\n    length=50,\n    noise=0.0001\n)\n\nkernel_function = partial(\n    multiplicative_combined_kernel,\n    first_kernel=kernel_function_periodic,\n    second_kernel=kernel_function_rbf\n)\n\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"Multiplicative Periodic RBF\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-17-output-1.png){}\n:::\n:::\n\n\n## The Gaussian Process Model\n\n::: {#ba5e9a29 .cell execution_count=17}\n``` {.python .cell-code}\ndef RBF_gaussian_process(\n        trend_kernel_variance_dist,\n        trend_kernel_noise_dist,\n        trend_kernel_length_dist,\n\n        seasonal_kernel_variance_dist,\n        seasonal_kernel_noise_dist,\n        seasonal_kernel_length_dist,\n        seasonal_kernel_period_dist,\n\n\n        day_dist,\n        effect_mean_dist,\n        effect_mu_dist,\n        effect_sigma_dist,\n    ):\n\n    def get_model(X, y, days, arm_indexes):\n        # The parameters of the kernel function are sampled from their\n        # associated prior distributions\n        trend_kernel_variance = numpyro.sample(\n            \"trend_kernel_variance\",\n            trend_kernel_variance_dist\n        )\n        trend_kernel_noise = numpyro.sample(\n            \"trend_kernel_noise\",\n            trend_kernel_noise_dist\n        )\n        trend_kernel_length = numpyro.sample(\n            \"trend_kernel_length\",\n            trend_kernel_length_dist\n        )\n\n        seasonal_kernel_variance = numpyro.sample(\n            \"seasonal_kernel_variance\",\n            seasonal_kernel_variance_dist\n        )\n        seasonal_kernel_noise = numpyro.sample(\n            \"seasonal_kernel_noise\",\n            seasonal_kernel_noise_dist\n        )\n        seasonal_kernel_length = numpyro.sample(\n            \"seasonal_kernel_length\",\n            seasonal_kernel_length_dist\n        )\n        seasonal_kernel_period = numpyro.sample(\n            \"seasonal_kernel_length_periodic\",\n            seasonal_kernel_period_dist\n        )\n\n        coefficients_days = numpyro.sample(\n            \"coefficients_days\",\n            day_dist\n        )\n        noise = numpyro.sample(\n            \"noise_dist\",\n            noise_dist\n        )\n\n        effect_mu = numpyro.sample(\n            \"effect_mu\",\n            effect_mu_dist,\n        )\n        effect_sigma = numpyro.sample(\n            \"effect_sigma\",\n            effect_sigma_dist,\n        )\n        with numpyro.plate(\"number_arms\", X.shape[1]):\n            effect = numpyro.sample(\n                \"effect_dist\",\n                dist.Normal(\n                    effect_mu,\n                    effect_sigma,\n                )\n            )\n\n        # Compute the covariance matrix using the RBF kernel\n        covariance_matrix_trend = RBF_kernel(\n            X=X,\n            X_prime=X,\n            variance=trend_kernel_variance,\n            length=trend_kernel_length,\n            noise=trend_kernel_noise\n        )\n        covariance_matrix_seasonal = periodic_kernel(\n            X=X,\n            X_prime=X,\n            variance=seasonal_kernel_variance,\n            length=seasonal_kernel_length,\n            period=seasonal_kernel_period,\n            noise=seasonal_kernel_noise\n        )\n        trend_intercept =  numpyro.sample(\n            \"trend_intercept\",\n            dist.MultivariateNormal(\n                loc=jnp.zeros(X.shape[0]),\n                covariance_matrix=covariance_matrix_trend\n            )\n        )\n        seasonal_intercept =  numpyro.sample(\n            \"seasonal_intercept\",\n            dist.MultivariateNormal(\n                loc=jnp.zeros(X.shape[0]),\n                covariance_matrix=covariance_matrix_seasonal,\n            )\n        )\n\n        global_intercept = numpyro.deterministic(\n            \"global_intercept\", \n            trend_intercept + seasonal_intercept,\n        )\n        days_effect = numpyro.deterministic(\n            \"days_effect\", \n            jnp.dot(days, coefficients_days),\n        )\n\n        # Sample y from a MV Normal distribution of mean 0\n        # and covariance determined by the kernel function\n        numpyro.sample(\n            \"y\",\n            dist.Normal(\n                global_intercept + days_effect + effect[arm_indexes],\n                noise\n            ),\n            obs=y,\n        )\n\n    return get_model\n```\n:::\n\n\n# Simulating the data\n\n## Generating the underlying process\n\n::: {#05f7c715 .cell execution_count=18}\n``` {.python .cell-code}\nkernel_function_seasonality = partial(\n    periodic_kernel,\n    variance=.25,\n    length=2,\n    period=7, # every seven days we have weekly variations\n    noise=0.0001\n)\nkernel_function_slow_variation = partial(\n    RBF_kernel,\n    variance=1,\n    length=16, # slow variation with 21 days decay\n    noise=0.0001\n)\n\nkernel_function_underlying = partial(\n    additive_combined_kernel,\n    first_kernel=kernel_function_seasonality,\n    second_kernel=kernel_function_slow_variation\n)\n\ngenerated_covariance, distance, sampled_function_underlying = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function_underlying,\n    samples=1,\n)\nsampled_function_underlying = (\n    (sampled_function_underlying - sampled_function_underlying.min()) / \n    (sampled_function_underlying.max() - sampled_function_underlying.min())\n)\n\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_function_underlying,\n    kernel_name=\"Simulated Underlying Process\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-19-output-1.png){}\n:::\n:::\n\n\n## Adding the experimental effects\n\n::: {#c96eaa37 .cell execution_count=19}\n``` {.python .cell-code}\n# We assume constant effect as a fraction of standard deviation\n# This is somehow unrealistic\nexperimental_delta = np.array(\n    [\n        0.,\n        sampled_function_underlying.std() * 0.5,\n        sampled_function_underlying.std() * 0.75,\n        sampled_function_underlying.std() * 1.,\n        sampled_function_underlying.std() * 1.25\n    ]\n).reshape(-1, 1)\n\n\nsampled_functions_underlying = []\nfor arm in range(experimental_delta.shape[0]):\n\n    kernel_function = partial(\n        white_noise_kernel,\n        variance=.01,\n\n    )\n    _, _, sampled_experimental_effect_function = simulate_kernel(\n        X=TIME_IDX,\n        X_prime=TIME_IDX,\n        kernel_function=kernel_function,\n        mean=experimental_delta[arm, :],\n        samples=1,\n    )\n    \n    sampled_functions_underlying.append(\n        (\n            sampled_function_underlying + \n            sampled_experimental_effect_function\n        ).flatten()\n    )\n\nsampled_functions_underlying = np.array(sampled_functions_underlying)\n```\n:::\n\n\n::: {#03cae97f .cell execution_count=20}\n``` {.python .cell-code}\nfig, ax = plt.subplots(1, 1, figsize=(8, 4))\nfor idx, series in enumerate(range(sampled_functions_underlying.shape[0])):\n\n    ax = plot_univariate_series(\n        series_data={\n            \"x\": {\n                \"data\": TIME_IDX,\n                \"label\": \"x\"\n            },\n            \"y\": {\n                \"data\":  sampled_functions_underlying[series, :],\n                \"label\": \"y\"\n            },\n\n        },\n        ax=ax,\n        alpha=0.25 if idx != 0 else 1.\n    )\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-21-output-1.png){}\n:::\n:::\n\n\n## Adding WeekDay Effects\n\n::: {#86e45552 .cell execution_count=21}\n``` {.python .cell-code}\nweekday_effect = np.array(\n    [\n        0.,\n        0.,\n        0.,\n        0.,\n        0.15,\n        -0.1,\n        -0.05\n    ]\n)\nweekday_effect = np.hstack([weekday_effect for _ in range(TIME_IDX.shape[0] // 6)])\nweekday_effect = weekday_effect[:TIME_IDX.shape[0]]\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 4))\nax = plot_univariate_series(\n    series_data={\n        \"x\": {\n            \"data\": TIME_IDX,\n            \"label\": \"x\"\n        },\n        \"y\": {\n            \"data\":  np.ones(shape=(TIME_IDX.shape[0])) * weekday_effect,\n            \"label\": \"y\"\n        },\n\n    },\n    ax=ax,\n    alpha=0.5\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-22-output-1.png){}\n:::\n:::\n\n\n::: {#c61e0caf .cell execution_count=22}\n``` {.python .cell-code}\nweekday_deltas = (sampled_functions_underlying) * weekday_effect\nsimulated_series = sampled_functions_underlying + weekday_deltas\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 4))\nfor idx, series in enumerate(range(simulated_series.shape[0])):\n\n    ax = plot_univariate_series(\n        series_data={\n            \"x\": {\n                \"data\": TIME_IDX,\n                \"label\": \"x\"\n            },\n            \"y\": {\n                \"data\":  simulated_series[series, :],\n                \"label\": \"y\"\n            },\n\n        },\n        ax=ax,\n        alpha=0.25 if idx != 0 else 1.\n    )\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-23-output-1.png){}\n:::\n:::\n\n\n# Fitting Gaussian Process Models to A/B test data\n\n# Visualize the results\n\n# Conclusion\n\n# Hardware and Requirements\nHere you can find the hardware and python requirements used for building this post.\n\n::: {#9e9a9737 .cell execution_count=23}\n``` {.python .cell-code}\n%watermark\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLast updated: 2025-04-01T21:51:53.941136+01:00\n\nPython implementation: CPython\nPython version       : 3.13.2\nIPython version      : 9.0.2\n\nCompiler    : Clang 18.1.8 \nOS          : Darwin\nRelease     : 24.3.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 14\nArchitecture: 64bit\n\n```\n:::\n:::\n\n\n::: {#63626ecd .cell execution_count=24}\n``` {.python .cell-code}\n%watermark --iversions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nscipy     : 1.15.2\nseaborn   : 0.13.2\npandas    : 2.2.3\nmatplotlib: 3.10.1\njax       : 0.5.2\nnumpyro   : 0.18.0\nnumpy     : 2.2.4\n\n```\n:::\n:::\n\n\n",
    "supporting": [
      "ab_test_gp_files"
    ],
    "filters": [],
    "includes": {}
  }
}