{
  "hash": "6266a44bce027630326334fff4fe3d43",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Analyzing A/B Test Data using Gaussian Processes\ndescription: 'This post illustrates how to analyze longitudinal A/B test data using Gaussian Process.'\ndate: '2025-03-29'\ncategories:\n  - JAX\n  - Numpyro\n  - Bayesian Statistics\n  - A/B Test\n  - Gaussian Process\nbibliography: bibliographies/ab_test_gp.bib\njupyter: python3\n---\n\n::: {#7a7bebec .cell vscode='{\"languageId\":\"python\"}' execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\n%load_ext watermark\n\nNUM_CHAINS = 4\n\nfrom typing import Dict, Any, Callable, Tuple\n\nfrom functools import partial\n\nimport pandas as pd\nimport numpy as np\n\nimport scipy\nfrom scipy.stats import lognorm, median_abs_deviation\n\nimport jax\nfrom jax.typing import ArrayLike\nimport jax.random as random\nfrom jax import jit\nfrom jax import numpy as jnp\nfrom jax import vmap\n\nimport numpyro\nnumpyro.set_host_device_count(NUM_CHAINS)\nimport numpyro.distributions as dist\nfrom numpyro.infer import (\n    MCMC,\n    NUTS,\n    Predictive\n)\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.figure import Figure\nfrom matplotlib import gridspec\n\nimport seaborn as sns\n\ndef plot_univariate_series(\n        series_data: Dict[Any, Any], \n        ax: plt.Axes, \n        **plot_kwargs: Any\n    ) -> plt.Axes:\n    ax.scatter(\n        series_data[\"x\"][\"data\"],\n        series_data[\"y\"][\"data\"],\n        **plot_kwargs\n    )\n    ax.plot(\n        series_data[\"x\"][\"data\"],\n        series_data[\"y\"][\"data\"],\n        **plot_kwargs\n    )\n    ax.set_xlabel(series_data[\"x\"][\"label\"])\n    ax.set_ylabel(series_data[\"y\"][\"label\"])\n\n    ax.tick_params(\n        direction=\"in\",\n        top=True, axis=\"x\",\n        rotation=45,\n    )\n    ax.grid(\n        visible=True,\n        which=\"major\",\n        axis=\"x\",\n        color=\"k\",\n        alpha=0.25,\n        linestyle=\"--\",\n    )\n    return ax\n```\n:::\n\n\n# Premise\n\nWhat will be illustrated in this post is strongly in spired by the content of the books \"Statistical Rethinking\" [@mcelreath2018statistical] and \"Bayesian Data Analysis 3rd Edition\" [@gelman1995bayesian]. In particular, the idea to separately code and illustrate the behavior of different covariance kernel functions comes from the amazing [\"Kernel Cookbook\"](https://www.cs.toronto.edu/~duvenaud/cookbook/) and PhD thesis of David Duvenaud [@duvenaud2014automatic].\n\nThis post assumes some level of knowledge in bayesian statistics and probabilistic programming.\n\n## What we will cover\n\n1. Very brief illustration of longitudinal A/B test.\n2. Very brief illustration of gaussian processes and their application to analyzing A/B test data.\n3. Overview of how to implement a gaussian process model using [Numpyro](https://num.pyro.ai/en/stable/index.html) and [JAX](https://docs.jax.dev/en/latest/index.html).\n4. Simulating A/B test data.\n5. Analyzing A/B test data within an modelling setting.\n\n## What we will **not** cover\n\n1. Detailed coverage of A/B test (e.g., sampling, randomization etc...).\n2. Hypothesis testing (we will focus on modelling).\n3. Fundamentals of bayesian statistics.\n4. Detailed overview of Gaussian processes.\n5. Probabilistic programming and sampling algorithms.\n\n# Introduction\n\n::: {#2065f354 .cell execution_count=2}\n``` {.python .cell-code}\nBLUE = (0, 83, 159)\nBLUE = tuple(value / 255. for value in BLUE)\n\nRED = (238, 28, 46)\nRED = tuple(value / 255. for value in RED)\n\nTIME_IDX = np.arange(7*6)\nDATES = pd.date_range(\n    start=\"01-01-2023\",\n    periods=len(TIME_IDX)\n).values\nTIME = \"date\"\nMODELS = [\n    \"Branch A\",\n    \"Branch B\",\n    \"Branch C\",\n    \"Branch D\",\n    \"Branch E\",\n]\n```\n:::\n\n\n## A/B tests and longitudinal A/B tests\n\n## Gaussian Process\n\n## Gaussian Process for A/B test\n\n# Implementing a Gaussian Process Model in Numpyro\n\n## Kernel Functions\n\n::: {#7d37bdb9 .cell vscode='{\"languageId\":\"python\"}' execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\ndef simulate_kernel(\n    X: ArrayLike, \n    X_prime:ArrayLike, \n    kernel_function: Callable, \n    samples: int,\n    mean: float = 0.\n) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:\n    generated_covariance = kernel_function(\n        X=X,\n        X_prime=X_prime,\n    )\n    distance = generated_covariance[:, len(X) // 2]\n    sampled_functions = np.random.multivariate_normal(\n        mean=np.zeros(\n            shape=generated_covariance.shape[0]\n        ) + mean,\n        cov=generated_covariance,\n        size=samples\n    )\n    return generated_covariance, distance, sampled_functions\n\ndef visualize_kernel(\n    X: ArrayLike, \n    generated_covariance: ArrayLike, \n    distance: ArrayLike, \n    sampled_functions: ArrayLike, \n    kernel_name: str\n) -> Figure:\n\n    fig = plt.figure(\n        figsize=(8, 8),\n        tight_layout=True\n    )\n    grid = gridspec.GridSpec(\n        nrows=2,\n        ncols=2\n    )\n    ax_functions = fig.add_subplot(grid[0, :])\n    ax_distance = fig.add_subplot(grid[1, 0])\n    ax_covariance = fig.add_subplot(grid[1, 1])\n\n    for index in range(sampled_functions.shape[0]):\n\n        ax_functions = plot_univariate_series(\n            series_data={\n                \"x\": {\n                    \"data\": X,\n                    \"label\": \"x\"\n                },\n                \"y\": {\n                    \"data\": sampled_functions[index, :],\n                    \"label\": \"y\"\n                },\n\n            },\n            ax=ax_functions,\n            alpha=0.5\n        )\n\n    ax_functions.set_title(\"Sampled Functions \\n from MvNormal\")\n    ax_functions.axhline(0, linestyle=\"--\", c=\"k\")\n\n    ax_distance.plot(\n        X - (len(X) // 2),\n        distance\n    )\n    ax_distance.grid(alpha=0.5)\n    ax_distance.set_title(f\"Distance Function \\n Determined by {kernel_name} Kernel\")\n    ax_covariance.set_ylabel(\"Similarity\")\n    ax_covariance.set_xlabel(\"Distance\")\n\n    ax_covariance.imshow(\n        generated_covariance\n    )\n    ax_covariance.set_ylabel(\"x'\")\n    ax_covariance.set_xlabel(\"x\")\n    ax_covariance.set_title(f\"Covariance \\n Determined by {kernel_name} Kernel\")\n\n\n    plt.suptitle(f\"{kernel_name} Kernel\")\n    return fig\n```\n:::\n\n\n### Radial Basis Function Kernel\n\n::: {#9be14157 .cell execution_count=4}\n``` {.python .cell-code}\n@jit\ndef RBF_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    variance: float, \n    length: float, \n    noise: float,\n    jitter: float =1.0e-6, \n    include_noise: bool =True\n) -> ArrayLike:\n    squared_differences = jnp.power(\n        (X[:, None] - X_prime),\n        2.0\n    )\n    squared_length_scale = 2 * jnp.power(\n        length,\n        2.0\n    )\n    covariance_matrix = jnp.exp(\n        - (squared_differences / squared_length_scale)\n    )\n    scaled_covariance_matrix = variance * covariance_matrix\n\n    if include_noise:\n        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])\n\n    return scaled_covariance_matrix\n```\n:::\n\n\n::: {#1751f868 .cell execution_count=5}\n``` {.python .cell-code}\nkernel_function = partial(\n    RBF_kernel,\n    variance=1,\n    length=10,\n    noise=0.001\n)\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"RBF\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\n### Matern Kernel\n\n::: {#f1c86142 .cell execution_count=6}\n``` {.python .cell-code}\n@jit\ndef rational_quadratic_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    variance: float, \n    length: float, \n    noise: float, \n    alpha: float, \n    jitter: float =1.0e-6, \n    include_noise: bool =True\n) -> ArrayLike:\n\n    squared_differences = jnp.power(\n        (X[:, None] - X_prime),\n        2.0\n    )\n    squared_length_scale = 2 * alpha * jnp.power(\n        length,\n        2.0\n    )\n    covariance_matrix = jnp.power(\n        1 + (squared_differences / squared_length_scale),\n        - alpha\n    )\n    scaled_covariance_matrix = variance * covariance_matrix\n\n    if include_noise:\n        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])\n\n    return scaled_covariance_matrix\n```\n:::\n\n\n::: {#681928d3 .cell execution_count=7}\n``` {.python .cell-code}\nkernel_function = partial(\n    rational_quadratic_kernel,\n    variance=1,\n    length=10,\n    alpha=3,\n    noise=0.001\n)\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3\n\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"Rational Quadratic\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\n### Periodic Kernel\n\n::: {#a16d1de3 .cell execution_count=8}\n``` {.python .cell-code}\n@jit\ndef periodic_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    variance: float, \n    length: float, \n    noise: float, \n    period: float, \n    jitter: float = 1.0e-6, \n    include_noise: bool =True\n) -> ArrayLike:\n\n    periodic_difference = jnp.pi * jnp.abs(X[:, None] - X_prime) / period\n    sine_squared_difference = 2 * jnp.power(\n        jnp.sin(periodic_difference),\n        2.0\n    )\n    squared_length_scale = jnp.power(\n        length,\n        2.0\n    )\n    covariance_matrix = jnp.exp(\n        - (sine_squared_difference / squared_length_scale)\n    )\n    scaled_covariance_matrix = variance * covariance_matrix\n\n    if include_noise:\n        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])\n\n    return scaled_covariance_matrix\n```\n:::\n\n\n::: {#3e2363e6 .cell execution_count=9}\n``` {.python .cell-code}\nkernel_function = partial(\n    periodic_kernel,\n    variance=1,\n    length=5,\n    period=10,\n    noise=0.001\n)\n\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"Periodic\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\n### Linear Kernel\n\n::: {#e0afd7ce .cell execution_count=10}\n``` {.python .cell-code}\n@jit\ndef linear_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    variance: float, \n    noise: float, \n    jitter: float = 1.0e-6, \n    include_noise: bool = True\n) -> ArrayLike:\n    scaled_covariance_matrix = variance + (X[:, None] * X_prime)\n    if include_noise:\n        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])\n\n    return scaled_covariance_matrix\n```\n:::\n\n\n::: {#06728cdf .cell execution_count=11}\n``` {.python .cell-code}\nkernel_function = partial(\n    linear_kernel,\n    variance=0.01,\n    noise=0.001\n)\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"Linear\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-12-output-1.png){}\n:::\n:::\n\n\n### White Noise Kernel\n\n::: {#13b2db35 .cell execution_count=12}\n``` {.python .cell-code}\n@jit\ndef white_noise_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    variance: float\n) -> ArrayLike:\n    covariance_matrix = variance * jnp.eye(X.shape[0])\n    return covariance_matrix\n```\n:::\n\n\n::: {#315aa866 .cell execution_count=13}\n``` {.python .cell-code}\nkernel_function = partial(\n    white_noise_kernel,\n    variance=0.01,\n)\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"White Noise\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-14-output-1.png){}\n:::\n:::\n\n\n### Combining Kernels\n\n::: {#6cd3da16 .cell execution_count=14}\n``` {.python .cell-code}\n@partial(jit, static_argnums=(2,3))\ndef additive_combined_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    first_kernel: Callable, \n    second_kernel: Callable\n) -> ArrayLike:\n    return first_kernel(X=X, X_prime=X_prime) + second_kernel(X=X, X_prime=X_prime)\n\n@partial(jit, static_argnums=(2,3))\ndef multiplicative_combined_kernel(\n    X: ArrayLike, \n    X_prime: ArrayLike, \n    first_kernel: Callable, \n    second_kernel: Callable\n) -> ArrayLike:\n    return first_kernel(X=X, X_prime=X_prime) * second_kernel(X=X, X_prime=X_prime)\n```\n:::\n\n\n::: {#e1f25ca6 .cell execution_count=15}\n``` {.python .cell-code}\nkernel_function_linear = partial(\n    linear_kernel,\n    variance=0.0001,\n    noise=0.000\n)\nkernel_function_rbf = partial(\n    RBF_kernel,\n    variance=30,\n    length=2,\n    noise=10\n)\n\nkernel_function = partial(\n    additive_combined_kernel,\n    first_kernel=kernel_function_linear,\n    second_kernel=kernel_function_rbf\n)\n\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"Additive Linear+RBF\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-16-output-1.png){}\n:::\n:::\n\n\n::: {#af7ecd88 .cell execution_count=16}\n``` {.python .cell-code}\nkernel_function_periodic = partial(\n    periodic_kernel,\n    variance=5,\n    length=5,\n    period=10,\n    noise=0.0001\n)\nkernel_function_rbf = partial(\n    RBF_kernel,\n    variance=.1,\n    length=50,\n    noise=0.0001\n)\n\nkernel_function = partial(\n    multiplicative_combined_kernel,\n    first_kernel=kernel_function_periodic,\n    second_kernel=kernel_function_rbf\n)\n\ngenerated_covariance, distance, sampled_functions = simulate_kernel(\n    X=TIME_IDX,\n    X_prime=TIME_IDX,\n    kernel_function=kernel_function,\n    samples=3,\n)\nfig = visualize_kernel(\n    TIME_IDX,\n    generated_covariance=generated_covariance,\n    distance=distance,\n    sampled_functions=sampled_functions,\n    kernel_name=\"Multiplicative Periodic RBF\"\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ab_test_gp_files/figure-html/cell-17-output-1.png){}\n:::\n:::\n\n\n## The Gaussian Process Model\n\n::: {#f84c0ea3 .cell execution_count=17}\n``` {.python .cell-code}\ndef RBF_gaussian_process(\n        trend_kernel_variance_dist,\n        trend_kernel_noise_dist,\n        trend_kernel_length_dist,\n\n        seasonal_kernel_variance_dist,\n        seasonal_kernel_noise_dist,\n        seasonal_kernel_length_dist,\n        seasonal_kernel_period_dist,\n\n\n        day_dist,\n        effect_mean_dist,\n        effect_mu_dist,\n        effect_sigma_dist,\n    ):\n\n    def get_model(X, y, days, arm_indexes):\n        # The parameters of the kernel function are sampled from their\n        # associated prior distributions\n        trend_kernel_variance = numpyro.sample(\n            \"trend_kernel_variance\",\n            trend_kernel_variance_dist\n        )\n        trend_kernel_noise = numpyro.sample(\n            \"trend_kernel_noise\",\n            trend_kernel_noise_dist\n        )\n        trend_kernel_length = numpyro.sample(\n            \"trend_kernel_length\",\n            trend_kernel_length_dist\n        )\n\n        seasonal_kernel_variance = numpyro.sample(\n            \"seasonal_kernel_variance\",\n            seasonal_kernel_variance_dist\n        )\n        seasonal_kernel_noise = numpyro.sample(\n            \"seasonal_kernel_noise\",\n            seasonal_kernel_noise_dist\n        )\n        seasonal_kernel_length = numpyro.sample(\n            \"seasonal_kernel_length\",\n            seasonal_kernel_length_dist\n        )\n        seasonal_kernel_period = numpyro.sample(\n            \"seasonal_kernel_length_periodic\",\n            seasonal_kernel_period_dist\n        )\n\n        coefficients_days = numpyro.sample(\n            \"coefficients_days\",\n            day_dist\n        )\n        noise = numpyro.sample(\n            \"noise_dist\",\n            noise_dist\n        )\n\n        effect_mu = numpyro.sample(\n            \"effect_mu\",\n            effect_mu_dist,\n        )\n        effect_sigma = numpyro.sample(\n            \"effect_sigma\",\n            effect_sigma_dist,\n        )\n        with numpyro.plate(\"number_arms\", X.shape[1]):\n            effect = numpyro.sample(\n                \"effect_dist\",\n                dist.Normal(\n                    effect_mu,\n                    effect_sigma,\n                )\n            )\n\n        # Compute the covariance matrix using the RBF kernel\n        covariance_matrix_trend = RBF_kernel(\n            X=X,\n            X_prime=X,\n            variance=trend_kernel_variance,\n            length=trend_kernel_length,\n            noise=trend_kernel_noise\n        )\n        covariance_matrix_seasonal = periodic_kernel(\n            X=X,\n            X_prime=X,\n            variance=seasonal_kernel_variance,\n            length=seasonal_kernel_length,\n            period=seasonal_kernel_period,\n            noise=seasonal_kernel_noise\n        )\n        trend_intercept =  numpyro.sample(\n            \"trend_intercept\",\n            dist.MultivariateNormal(\n                loc=jnp.zeros(X.shape[0]),\n                covariance_matrix=covariance_matrix_trend\n            )\n        )\n        seasonal_intercept =  numpyro.sample(\n            \"seasonal_intercept\",\n            dist.MultivariateNormal(\n                loc=jnp.zeros(X.shape[0]),\n                covariance_matrix=covariance_matrix_seasonal,\n            )\n        )\n\n        global_intercept = numpyro.deterministic(\n            \"global_intercept\", \n            trend_intercept + seasonal_intercept,\n        )\n        days_effect = numpyro.deterministic(\n            \"days_effect\", \n            jnp.dot(days, coefficients_days),\n        )\n\n        # Sample y from a MV Normal distribution of mean 0\n        # and covariance determined by the kernel function\n        numpyro.sample(\n            \"y\",\n            dist.Normal(\n                global_intercept + days_effect + effect[arm_indexes],\n                noise\n            ),\n            obs=y,\n        )\n\n    return get_model\n```\n:::\n\n\n# Simulating the data\n\n# Fitting Gaussian Process Models to A/B test data\n\n# Visualize the results\n\n# Conclusion\n\n# Hardware and Requirements\nHere you can find the hardware and python requirements used for building this post.\n\n::: {#a1c3ac2d .cell execution_count=18}\n``` {.python .cell-code}\n%watermark\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLast updated: 2025-03-31T08:17:37.059520+01:00\n\nPython implementation: CPython\nPython version       : 3.13.2\nIPython version      : 9.0.2\n\nCompiler    : Clang 18.1.8 \nOS          : Darwin\nRelease     : 24.3.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 14\nArchitecture: 64bit\n\n```\n:::\n:::\n\n\n::: {#8669fedb .cell execution_count=19}\n``` {.python .cell-code}\n%watermark --iversions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npandas    : 2.2.3\njax       : 0.5.2\nscipy     : 1.15.2\nnumpyro   : 0.18.0\nnumpy     : 2.2.4\nmatplotlib: 3.10.1\nseaborn   : 0.13.2\n\n```\n:::\n:::\n\n\n",
    "supporting": [
      "ab_test_gp_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}