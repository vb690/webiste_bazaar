{
  "hash": "3b8ff37802ff363334dc92183575a272",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Hail Risk Estimation and Classification\"\ntitle-block-banner-color: \"#5465ff\"\ntitle-block-banner: \"#E2FFFF\"\nabstract: \"This analysis illustrates how to estimate hourly hail risk in the US with county-level granularity. The analysis poses particular attention to the mitigation of issues arising from small samples.\"\nauthors: Valerio Bonometti\ndate: 23/12/2025\ncategories:\n  - JAX\n  - Numpyro\n  - Bayesian Statistics\n  - Quantile Regression\n  - Zero-inflated Regression\ntoc: true\ntoc-title: Table of Contents\ntoc-depth: 4\ntoc-expand: 4\nnumber-sections: true\nexecute:\n    freeze: auto\n    cache: True\nhtml:\n    code-fold: true\n    page-layout: full\n---\n\n\n\n![Observed Hail Events](results/gif_images/hail_events.gif){width=100%}\n\n\n## Request Description\n\nThis analysis was conducted following a request from DevCo Americas team, in particular Environmental Affairs and Project controls. The stakeholder wanted to know if it was possible to leverage a publicly available dataset from the [National Oceanic and Atmospheric Administration](https://www.noaa.gov) (i.e., NOAA) for creating risk categories for Hail Extreme events in the U.S. The request was to perform this classification exercise on the entire U.S. territory with county-level granularity.\n\n## Hypotheses to be investigated {#sec-hypotheses_investigated}\n\nThe analysis does not have specific hypotheses to investigate but rather poses some challenges that needs to be overcome while trying to perform the estimation tasks. Here an overview of said challenges:\n\n1. In order to estimate the level of risk for a certain county in the U.S. we first needed to obtain a reliable estimate of the probability of an extreme hail event to occur in a given county. \n\n2. Due to the nature of extreme hail events, calculating their probability cannot be done by simply evaluating the observed frequency of such events but requires to leverage specific statistical frameworks (e.g., [Extreme Value Theory](https://en.wikipedia.org/wiki/Extreme_value_theory)) (i.e., EVA) than can provide long term likelihood for such events (e.g., 100s of years.)\n\n3.  In order to obtain reliable estimates, EVA usually requires that a sufficiently large number of events have been observed. Unfortunately increasing the level of granularity (i.e., looking at county level) forces to work with small samples. Moreover, this challenge is even more pronounced in those states where the Extreme Hail Events are more rare which are also states where a correct risk assessment is more critical.\n\n\n# Methodology\n\nIn this section we will outline our methodology with a particular focus on the data used and the analyses conducted for overcoming the challenges outlined in section @sec-hypotheses_investigated.\n\n::: {#6b78bf9e .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\n%load_ext watermark\n\nfrom pathlib import Path\n\nimport os\nimport numpyro\n\nnumpyro.set_host_device_count(os.cpu_count())\n\nfrom IPython.display import Image\n\nfrom itertools import product\nfrom functools import partial\n\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\n\nfrom jax import vmap\n\nfrom typing import List, Tuple, Any, Callable, Dict\nfrom numpy.typing import ArrayLike\n\nfrom matplotlib.axes import Axes\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nSMALL_FONT_SIZE = 12\nMEDIUM_FONT_SIZE = 15\nBIGGER_FONT_SIZE = 18\n\nSINGLE_STATE = [\"florida\"]\n\n# hail alley\nMULTIPLE_STATES = [\n    \"texas\",\n    \"oklahoma\",\n    \"kansas\",\n    \"nebraska\",\n    \"colorado\",\n    \"missouri\",\n    \"iowa\",\n    # medium states\n    \"louisiana\",\n    \"mississippi\",\n    \"alabama\",\n    \"florida\",\n    # dry states\n    \"california\",\n    \"new mexico\",\n    \"arizona\",\n]\n\nLOWER_CUT_OFF_YEAR = 1999\nUPPER_CUT_OFF_YEAR = 2024\n\nSELECTED_ANALYSIS_STATES = SINGLE_STATE\n\nFLOAT_PRECISION = 2\nDEGREES_PRECISION = 1e-2\n\nTIME_COLUMN = \"begin_date_time\"\n\nCONTINUOUS_MODELLING_COLUMNS = [\n    \"year\",\n    \"month\",\n    \"hour\",\n    \"begin_lat\",\n    \"begin_lon\",\n    \"state\",\n    \"countyfp\",\n    \"magnitude\",\n]\n\nCOUNT_MODELLING_COLUMNS = [\n    \"year\",\n    \"month\",\n    \"hour\",\n    \"state\",\n    \"countyfp\",\n]\n\nNUMBER_ITERATIONS = 100\nNUMBER_PARTICLES = 1\n\nCONTINUOUS_TARGET = \"magnitude\"\nCOUNT_TARGET = \"number_events\"\n\nLAT_COVARIATES = \"begin_lat\"\nLON_COVARIATES = \"begin_lon\"\n\nYEAR_COVARIATES = \"year\"\nMONTH_COVARIATE = \"month\"\nHOUR_COVARIATE = \"hour\"\n\nCOUNTIES_INDEX = \"countyfp\"\nSTATE_INDEX = \"state\"\n\nCRITICAL_VALUE_CONTINUOUS_TARGET = 1.77\nCRITICAL_VALUE_COUNT_TARGET = 5\nCRITICAL_QUANTILE = 0.95\n\nCOLORMAP_NAME = \"Blues\"\nCOLORMAP = mpl.colormaps[COLORMAP_NAME].resampled(5)\n\nDATA_PATH = Path(\"local_data\")\nRESULTS_PATH = Path(\"results\")\nGIFS_PATH = Path(RESULTS_PATH, \"gif_images\")\nIMAGES_PATH = Path(RESULTS_PATH, \"images\")\n\nplt.rc('font', size=SMALL_FONT_SIZE) \nplt.rc('axes', titlesize=MEDIUM_FONT_SIZE)\nplt.rc('axes', labelsize=SMALL_FONT_SIZE)\nplt.rc('xtick', labelsize=SMALL_FONT_SIZE)\nplt.rc('ytick', labelsize=SMALL_FONT_SIZE)\nplt.rc('legend', fontsize=SMALL_FONT_SIZE)    \nplt.rc('figure', titlesize=BIGGER_FONT_SIZE)\nplt.rc('figure', dpi=100)\n```\n:::\n\n\n## Data Gathering and Data Description\n\nIn this paragraph we will provide an overview of the data employed in this analysis:\n\n1. The dataset from NOAA containing records of hail events.\n2. The [geo-dataframe](https://geopandas.org/en/stable/gallery/create_geopandas_from_pandas.html) used for mapping hail events onto U.S. counties.\n\n### NOAA Dataset\n\nThe dataset we used for conducting this analysis is the [Storm Event Database](https://www.ncdc.noaa.gov/stormevents/). This dataset contains records that documents:\n\n1. The occurrence of storms and other significant weather phenomena.\n2. Rare, unusual, weather phenomena\n3. Other significant meteorological events\n\nA common characteristic of these events is being so intense or exceptional so to cause potential damage to people, structure and causing disruption to commerce. The database\ncontains data from 1950 until 2024. However due to changes in the record, measurement and collection strategy the portion containing detailed and reliable information is limited to the interval 1996 - 2024. This is also the portion of the dataset that we have used for our analyses.\n\n### Counties Geometry Dataset\n\nIn order to to map and visualize our analysis onto the U.S. territory (and counties in particular) we obtained a geo-dataframe containing geometry files for each state in the U.S. This dataset was not used just for visualization purposes but it also played an important role in the estimation of extreme hail events at county level (more details will be given in section @sec-quantile_regression.) \n \n## Data Exploration\n\nIn this section we will outline a few key characteristics of hail events as a phenomena, putting particular attention on those characteristics that makes necessary the use of particular statistical framework for addressing the challenges we have outlined in @sec-hypotheses_investigated. Due to space constrains we will report here only the results for the state of Florida. \n\nThe state of Florida perfectly illustrates the challenges of estimating extreme hail events in liminal situations: in states like California and Texas the risk assessment becomes easier due to either the complete absence or the abundance of hail events. Florida on the other end seems to have a much more nuanced risk profile.\n\n### Data Preparation\n\nIn this section we will prepare the data for further analysis, mostly focusing on creating datasets that are suitable for the different types of statistical approaches we want to try.\n\n::: {#6e6d4c23 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\nimport numpy as np\n\nimport pandas as pd\nimport geopandas as gpd\n\nfrom pyextremes import get_extremes\n\n\ndef pad_dataset(\n    df: pd.DataFrame,\n    categorical_columns: List[str],\n    fill_value: float = 0.0,\n) -> pd.DataFrame:\n    \"\"\"Pad a dataset over all the dimensions defined by indexing columns filling the missing\n    entries with fill_value.\n    \"\"\"\n    new_index = list([df[column].unique() for column in categorical_columns])\n    new_index = product(*new_index)\n    df = df.set_index(keys=categorical_columns)\n    df = df.reindex(new_index).fillna(fill_value).reset_index()\n    return df\n\n\ndef make_grid(polygon: Any, edge_size: float) -> gpd.GeoSeries:\n    \"\"\"Create a grid of the size of polygon made of squares of size edge_size\"\"\"\n    bounds = polygon.bounds\n    x_coords = np.arange(bounds[0] + edge_size / 2, bounds[2], edge_size)\n    y_coords = np.arange(bounds[1] + edge_size / 2, bounds[3], edge_size)\n    combinations = np.array(list(product(x_coords, y_coords)))\n    squares = gpd.points_from_xy(combinations[:, 0], combinations[:, 1]).buffer(\n        edge_size / 1, cap_style=3\n    )\n    return gpd.GeoSeries(squares[squares.intersects(polygon)])\n\n\ndef generate_grid_df(\n    geometries_df: gpd.GeoDataFrame,\n    edge_size: float,\n    state: List[str] = SELECTED_ANALYSIS_STATES,\n) -> gpd.GeoDataFrame:\n    \"\"\"Generate a geod-dataframe made of giddified polygons\"\"\"\n    state_geometries_df = geometries_df[\n        geometries_df[STATE_INDEX].str.lower().isin([s for s in state])\n    ].copy()\n    grid_df = []\n    list_counties_fp = state_geometries_df[COUNTIES_INDEX].values\n    list_states_fp = state_geometries_df[STATE_INDEX].values\n    list_geometries = state_geometries_df[\"geometry\"].values\n    for countyfp, state, geometry in zip(\n        list_counties_fp, list_states_fp, list_geometries\n    ):\n\n        generated_grid = make_grid(geometry, edge_size=edge_size)\n        grid_df.append(\n            pd.DataFrame(\n                {\n                    COUNTIES_INDEX: countyfp,\n                    STATE_INDEX: state,\n                    \"geometry\": generated_grid,\n                }\n            )\n        )\n    grid_df = gpd.GeoDataFrame(pd.concat(grid_df))\n    grid_df[\"begin_lat\"] = grid_df[\"geometry\"].centroid.y.round(FLOAT_PRECISION)\n    grid_df[\"begin_lon\"] = grid_df[\"geometry\"].centroid.x.round(FLOAT_PRECISION)\n    return grid_df\n\n\ndef split_dataset(\n    df: pd.DataFrame,\n    ordering_columns: List[str],\n    split_fraction: float = 0.33,\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Split dataset in two portions\"\"\"\n    df = df.sort_values(ordering_columns)\n    cut_off = int(len(df) * split_fraction)\n    return df[:-cut_off].copy(), df[-cut_off:].copy()\n\n\ndef perform_dithering(\n    magnitudes: ArrayLike,\n    cap: float = 0.5,\n    dithering_coefficient: float = 0.147,\n    dithering_bias: float = 0.0279,\n) -> ArrayLike:\n    \"\"\"Perform dithering following the procedure in\n    https://www.columbia.edu/~mkt14/publications/MWR-final-hailsize.pdf\n    \"\"\"\n    dithering_amounts = dithering_bias + dithering_coefficient * magnitudes\n    dithering_amounts = np.clip(dithering_amounts, a_min=0.0, a_max=cap)\n    dithering_noise = np.random.uniform(-dithering_amounts, dithering_amounts)\n    dithered_magnitudes = np.clip(magnitudes + dithering_noise, 0, None)\n    return dithered_magnitudes\n\n\ndef make_time_series_continuous(\n    series: pd.Series, start: str, end: str, frequency: str\n) -> pd.Series:\n    \"\"\"Make a time series continuous and pad the missing values with zeros\"\"\"\n    new_index = pd.date_range(start=start, end=end, freq=frequency)\n    return series.reindex(new_index, fill_value=0)\n\n\ndef downscale_coordinates(\n    values: ArrayLike, precision: int, degree_resolution: float\n) -> ArrayLike:\n    \"\"\"Downscale latitude and longitude coordinate to a certain degree resolution.\"\"\"\n    return np.round(degree_resolution * np.round(values / degree_resolution), precision)\n\n\ndef create_covariates_df(\n    geometries_df: gpd.GeoDataFrame,\n    edge_size: float,\n    state: List[str],\n    months: List[int],\n    hours: List[int],\n    years: List[int],\n) -> gpd.GeoDataFrame:\n    \"\"\"Create a dataframe of covariates for inference\"\"\"\n    covariates_df = []\n    grid_df = generate_grid_df(\n        geometries_df=geometries_df,\n        edge_size=edge_size,\n        state=state,\n    )\n    for year, month, hour in list(product(years, months, hours)):\n\n        timed_grid_df = grid_df.copy()\n        timed_grid_df[\"year\"] = year\n        timed_grid_df[\"month\"] = month\n        timed_grid_df[\"hour\"] = hour\n\n        covariates_df.append(timed_grid_df)\n\n    covariates_df = pd.concat(covariates_df)\n    covariates_df = gpd.GeoDataFrame(covariates_df)\n    return covariates_df\n\n\ndef create_maxima_dataset(\n    df: pd.DataFrame, grouping_columns: List[str], value_column: str\n) -> pd.DataFrame:\n    \"\"\"Create the dataset for fitting gen-extreme data\"\"\"\n    maxima_df = df[grouping_columns + [value_column]].copy()\n    maxima_df = maxima_df.groupby(grouping_columns)[value_column].max().reset_index()\n    return maxima_df\n\n\ndef get_extremes_in_blocks(\n    maxima_df: pd.Series,\n    county_fp: str,\n    state_fp,\n    start: str,\n    end: str,\n) -> pd.DataFrame:\n    \"\"\"Extract the extreme values using either Point of Threshold or Block Maxima\"\"\"\n    maxima_df = make_time_series_continuous(\n        series=maxima_df,\n        start=start,\n        end=end,\n        frequency=\"1h\",\n    )\n    extremes = get_extremes(\n        ts=maxima_df,\n        method=\"BM\",\n    )\n    extremes = extremes.reset_index()\n    extremes[COUNTIES_INDEX] = county_fp\n    extremes[STATE_INDEX] = state_fp\n    return extremes\n\n\ndef create_eva_dataset(\n    modelling_df: pd.DataFrame, geometries_df: gpd.GeoDataFrame\n) -> pd.DataFrame:\n    \"\"\"Create a dataset used for Extreme Value Analysis\"\"\"\n    maxima_data = create_maxima_dataset(\n        df=modelling_df,\n        grouping_columns=[TIME_COLUMN, STATE_INDEX, COUNTIES_INDEX],\n        value_column=CONTINUOUS_TARGET,\n    )\n    grouped = maxima_data.groupby([COUNTIES_INDEX, STATE_INDEX])\n\n    list_df = [group.set_index(TIME_COLUMN)[CONTINUOUS_TARGET] for _, group in grouped]\n    list_category_names = grouped.groups.keys()\n\n    partialized_get_extremes_df = partial(\n        get_extremes_in_blocks,\n        start=maxima_data[TIME_COLUMN].min(),\n        end=maxima_data[TIME_COLUMN].max(),\n    )\n    eva_df = Parallel(n_jobs=-1)(\n        delayed(partialized_get_extremes_df)(data, county_fp, state_fp)\n        for data, (county_fp, state_fp) in zip(list_df, list_category_names)\n    )\n    eva_df: pd.DataFrame = pd.concat(eva_df)\n    eva_df[CONTINUOUS_TARGET] = eva_df[CONTINUOUS_TARGET].replace(\n        to_replace=0,\n        value=eva_df[CONTINUOUS_TARGET].mean(),\n    )\n    eva_df = eva_df.rename({\"date-time\": TIME_COLUMN}, axis=1)\n    eva_df = pd.merge(\n        eva_df,\n        geometries_df[[COUNTIES_INDEX, STATE_INDEX, \"geometry\"]].drop_duplicates(),\n        on=[COUNTIES_INDEX, STATE_INDEX],\n        how=\"inner\",\n    )\n    eva_df = pd.merge(\n        eva_df,\n        modelling_df[[\"county\", COUNTIES_INDEX, STATE_INDEX]].drop_duplicates(),\n        on=[COUNTIES_INDEX, STATE_INDEX],\n        how=\"inner\",\n    )\n    return eva_df\n\n\ndef create_continuous_dataset(\n    modelling_df: pd.DataFrame, geometries_df: gpd.GeoDataFrame\n) -> pd.DataFrame:\n    \"\"\"Create a dataset for continuous modelling\"\"\"\n    continuous_modelling_df = modelling_df[CONTINUOUS_MODELLING_COLUMNS + [\"begin_date_time\"]].copy()\n    continuous_modelling_df = (\n        continuous_modelling_df.groupby(\n            list(continuous_modelling_df.drop(CONTINUOUS_TARGET, axis=1))\n        )[CONTINUOUS_TARGET]\n        .max()\n        .reset_index()\n        .drop(\"begin_date_time\", axis=1)\n    )\n    continuous_modelling_df = pd.merge(\n        continuous_modelling_df,\n        geometries_df[[COUNTIES_INDEX, STATE_INDEX, \"geometry\"]],\n        on=[COUNTIES_INDEX, STATE_INDEX],\n        how=\"inner\",\n    )\n    return continuous_modelling_df\n\n\ndef create_count_dataset(\n    modelling_df: pd.DataFrame, geometries_df: gpd.GeoDataFrame\n) -> gpd.GeoDataFrame:\n    \"\"\"Create a dataset for count modelling\"\"\"\n    count_modelling_df = modelling_df[\n        [\n            \"county\",\n            \"begin_date_time\",\n            \"year\",\n            \"month\",\n            \"hour\",\n            \"state\",\n            COUNTIES_INDEX,\n        ]\n    ].copy()\n\n    count_modelling_df = (\n        count_modelling_df.groupby(\n            [\n                \"county\",\n                \"begin_date_time\",\n                \"year\",\n                \"month\",\n                \"hour\",\n                \"state\",\n                COUNTIES_INDEX,\n            ]\n        )\n        .size()\n        .reset_index()\n        .rename({0: COUNT_TARGET}, axis=1)\n        .groupby(COUNT_MODELLING_COLUMNS)[COUNT_TARGET]\n        .max()\n        .reset_index()\n    )\n    count_modelling_df = pad_dataset(\n        df=count_modelling_df,\n        categorical_columns=COUNT_MODELLING_COLUMNS,\n    )\n    count_modelling_df = pd.merge(\n        count_modelling_df,\n        geometries_df[[COUNTIES_INDEX, STATE_INDEX, \"geometry\"]],\n        on=[COUNTIES_INDEX, STATE_INDEX],\n        how=\"inner\",\n    )\n    return count_modelling_df\n\n\ndef create_all_datasets(\n    modelling_df: pd.DataFrame,\n    geometries_df: gpd.GeoDataFrame,\n    state: List[str] = SELECTED_ANALYSIS_STATES,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"Create all dataset required for the notebook\"\"\"\n    state_modelling_df = modelling_df[modelling_df[\"state\"].isin(state)].copy()\n    continuous_state_modelling_df = create_continuous_dataset(\n        modelling_df=state_modelling_df,\n        geometries_df=geometries_df,\n    )\n    count_state_modelling_df = create_count_dataset(\n        modelling_df=state_modelling_df,\n        geometries_df=geometries_df,\n    )\n    eva_state_modelling_df = create_eva_dataset(\n        modelling_df=state_modelling_df,\n        geometries_df=geometries_df,\n    )\n\n    return (\n        continuous_state_modelling_df,\n        count_state_modelling_df,\n        eva_state_modelling_df,\n    )\n```\n:::\n\n\nAs a first step we read and pre-process the data used for modelling in general\n\n::: {#f5703a8f .cell execution_count=4}\n``` {.python .cell-code}\nmodelling_df = pd.read_parquet(Path(DATA_PATH, \"modelling_df.parquet\"))\nmodelling_df[\"state\"] = modelling_df[\"state\"].apply(\n    lambda x: [i.lower() for i in x.split(\" \")]\n)\nmodelling_df[\"state\"] = modelling_df[\"state\"].apply(lambda x: \"_\".join(x))\nmodelling_df = modelling_df.rename({\"yearly\": \"year\"}, axis=1)\n\nmodelling_df[\"begin_lat\"] = modelling_df[\"begin_lat\"].values.round(FLOAT_PRECISION)\nmodelling_df[\"begin_lon\"] = modelling_df[\"begin_lon\"].values.round(FLOAT_PRECISION)\nmodelling_df[\"magnitude\"] = perform_dithering(\n    magnitudes=modelling_df[\"magnitude\"].values\n)\nmodelling_df\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>county</th>\n      <th>begin_lat</th>\n      <th>begin_lon</th>\n      <th>begin_date_time</th>\n      <th>countyfp</th>\n      <th>statefp</th>\n      <th>countyfp_nozero</th>\n      <th>magnitude</th>\n      <th>state</th>\n      <th>monthly</th>\n      <th>daily</th>\n      <th>year</th>\n      <th>month</th>\n      <th>hour</th>\n      <th>time_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>abbeville</td>\n      <td>34.09</td>\n      <td>-82.60</td>\n      <td>2009-04-10 19:00:00</td>\n      <td>001</td>\n      <td>45</td>\n      <td>1</td>\n      <td>1.491996</td>\n      <td>south_carolina</td>\n      <td>2009-04-06</td>\n      <td>2009-04-10</td>\n      <td>2009</td>\n      <td>4</td>\n      <td>19</td>\n      <td>116353</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>abbeville</td>\n      <td>34.09</td>\n      <td>-82.59</td>\n      <td>2010-03-28 19:00:00</td>\n      <td>001</td>\n      <td>45</td>\n      <td>1</td>\n      <td>0.933026</td>\n      <td>south_carolina</td>\n      <td>2010-03-02</td>\n      <td>2010-03-28</td>\n      <td>2010</td>\n      <td>3</td>\n      <td>19</td>\n      <td>124801</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>abbeville</td>\n      <td>34.09</td>\n      <td>-82.59</td>\n      <td>2010-04-25 01:00:00</td>\n      <td>001</td>\n      <td>45</td>\n      <td>1</td>\n      <td>2.002821</td>\n      <td>south_carolina</td>\n      <td>2010-04-01</td>\n      <td>2010-04-25</td>\n      <td>2010</td>\n      <td>4</td>\n      <td>1</td>\n      <td>125455</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>abbeville</td>\n      <td>34.10</td>\n      <td>-82.62</td>\n      <td>1998-06-09 16:00:00</td>\n      <td>001</td>\n      <td>45</td>\n      <td>1</td>\n      <td>1.143295</td>\n      <td>south_carolina</td>\n      <td>1998-06-03</td>\n      <td>1998-06-09</td>\n      <td>1998</td>\n      <td>6</td>\n      <td>16</td>\n      <td>21358</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>abbeville</td>\n      <td>34.10</td>\n      <td>-82.60</td>\n      <td>1997-04-22 14:00:00</td>\n      <td>001</td>\n      <td>45</td>\n      <td>1</td>\n      <td>0.810593</td>\n      <td>south_carolina</td>\n      <td>1997-04-09</td>\n      <td>1997-04-22</td>\n      <td>1997</td>\n      <td>4</td>\n      <td>14</td>\n      <td>11444</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>310393</th>\n      <td>ziebach</td>\n      <td>45.40</td>\n      <td>-101.86</td>\n      <td>2013-07-23 18:00:00</td>\n      <td>137</td>\n      <td>46</td>\n      <td>137</td>\n      <td>1.811021</td>\n      <td>south_dakota</td>\n      <td>2013-07-14</td>\n      <td>2013-07-23</td>\n      <td>2013</td>\n      <td>7</td>\n      <td>18</td>\n      <td>153912</td>\n    </tr>\n    <tr>\n      <th>310394</th>\n      <td>ziebach</td>\n      <td>45.42</td>\n      <td>-101.81</td>\n      <td>2019-09-29 22:00:00</td>\n      <td>137</td>\n      <td>46</td>\n      <td>137</td>\n      <td>0.906396</td>\n      <td>south_dakota</td>\n      <td>2019-09-11</td>\n      <td>2019-09-29</td>\n      <td>2019</td>\n      <td>9</td>\n      <td>22</td>\n      <td>208132</td>\n    </tr>\n    <tr>\n      <th>310395</th>\n      <td>ziebach</td>\n      <td>45.47</td>\n      <td>-101.65</td>\n      <td>2003-07-03 22:00:00</td>\n      <td>137</td>\n      <td>46</td>\n      <td>137</td>\n      <td>0.675737</td>\n      <td>south_dakota</td>\n      <td>2003-06-07</td>\n      <td>2003-07-03</td>\n      <td>2003</td>\n      <td>7</td>\n      <td>22</td>\n      <td>65764</td>\n    </tr>\n    <tr>\n      <th>310396</th>\n      <td>ziebach</td>\n      <td>45.47</td>\n      <td>-101.65</td>\n      <td>2008-07-18 18:00:00</td>\n      <td>137</td>\n      <td>46</td>\n      <td>137</td>\n      <td>1.624055</td>\n      <td>south_dakota</td>\n      <td>2008-07-10</td>\n      <td>2008-07-18</td>\n      <td>2008</td>\n      <td>7</td>\n      <td>18</td>\n      <td>109968</td>\n    </tr>\n    <tr>\n      <th>310397</th>\n      <td>ziebach</td>\n      <td>45.47</td>\n      <td>-101.65</td>\n      <td>2008-07-23 15:00:00</td>\n      <td>137</td>\n      <td>46</td>\n      <td>137</td>\n      <td>1.665959</td>\n      <td>south_dakota</td>\n      <td>2008-07-10</td>\n      <td>2008-07-23</td>\n      <td>2008</td>\n      <td>7</td>\n      <td>15</td>\n      <td>110085</td>\n    </tr>\n  </tbody>\n</table>\n<p>310398 rows × 15 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {#294cc547 .cell execution_count=5}\n``` {.python .cell-code}\nmodelling_df.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 310398 entries, 0 to 310397\nData columns (total 15 columns):\n #   Column           Non-Null Count   Dtype         \n---  ------           --------------   -----         \n 0   county           310398 non-null  object        \n 1   begin_lat        310398 non-null  float64       \n 2   begin_lon        310398 non-null  float64       \n 3   begin_date_time  310398 non-null  datetime64[ns]\n 4   countyfp         310398 non-null  object        \n 5   statefp          310398 non-null  object        \n 6   countyfp_nozero  310398 non-null  object        \n 7   magnitude        310398 non-null  float64       \n 8   state            310398 non-null  object        \n 9   monthly          310398 non-null  datetime64[ns]\n 10  daily            310398 non-null  datetime64[ns]\n 11  year             310398 non-null  int32         \n 12  month            310398 non-null  int32         \n 13  hour             310398 non-null  int32         \n 14  time_index       310398 non-null  int64         \ndtypes: datetime64[ns](3), float64(3), int32(3), int64(1), object(5)\nmemory usage: 32.0+ MB\n```\n:::\n:::\n\n\nAs well as the geo-data used for visualizing our results spatially\n\n::: {#61e69552 .cell execution_count=6}\n``` {.python .cell-code}\ngeometries_df = gpd.read_file(filename=Path(DATA_PATH, \"us_counties_df.geojson\"))\ngeometries_df = geometries_df.rename({\"state_name\": \"state\"}, axis=1)\ngeometries_df[\"state\"] = geometries_df[\"state\"].apply(\n    lambda x: [i.lower() for i in x.split(\" \")]\n)\ngeometries_df[\"state\"] = geometries_df[\"state\"].apply(lambda x: \"_\".join(x))\ngeometries_df\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>geo_point_2d</th>\n      <th>statefp</th>\n      <th>countyfp</th>\n      <th>countyns</th>\n      <th>geoid</th>\n      <th>name</th>\n      <th>namelsad</th>\n      <th>stusab</th>\n      <th>lsad</th>\n      <th>classfp</th>\n      <th>...</th>\n      <th>cbsafp</th>\n      <th>metdivfp</th>\n      <th>funcstat</th>\n      <th>aland</th>\n      <th>awater</th>\n      <th>intptlat</th>\n      <th>intptlon</th>\n      <th>state</th>\n      <th>countyfp_nozero</th>\n      <th>geometry</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>{ \"lon\": -96.615073673300003, \"lat\": 28.439109...</td>\n      <td>48</td>\n      <td>057</td>\n      <td>01383814</td>\n      <td>48057</td>\n      <td>Calhoun</td>\n      <td>Calhoun County</td>\n      <td>TX</td>\n      <td>06</td>\n      <td>H1</td>\n      <td>...</td>\n      <td>38920</td>\n      <td>None</td>\n      <td>A</td>\n      <td>1312707005</td>\n      <td>1361884774</td>\n      <td>+28.4417191</td>\n      <td>-096.5795739</td>\n      <td>texas</td>\n      <td>57</td>\n      <td>POLYGON ((-96.87329 28.62291, -96.87148 28.624...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>{ \"lon\": -86.190418669300001, \"lat\": 36.751316...</td>\n      <td>21</td>\n      <td>003</td>\n      <td>00516848</td>\n      <td>21003</td>\n      <td>Allen</td>\n      <td>Allen County</td>\n      <td>KY</td>\n      <td>06</td>\n      <td>H1</td>\n      <td>...</td>\n      <td>14540</td>\n      <td>None</td>\n      <td>A</td>\n      <td>891838779</td>\n      <td>19482100</td>\n      <td>+36.7507703</td>\n      <td>-086.1924580</td>\n      <td>kentucky</td>\n      <td>3</td>\n      <td>POLYGON ((-86.2958 36.85107, -86.29347 36.8526...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>{ \"lon\": -97.721324086699994, \"lat\": 48.369456...</td>\n      <td>38</td>\n      <td>099</td>\n      <td>01034214</td>\n      <td>38099</td>\n      <td>Walsh</td>\n      <td>Walsh County</td>\n      <td>ND</td>\n      <td>06</td>\n      <td>H1</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>A</td>\n      <td>3319346396</td>\n      <td>32181391</td>\n      <td>+48.3769789</td>\n      <td>-097.7222304</td>\n      <td>north_dakota</td>\n      <td>99</td>\n      <td>POLYGON ((-98.29185 48.36969, -98.29211 48.369...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>{ \"lon\": -84.649203742099999, \"lat\": 36.135024...</td>\n      <td>47</td>\n      <td>129</td>\n      <td>01639778</td>\n      <td>47129</td>\n      <td>Morgan</td>\n      <td>Morgan County</td>\n      <td>TN</td>\n      <td>06</td>\n      <td>H1</td>\n      <td>...</td>\n      <td>28940</td>\n      <td>None</td>\n      <td>A</td>\n      <td>1352439675</td>\n      <td>823018</td>\n      <td>+36.1386970</td>\n      <td>-084.6392616</td>\n      <td>tennessee</td>\n      <td>129</td>\n      <td>POLYGON ((-84.79101 36.05853, -84.79184 36.059...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>{ \"lon\": -105.367471778, \"lat\": 38.10867790150...</td>\n      <td>08</td>\n      <td>027</td>\n      <td>00198129</td>\n      <td>08027</td>\n      <td>Custer</td>\n      <td>Custer County</td>\n      <td>CO</td>\n      <td>06</td>\n      <td>H1</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>A</td>\n      <td>1913031921</td>\n      <td>3364150</td>\n      <td>+38.1019955</td>\n      <td>-105.3735123</td>\n      <td>colorado</td>\n      <td>27</td>\n      <td>POLYGON ((-105.7969 38.26505, -105.78341 38.26...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3228</th>\n      <td>{ \"lon\": -89.823002441400007, \"lat\": 31.569641...</td>\n      <td>28</td>\n      <td>065</td>\n      <td>00695756</td>\n      <td>28065</td>\n      <td>Jefferson Davis</td>\n      <td>Jefferson Davis County</td>\n      <td>MS</td>\n      <td>06</td>\n      <td>H1</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>A</td>\n      <td>1057871313</td>\n      <td>1791496</td>\n      <td>+31.5648075</td>\n      <td>-089.8270863</td>\n      <td>mississippi</td>\n      <td>65</td>\n      <td>POLYGON ((-89.97537 31.59155, -89.97535 31.592...</td>\n    </tr>\n    <tr>\n      <th>3229</th>\n      <td>{ \"lon\": -89.414372033899994, \"lat\": 35.197080...</td>\n      <td>47</td>\n      <td>047</td>\n      <td>01639742</td>\n      <td>47047</td>\n      <td>Fayette</td>\n      <td>Fayette County</td>\n      <td>TN</td>\n      <td>06</td>\n      <td>H1</td>\n      <td>...</td>\n      <td>32820</td>\n      <td>None</td>\n      <td>A</td>\n      <td>1825359642</td>\n      <td>3774635</td>\n      <td>+35.1969933</td>\n      <td>-089.4138027</td>\n      <td>tennessee</td>\n      <td>47</td>\n      <td>POLYGON ((-89.63773 35.17934, -89.63768 35.181...</td>\n    </tr>\n    <tr>\n      <th>3230</th>\n      <td>{ \"lon\": -97.891896839799998, \"lat\": 42.636781...</td>\n      <td>31</td>\n      <td>107</td>\n      <td>00835875</td>\n      <td>31107</td>\n      <td>Knox</td>\n      <td>Knox County</td>\n      <td>NE</td>\n      <td>06</td>\n      <td>H1</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>A</td>\n      <td>2870854403</td>\n      <td>81011709</td>\n      <td>+42.6344045</td>\n      <td>-097.8913492</td>\n      <td>nebraska</td>\n      <td>107</td>\n      <td>POLYGON ((-97.60303 42.85796, -97.60294 42.857...</td>\n    </tr>\n    <tr>\n      <th>3231</th>\n      <td>{ \"lon\": -123.098321728, \"lat\": 45.56009084080...</td>\n      <td>41</td>\n      <td>067</td>\n      <td>01155137</td>\n      <td>41067</td>\n      <td>Washington</td>\n      <td>Washington County</td>\n      <td>OR</td>\n      <td>06</td>\n      <td>H1</td>\n      <td>...</td>\n      <td>38900</td>\n      <td>None</td>\n      <td>A</td>\n      <td>1875859540</td>\n      <td>6114246</td>\n      <td>+45.5535419</td>\n      <td>-123.0976146</td>\n      <td>oregon</td>\n      <td>67</td>\n      <td>POLYGON ((-123.20926 45.43371, -123.20976 45.4...</td>\n    </tr>\n    <tr>\n      <th>3232</th>\n      <td>{ \"lon\": -72.713798537399995, \"lat\": 42.990603...</td>\n      <td>50</td>\n      <td>025</td>\n      <td>01461769</td>\n      <td>50025</td>\n      <td>Windham</td>\n      <td>Windham County</td>\n      <td>VT</td>\n      <td>06</td>\n      <td>H1</td>\n      <td>...</td>\n      <td>None</td>\n      <td>None</td>\n      <td>A</td>\n      <td>2034457838</td>\n      <td>32920750</td>\n      <td>+42.9953348</td>\n      <td>-072.7219550</td>\n      <td>vermont</td>\n      <td>25</td>\n      <td>POLYGON ((-72.86874 43.11317, -72.86803 43.125...</td>\n    </tr>\n  </tbody>\n</table>\n<p>3233 rows × 22 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {#1213d7eb .cell execution_count=7}\n``` {.python .cell-code}\ngeometries_df.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'geopandas.geodataframe.GeoDataFrame'>\nRangeIndex: 3233 entries, 0 to 3232\nData columns (total 22 columns):\n #   Column           Non-Null Count  Dtype   \n---  ------           --------------  -----   \n 0   geo_point_2d     3233 non-null   object  \n 1   statefp          3233 non-null   object  \n 2   countyfp         3233 non-null   object  \n 3   countyns         3233 non-null   object  \n 4   geoid            3233 non-null   object  \n 5   name             3233 non-null   object  \n 6   namelsad         3233 non-null   object  \n 7   stusab           3233 non-null   object  \n 8   lsad             3233 non-null   object  \n 9   classfp          3233 non-null   object  \n 10  mtfcc            3233 non-null   object  \n 11  csafp            1255 non-null   object  \n 12  cbsafp           1915 non-null   object  \n 13  metdivfp         110 non-null    object  \n 14  funcstat         3233 non-null   object  \n 15  aland            3233 non-null   int64   \n 16  awater           3233 non-null   int64   \n 17  intptlat         3233 non-null   object  \n 18  intptlon         3233 non-null   object  \n 19  state            3233 non-null   object  \n 20  countyfp_nozero  3233 non-null   object  \n 21  geometry         3233 non-null   geometry\ndtypes: geometry(1), int64(2), object(19)\nmemory usage: 555.8+ KB\n```\n:::\n:::\n\n\nWe then proceed at creating 3 datasets to be used by the 3 different statistical approaches we are going to adopt:\n\n::: {#45ab3a94 .cell execution_count=8}\n``` {.python .cell-code}\ncontinuous_state_modelling_df, count_state_modelling_df, eva_state_modelling_df = (\n    create_all_datasets(\n        modelling_df=modelling_df[\n            (modelling_df[\"year\"] >= LOWER_CUT_OFF_YEAR)\n            & (modelling_df[\"year\"] <= UPPER_CUT_OFF_YEAR)\n        ],\n        geometries_df=geometries_df,\n        state=SELECTED_ANALYSIS_STATES,\n    )\n)\n```\n:::\n\n\n1. One for extreme value analysis.\n\n::: {#8de0bbc3 .cell execution_count=9}\n``` {.python .cell-code}\neva_state_modelling_df.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1768 entries, 0 to 1767\nData columns (total 6 columns):\n #   Column           Non-Null Count  Dtype         \n---  ------           --------------  -----         \n 0   begin_date_time  1768 non-null   datetime64[ns]\n 1   magnitude        1768 non-null   float64       \n 2   countyfp         1768 non-null   object        \n 3   state            1768 non-null   object        \n 4   geometry         1768 non-null   geometry      \n 5   county           1768 non-null   object        \ndtypes: datetime64[ns](1), float64(1), geometry(1), object(3)\nmemory usage: 83.0+ KB\n```\n:::\n:::\n\n\n2. One with a continuous target (i.e., hail magnitude) for the quantile regression.\n\n::: {#c5c36265 .cell execution_count=10}\n``` {.python .cell-code}\ncontinuous_state_modelling_df.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3813 entries, 0 to 3812\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype   \n---  ------     --------------  -----   \n 0   year       3813 non-null   int32   \n 1   month      3813 non-null   int32   \n 2   hour       3813 non-null   int32   \n 3   begin_lat  3813 non-null   float64 \n 4   begin_lon  3813 non-null   float64 \n 5   state      3813 non-null   object  \n 6   countyfp   3813 non-null   object  \n 7   magnitude  3813 non-null   float64 \n 8   geometry   3813 non-null   geometry\ndtypes: float64(3), geometry(1), int32(3), object(2)\nmemory usage: 223.5+ KB\n```\n:::\n:::\n\n\n3. One with a count target (i.e, number of hail events) for the zero-inflated negative binomial regression.\n\n::: {#8776a89f .cell execution_count=11}\n``` {.python .cell-code}\ncount_state_modelling_df.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 501696 entries, 0 to 501695\nData columns (total 7 columns):\n #   Column         Non-Null Count   Dtype   \n---  ------         --------------   -----   \n 0   year           501696 non-null  int32   \n 1   month          501696 non-null  int32   \n 2   hour           501696 non-null  int32   \n 3   state          501696 non-null  object  \n 4   countyfp       501696 non-null  object  \n 5   number_events  501696 non-null  float64 \n 6   geometry       501696 non-null  geometry\ndtypes: float64(1), geometry(1), int32(3), object(2)\nmemory usage: 21.1+ MB\n```\n:::\n:::\n\n\n### Visualizing Hail Events\n\n::: {#c05b304f .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\nimport gif\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\n\ndef plot_distribution_hail(\n    continuous_state_modelling_df: pd.DataFrame,\n    x: str,\n    ax: plt.Axes,\n    states: List[str] = SELECTED_ANALYSIS_STATES,\n    **kwargs: Any,\n) -> plt.Axes:\n    \"\"\"Plot the distribution of hail magnitude\"\"\"\n    title_states = \"\\n\".join([state.capitalize() for state in states])\n    sns.histplot(data=continuous_state_modelling_df, x=x, ax=ax, **kwargs)\n    ax.grid(alpha=0.5)\n    ax.set_title(f\"{title_states}\\nHourly Hail Events\")\n    return ax\n\n\ndef plot_hail_distribution_with_tails(\n    magnitude_events: ArrayLike, state: str, threshold=CRITICAL_VALUE_CONTINUOUS_TARGET\n) -> Tuple[plt.Figure, plt.Axes, plt.Axes]:\n    \"\"\"Plot a distribution of hail events magnitude with a focus on the tail.\"\"\"\n    fig, ax = plt.subplots(1, 1, figsize=[8, 5])\n    sub_ax = inset_axes(\n        ax,\n        width=2,\n        height=2,\n        loc=\"center right\",\n    )\n    sns.histplot(\n        data=magnitude_events,\n        bins=8,\n        ax=ax,\n        alpha=1,\n    )\n    sns.histplot(\n        data=magnitude_events[magnitude_events > threshold],\n        bins=8,\n        ax=sub_ax,\n        alpha=1,\n    )\n    ax.set_title(f\"Distribution Hail Events\\n{state.capitalize()}\")\n    ax.set_xlabel(\"Hail Size\")\n\n    sub_ax.set_title(f\"Extreme Hail Events\\n{state.capitalize()}\")\n    sub_ax.set_xlabel(\"Hail Size\")\n\n    ax.axvline(\n        1.77,\n        linestyle=\"--\",\n        c=\"r\",\n        linewidth=5,\n        label=\"Critical Value\",\n    )\n    ax.grid(alpha=0.5, zorder=0)\n    ax.legend()\n    return fig, ax, sub_ax\n\n\ndef plot_hail_events_on_map(\n    geometries_df: gpd.GeoDataFrame,\n    state_modelling_df: pd.DataFrame,\n    selected_state: str = SELECTED_ANALYSIS_STATES,\n    threshold: float = CRITICAL_VALUE_CONTINUOUS_TARGET,\n    extreme_threshold: float = 2.5,\n    extreme_events_size: float = 25,\n    **scatter_kwargs: Any,\n):\n    \"\"\"Visualize the hail events on a map with highlight of above-threshold event\"\"\"\n    fig, axs = plt.subplots(1, 2, figsize=(10, 10), sharey=True, sharex=True)\n\n    (\n        geometries_df[geometries_df[STATE_INDEX] == selected_state].boundary.plot(\n            ax=axs[0], color=\"k\", linewidth=0.5\n        )\n    )\n    (\n        geometries_df[geometries_df[STATE_INDEX] == selected_state].boundary.plot(\n            ax=axs[1], color=\"k\", linewidth=0.5\n        )\n    )\n    axs[0].scatter(\n        state_modelling_df[\"begin_lon\"].values,\n        state_modelling_df[\"begin_lat\"].values,\n        c=state_modelling_df[CONTINUOUS_TARGET].values,\n        vmin=0,\n        vmax=4.5,\n        cmap=COLORMAP_NAME,\n        **scatter_kwargs,\n    )\n    axs[1].scatter(\n        state_modelling_df[\n            (state_modelling_df[CONTINUOUS_TARGET] > threshold)\n            & (state_modelling_df[CONTINUOUS_TARGET] < extreme_threshold)\n        ][\"begin_lon\"].values,\n        state_modelling_df[\n            (state_modelling_df[CONTINUOUS_TARGET] > threshold)\n            & (state_modelling_df[CONTINUOUS_TARGET] < extreme_threshold)\n        ][\"begin_lat\"].values,\n        c=state_modelling_df[\n            (state_modelling_df[CONTINUOUS_TARGET] > threshold)\n            & (state_modelling_df[CONTINUOUS_TARGET] < extreme_threshold)\n        ][CONTINUOUS_TARGET].values,\n        vmin=0,\n        vmax=5,\n        cmap=COLORMAP_NAME,\n        **scatter_kwargs,\n    )\n    axs[1].scatter(\n        state_modelling_df[\n            (state_modelling_df[CONTINUOUS_TARGET] >= extreme_threshold)\n        ][\"begin_lon\"].values,\n        state_modelling_df[\n            (state_modelling_df[CONTINUOUS_TARGET] >= extreme_threshold)\n        ][\"begin_lat\"].values,\n        c=\"r\",\n        s=extreme_events_size,\n    )\n\n    for ax in axs:\n\n        ax.grid(alpha=0.5)\n\n    axs[0].set_title(f\"Hail Events\\n{selected_state.capitalize()} Counties\")\n    axs[1].set_title(f\"Extreme Hail Events\\n{selected_state.capitalize()} Counties\")\n    axs[0].set_xlabel(\"Longitude\")\n    axs[1].set_xlabel(\"Longitude\")\n    axs[0].set_ylabel(\"Latitude\")\n    axs[1].set_title(f\"Extreme Hail Events\\n{selected_state.capitalize()} Counties\")\n    return fig, axs\n\n\ndef plot_quantile_hail_time(\n    time_column: str,\n    y: str,\n    modelling_df: pd.DataFrame,\n    ax: plt.Axes,\n    quantile: float,\n    states: List[str] = SELECTED_ANALYSIS_STATES,\n    **kwargs: Any,\n) -> plt.Axes:\n    \"\"\"Plot the average hail attribute over a certain categorical columns\"\"\"\n    title_states = \"\\n\".join([state.capitalize() for state in states])\n    sns.lineplot(\n        data=modelling_df,\n        x=time_column,\n        y=y,\n        ax=ax,\n        estimator=partial(np.percentile, q=quantile),\n        n_boot=100,\n        **kwargs,\n    )\n    ax.grid(alpha=0.5)\n    ax.set_ylim(0, modelling_df[y].max())\n    ax.set_ylabel(y.capitalize())\n    ax.set_xlabel(time_column.capitalize())\n    ax.set_title(f\"{title_states}\\nQuantile {quantile} {y.capitalize()}\")\n    return ax\n\n\ndef plot_quantile_hail_geometry(\n    modelling_df: pd.DataFrame,\n    color_column: str,\n    max_value: float,\n    ax: Axes,\n    quantile: float = 0.5,\n    states: List[str] = SELECTED_ANALYSIS_STATES,\n) -> Axes:\n    \"\"\"Plot quantile of hail attribute over geometries\"\"\"\n    title_states = \"\\n\".join([state.capitalize() for state in states])\n    (\n        gpd.GeoDataFrame(\n            modelling_df.groupby(\"geometry\")[color_column]\n            .quantile(quantile)\n            .reset_index()\n        ).plot(\n            color_column,\n            cmap=COLORMAP_NAME,\n            ax=ax,\n            legend=True,\n            vmin=0.0,\n            vmax=max_value,\n            legend_kwds={\"shrink\": 0.7},\n        )\n    )\n    ax.grid(alpha=0.5)\n    ax.set_title(\n        f\"{title_states}\\nQuantile {quantile * 100} {color_column.capitalize()}\"\n    )\n    ax.set_ylabel(\"Longitude\")\n    ax.set_xlabel(\"Latitude\")\n    return ax\n\n\n@gif.frame\ndef plot_comparison_quantile_hail_geometry(\n    quantile: float,\n    continuous_state_modelling_df: gpd.GeoDataFrame,\n    count_state_modelling_df: gpd.GeoDataFrame,\n    states: List[str] = SELECTED_ANALYSIS_STATES,\n) -> None:\n    \"\"\"Compare hail magnitude and hail quantity over geometries\"\"\"\n    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n    for ax, df, color_column in zip(\n        axs,\n        [continuous_state_modelling_df, count_state_modelling_df],\n        [CONTINUOUS_TARGET, COUNT_TARGET],\n    ):\n\n        ax = plot_quantile_hail_geometry(\n            modelling_df=df,\n            color_column=color_column,\n            ax=ax,\n            quantile=quantile,\n            max_value=df[color_column].max(),\n            states=states,\n        )\n\n    plt.tight_layout()\n\n\n@gif.frame\ndef plot_comparison_quantile_hail_time(\n    y: str,\n    modelling_df: gpd.GeoDataFrame,\n    quantile: float,\n    color: Any,\n    states: List[str] = SELECTED_ANALYSIS_STATES,\n    **kwargs: Any,\n) -> None:\n    \"\"\"Compare average hail attribute over hour and month\"\"\"\n    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n    for ax, time_indicator in zip(axs, [\"hour\", \"month\", \"year\"]):\n\n        ax = plot_quantile_hail_time(\n            modelling_df=modelling_df,\n            time_column=time_indicator,\n            y=y,\n            ax=ax,\n            quantile=quantile,\n            states=states,\n            color=color,\n            **kwargs,\n        )\n\n    plt.tight_layout()\n```\n:::\n\n\nAs first thing we want to have a sense of how the hail events distributed spatially so we proceed at visualizing the entire history of hail events on Florida.\n\n::: {#67e6acff .cell execution_count=13}\n``` {.python .cell-code}\nfig, axs = plot_hail_events_on_map(\n    geometries_df=geometries_df,\n    state_modelling_df=continuous_state_modelling_df,\n    selected_state=SELECTED_ANALYSIS_STATES[0],\n    threshold=1.7,\n    s=1.5,\n    extreme_events_size=10,\n)\nfig.subplots_adjust(top=1.33)\nplt.tight_layout()\nplt.savefig(Path(IMAGES_PATH, \"hail_events_comparison_total.png\"))\nplt.close(\"all\")\n```\n:::\n\n\n![Comparing Normal and Extreme Hail Events](results/images/hail_events_comparison_total.png){#fig-hail_comparison_total}\n\n@fig-hail_comparison_total shows on the left side all the hail events coloured according to the estimated hail size of the event. On the right we can see a filtered version of the left panel where only events with estimated hail size greater than 1.7\" are shown, in particular we have highlighted in red those events with hail size greater than 2.5\" (i.e., exceptionally large hail events).\n\n::: {#f1c79113 .cell execution_count=14}\n``` {.python .cell-code}\npartialized_plot_comparison_quantile_hail_geometry = partial(\n    plot_comparison_quantile_hail_geometry,\n    continuous_state_modelling_df=continuous_state_modelling_df,\n    count_state_modelling_df=count_state_modelling_df[\n        count_state_modelling_df[\"number_events\"] > 0\n    ],\n    states=SELECTED_ANALYSIS_STATES,\n)\nquantiles = [.25, .75, .95, .99]\nframes = [partialized_plot_comparison_quantile_hail_geometry(quantile=quantile) for index, quantile in enumerate(quantiles)]\ngif.save(frames, Path(GIFS_PATH, 'spatial_map_quantiles.gif').as_posix(), duration=1000)\n```\n:::\n\n\n![Observed Spatial Quantiles](results/gif_images/spatial_map_quantiles.gif){#fig-sample_size}\n\nFrom @fig-hail_comparison_total and @fig-sample_size can already have an intuition of various characteristics of hail events\n\n1. They are not evenly distributed in space, there are areas which don't see events at all. @fig-sample_size shows the percentiles for both hail size and hail events across counties. Darker colors indicate higher hail size and number of hail events.\n\n2. Very large hail events are very rare and exceptionally large hail events are even more rare.\n\n3. Very large hail events can occur even in areas where hail per-se is a very rare event. @fig-sample_size shows three different type of percentile values of hail size for the different counties: 50%, 95% and 99% as we can see some of the most extreme values occur in area where there has been very few hail events.\n\n4. We can notice that there is spatial coherence in how the hail events distribute on the territory of Florida (possibly reflecting geographical characteristics of the various areas) \n\n::: {#065242d7 .cell execution_count=15}\n``` {.python .cell-code}\npartialized_plot_comparison_quantile_hail_time = partial(\n    plot_comparison_quantile_hail_time,\n    y=COUNT_TARGET,\n    modelling_df=count_state_modelling_df[\n        count_state_modelling_df[\"number_events\"] > 0\n    ],\n    states=SELECTED_ANALYSIS_STATES,\n    linewidth=3,\n    solid_capstyle='round'\n)\nquantiles = [25, 75, 95, 99]\nframes = [partialized_plot_comparison_quantile_hail_time(quantile=quantile, color=COLORMAP(index + 1)) for index, quantile in enumerate(quantiles)]\ngif.save(frames, Path(GIFS_PATH, 'temporal_quantiles.gif').as_posix(), duration=1000)\n```\n:::\n\n\n![Observed Temporal Quantiles](results/gif_images/temporal_quantiles.gif){#fig-temporal_quantiles}\n\n5. From @fig-temporal_quantiles we can also observe the presence of seasonal patterns and trending behaviour that changes when considering different quantiles for both hail sizes and number of hail events.\n\n![Comparing Daily Empirical Probability of Hail Events by Magnitude](results/images/hail_magnitudes_daily_probability.png){#fig-hail_magnitude_daily_probability}\n\n@fig-hail_magnitude_daily_probability shows in more detail how the more an event is severe, in terms of hail size (i.e., disruptive), the lower is its daily probability of occurring. I particular we can see the sharp drop from moderate (1.25\" to 1.75\") to severe (greater than 1.77\") hail storm.\n\nWhat this tells us is that hail storms, in particular those which have potential of being disruptive or dangerous, are better framed in terms of extreme events. In the next section we will see how these events require to be treated with particular care.\n\n### Visualizing The characteristics of Extreme Hail Events\n\nWe now want to have a better understanding of the statistical characteristics of hail events which are extreme in size, meaning they are above 1.77\"\n\n::: {#4f1d8de4 .cell execution_count=16}\n``` {.python .cell-code}\nfig, ax, sub_ax = plot_hail_distribution_with_tails(\n    magnitude_events=continuous_state_modelling_df[CONTINUOUS_TARGET].values,\n    state=SELECTED_ANALYSIS_STATES[0],\n)\nplt.savefig(Path(IMAGES_PATH, \"events_distribution.png\"))\n```\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-16-output-1.png){}\n:::\n:::\n\n\n![Visualizing the Distribution of Hail Events](results/images/events_distribution.png){#fig-distribution_hail_events}\n\n@fig-distribution_hail_events shows the distribution of the hail events with a red line indicating the critical value of 1.77\". We can observe a series of characteristics typical of hail events:\n\n1.  Most of the mass of the distribution lies around the 1\" mark indicating that the most common hail events are usually innocuous. We need to note that from @fig-distribution_hail_events we removed all the days in which there were no hail events. If not we would have observed the typical pattern of a [zero-inflated distribution](https://en.wikipedia.org/wiki/Zero-inflated_model).\n\n2. The distribution is left skewed with a relatively long and heavy right tail. This indicates that hail event of disruptive force can appear with a non-negligible probability but are in general very rare. \n\n3. Focusing on the extreme hail events (i.e., above the 1.77\" mark) we can see the distribution is again heavily skewed on the left suggesting that truly disastrous hail events (i.e., size above the 2.5\") can be very hard to predict.\n\n## Analyses Conducted\n\nIn this section we will outline the methodology we adopted for estimating hail size and hail risk at the county level.\n\n## Estimating Return Periods using Extreme Value Analysis\n\n::: {#2889e06d .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\nfrom scipy.stats import rankdata\nfrom scipy.stats import genextreme\n\nfrom pyextremes import get_extremes\nfrom pyextremes import EVA\n\n\ndef compute_empirical_return_periods(hail_events_extremes: ArrayLike) -> ArrayLike:\n    \"\"\"Compute the return period empirically from the data\"\"\"\n    ranks = rankdata(a=-hail_events_extremes)\n    excedance = ranks / (len(hail_events_extremes) + 1)\n    periods = 1 / excedance\n    return periods\n\n\ndef compute_mle_return_periods_scipy(\n    hail_events_extremes: pd.Series, max_years: int = 150, min_years: float = 2\n) -> List[float]:\n    \"\"\"Estimate the parameters of a genextreme distribution and compute the return periods using\n    the genextreme parameters estimated using scipy.\n    \"\"\"\n    c, loc, scale = genextreme.fit(hail_events_extremes)\n    periods = []\n    for year in np.arange(min_years, max_years):\n\n        periods.append(genextreme.ppf(1 - 1 / year, c, loc=loc, scale=scale))\n\n    return periods\n\n\ndef compute_mle_return_periods_pyextremes(\n    hail_events_extremes: pd.Series, max_years: int = 150\n) -> Tuple[ArrayLike, Any]:\n    \"\"\"Estimate the parameters of a genextreme distribution and compute the return periods using\n    the genextreme parameters estimated using pyextremes.\n    \"\"\"\n    model = EVA.from_extremes(extremes=hail_events_extremes, method=\"BM\")\n    model.fit_model(distribution_kwargs={\"floc\": 0})\n    periods, _, _ = model.get_return_value(np.arange(2, max_years))\n    return periods, model\n\n\ndef plot_fit_extreme_values(\n    ax: Axes,\n    category: str,\n    model_names: List[str],\n    models: Dict[str, Any],\n    critical_magnitude: float,\n    critical_period: int,\n) -> Axes:\n    \"\"\"Visualize how the estimated return function fit the empirical return values.\"\"\"\n    ax.scatter(\n        models[\"empirical\"][category][\"period\"],\n        models[\"empirical\"][category][\"magnitude\"],\n        s=80,\n        facecolors=\"none\",\n        edgecolors=\"k\",\n        label=\"Empirical Values\",\n    )\n    ax.axhline(critical_magnitude, c=\"r\", linestyle=\":\", label=\"Critical Limit\")\n    ax.axvline(critical_period, c=\"r\", linestyle=\"-.\", label=f\"{critical_period} Years\")\n\n    for model_name in model_names:\n\n        ax.plot(\n            models[model_name][category][\"period\"],\n            models[model_name][category][\"magnitude\"],\n            label=f\"{model_name}\",\n        )\n\n    ax.set_title(\"\\n\".join(category.split(\"_\")))\n    ax.grid(alpha=0.5)\n    ax.set_ylabel(\"Hail Size\")\n    ax.set_xlabel(\"Return Period\")\n    return ax\n```\n:::\n\n\nThe first estimation approach we decided to use was based on [Extreme Value Theory](https://en.wikipedia.org/wiki/Extreme_value_theory). In particular we followed [this tutorial](https://comptools.climatematch.io/tutorials/W2D3_ExtremesandVariability/student/W2D3_Intro.html) on extreme events provided by course on [computational tools in climate science](https://comptools.climatematch.io/tutorials/intro.html). \n\nExtreme Value Analysis (EVA) usually has the aim of estimating, giving a sample of data, the probability (i.e., risk) associated with an event that is extreme in nature. For doing so it usually require three steps:\n\n1. Individuating the extremes in a given period of time with a given time resolution (it usually boils down to be the annual extremes).\n\n2. Fit an appropriated statistical distribution to the extreme data.\n\n3. Estimate the probability of potentially unseen extreme events using the parameters of the distribution.\n\n\n### Individuating the \"Extremes\"\n\nIn order to individuate the extreme in our data we have two options:\n\n1. Block Maxima (i.e., BM): selecting the maximum value ove a \"block of time\" (e.g. a year).\n2. Peak Over Threshold (i.e., POT): selecting all the values higher than a given threshold.\n\nBoth options have their advantages and dis-advantages. We won't go into details and just say that we select BM mostly for convenience as it is the methodology that retains most of the data (which is already scarce in our situation). More information can be found in [this page](https://georgebv.github.io/pyextremes/user-guide/2-extreme-value-types/).\n\n![Example of Bock Maxima](results/images/block_maxima.png){#fig-block_maxima}\n\nIn @fig-block_maxima we can see how the maximum monthly hail sizes get converted in maximum **annual** hail sizes by applying a block maxima of size 12 months. One of the major advantages of block maxima is that of partially removing the issue of autocorrelation and seasonality as we take exactly one value for each year.\n\n### Fitting the Appropriated Distribution\n\nOnce we have obtained our samples of extreme values and did all that is in our power to ensure that they are independent and identically distributed (i.i.d.) we can proceed at fitting a distribution to the samples. Which distribution is more suitable depends on the characteristics of the events we are are tying to model.\n\nIn the case of extreme events what we are dealing with are tail events better described by skewed distributions with a fat tail. There are 3 types of distribution that are well suited for this: [Gumbell](https://en.wikipedia.org/wiki/Gumbel_distribution), [Weibull](https://en.wikipedia.org/wiki/Weibull_distribution) and [Frechet](https://en.wikipedia.org/wiki/Fréchet_distribution) which can be flexibly represented by a single 3 parameters distribution the Generalized [Extreme Value distribution (GEV)](https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution).\n\nThe Cumulative Distribution Function (CDF) for the GEV is defined as:\n\n$$\nH(x) = \n\\begin{cases}\n[1 + \\xi(\\frac{x - \\mu}{\\sigma})]^{1/ \\xi} ,& \\text{if } \\xi\\neq 0 \\\\\ne^{-(\\frac{x - \\mu}{\\sigma})} ,& \\text{if } \\xi = 0 \\\\\n\\end{cases}\n$$\n\nwith $\\mu$ being the location parameter (which controls the center of the distribution), $\\sigma$ being the scale parameter (controlling the spread of the distribution) and $\\xi$ being the shape parameter (which controls the behavior in the tails of the distribution). The CDF will become important later on when we will try to convert the probability associated with a given extreme value in a return period.\n\nThere are some important statistical reasons for choosing the GEV distribution for modelling annual maxima of hail events, but in our case we can see this more clearly by looking at @fig-gaus_gev_fit\n\n![Comparing Gaussian and GenExtreme Fit](results/images/hail_events_distributions.png){#fig-gaus_gev_fit}\n\nThe figure show both a Gaussian and a GEV fit to annual maxima for hail sizes. Other than the sub-optimal fit for the Gaussian distribution we can see how the tails in particular (where most problematic and truly extreme events concentrate) are heavily discounted.\n\n### Estimating the probability and return period for any extreme value\n\nOnce we have obtained the parameters of the GEV distribution it is possible to estimate the probability of a new, potentially unseen, hail event **as large as a critical value c** as simply $p(event <= c | \\mu, \\sigma, \\xi)$ with $\\mu$, $\\sigma$ and  $\\xi$ being estimated parameters. Of course the probability of observing an event **as large or larger then a critical value c** is simply given by $1 - p$.\n\nIt is also possible to convert this probability value in what is called a \"return period\" which indicate that on average how much time can pass between an events of certain magnitude or greater. This value is obtained by\n\n$$\nT = \\frac{1}{1 - p}\n$$\n\nGiven that in our case the time resolution of our extreme events is yearly, we can define each critical value as a \"T years event\" which indicates that every year there is a $1-p$ of observing an event **as large or larger then the critical value**. For example a \"100 years event\" indicate a probability of 0.01 of observing said event in a given year.\n\nLet's see how our EVA approach perform at the state level\n\n::: {#97a7c8d2 .cell execution_count=18}\n``` {.python .cell-code}\naggregated_eva_state_modelling = (\n    eva_state_modelling_df.groupby(eva_state_modelling_df[\"begin_date_time\"].dt.year)[\n        CONTINUOUS_TARGET\n    ]\n    .max()\n    .reset_index()\n)\nreturns = compute_empirical_return_periods(\n    aggregated_eva_state_modelling[CONTINUOUS_TARGET].values\n)\n\nplt.scatter(\n    returns,\n    aggregated_eva_state_modelling[CONTINUOUS_TARGET].values,\n    s=80,\n    facecolors=\"none\",\n    edgecolors=\"k\",\n    label=\"Empirical Values\",\n)\nplt.plot(\n    np.arange(1, 150),\n    compute_mle_return_periods_scipy(\n        aggregated_eva_state_modelling[CONTINUOUS_TARGET],\n        max_years=150,\n        min_years=1,\n    ),\n    c=COLORMAP(5),\n    linewidth=2,\n)\nplt.grid(alpha=0.5)\nplt.title(f\"Return Periods\\n{SINGLE_STATE[0].capitalize()}\")\nplt.ylabel(\"Hail Size\")\nplt.xlabel(\"Years\")\nplt.axvline(100, linestyle=\"-.\", c=\"r\", label=\"100 Years Event\")\nplt.axhline(\n    CRITICAL_VALUE_CONTINUOUS_TARGET, linestyle=\":\", c=\"r\", label='1.7\" Hail Event'\n)\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-18-output-1.png){}\n:::\n:::\n\n\nin this case we used scipy's `genextreme` distribution instead of the library `pyextreme`. We see how the estimated return periods fit nicely with the empirical values. \n\nEstimating return periods at the state level is not very useful as:\n\n1. US states are rather big.\n2. Over an entire state it is very likely that we will have at least one very large hail event per year. Therefore we might be overestimating the actual county-level risk profile.\n\nWe can perform the same type of analysis using county level data\n\n::: {#25b66383 .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\nmodels = {\n    \"empirical\": {},\n    \"MLE_scipy\": {},\n    \"MLE_pyextreme\": {},\n}\ncounty_names = eva_state_modelling_df[\"county\"].unique()\nfor county in tqdm(county_names):\n\n    hail_events_extremes = eva_state_modelling_df[\n        eva_state_modelling_df[\"county\"] == county\n    ][CONTINUOUS_TARGET].values\n    time_index = eva_state_modelling_df[eva_state_modelling_df[\"county\"] == county][\n        TIME_COLUMN\n    ].values\n\n    return_value, model = compute_mle_return_periods_pyextremes(\n        hail_events_extremes=pd.Series(data=hail_events_extremes, index=time_index),\n    )\n\n    models[\"empirical\"][county] = {\n        \"period\": compute_empirical_return_periods(\n            hail_events_extremes=hail_events_extremes,\n        ),\n        \"magnitude\": hail_events_extremes,\n    }\n\n    models[\"MLE_scipy\"][county] = {\n        \"period\": np.arange(2, 150),\n        \"magnitude\": compute_mle_return_periods_scipy(\n            hail_events_extremes=hail_events_extremes,\n        ),\n    }\n    models[\"MLE_pyextreme\"][county] = {\n        \"period\": np.arange(2, 150),\n        \"magnitude\": return_value,\n        \"model\": model,\n    }\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/68 [00:00<?, ?it/s]\r  4%|▍         | 3/68 [00:00<00:02, 25.17it/s]\r 10%|█         | 7/68 [00:00<00:02, 29.15it/s]\r 15%|█▍        | 10/68 [00:00<00:01, 29.24it/s]\r 19%|█▉        | 13/68 [00:00<00:01, 28.81it/s]\r 24%|██▎       | 16/68 [00:00<00:02, 24.82it/s]\r 28%|██▊       | 19/68 [00:00<00:02, 22.79it/s]\r 32%|███▏      | 22/68 [00:00<00:02, 22.76it/s]\r 37%|███▋      | 25/68 [00:01<00:01, 23.27it/s]\r 41%|████      | 28/68 [00:01<00:01, 24.32it/s]\r 46%|████▌     | 31/68 [00:01<00:01, 24.92it/s]\r 50%|█████     | 34/68 [00:01<00:01, 24.76it/s]\r 56%|█████▌    | 38/68 [00:01<00:01, 26.89it/s]\r 62%|██████▏   | 42/68 [00:01<00:00, 28.76it/s]\r 66%|██████▌   | 45/68 [00:01<00:00, 28.23it/s]\r 72%|███████▏  | 49/68 [00:01<00:00, 29.36it/s]\r 76%|███████▋  | 52/68 [00:01<00:00, 27.21it/s]\r 82%|████████▏ | 56/68 [00:02<00:00, 29.23it/s]\r 87%|████████▋ | 59/68 [00:02<00:00, 24.40it/s]\r 91%|█████████ | 62/68 [00:02<00:00, 25.57it/s]\r 97%|█████████▋| 66/68 [00:02<00:00, 27.46it/s]\r100%|██████████| 68/68 [00:02<00:00, 26.15it/s]\n```\n:::\n:::\n\n\n::: {#c82a68d6 .cell execution_count=20}\n``` {.python .cell-code}\nselected_counties = np.random.choice(county_names, 16)\n\nfig, axs = plt.subplots(4, 4, figsize=(8, 8), sharex=True, sharey=True)\nfor ax, county in zip(axs.flatten(), selected_counties):\n\n    ax = plot_fit_extreme_values(\n        ax=ax,\n        category=county,\n        model_names=[\"MLE_pyextreme\"],\n        models=models,\n        critical_magnitude=1.77,\n        critical_period=100,\n    )\n\nplt.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-20-output-1.png){}\n:::\n:::\n\n\nAs we can see the return periods can provide a substatially different profile depending on the county taken into consideration.\n\n## Estimating Expected Extreme Values and Hail Events using Bayesian Regression {#sec-bayesian_regression}\n\nLooking at @fig-hail_comparison_total we can hypothesize that the probability of observing a hail event of a certain magnitude can be a function of latitude and longitude. Indeed,the extreme hail events, although scattered, seem to show a certain degree of spatial consistency. \n\nSo why not simply empirically derive the likelihood of observing a given hail size in a given county? A bit like whe have been doing in @fig-sample_size. As we can see, the empirical percentiles derived from the observed data are very dis-homogeneous. This is because they are strongly influenced by the data we have available for a given county. According to @fig-sample_size sample sizes can vary wildly from county to county and make the empirical estimate subject to the influence of outliers.\n\nIn this case what we would want to do is to pool information from the entire state for helping estimating a given percentile of hail size in each county, even in those were we observe a limited number of events. On top of that we would also want to smooth out the contribution of outliers. One option in this case could be to fit a regression model on latitude and longitude data so to be able to estimate the hail size for a given event. However, relying a conventional linear regression approach would not be appropriated in our case as:\n\n1. Estimating the mean as simple least square regression would do is not appropriate as in our case we would be interested in the tail of the distribution.\n\n2. Assuming the residuals are normally distributed, as it is the case in standard linear regression, does not hold in our case.\n\n3. Looking at @fig-hail_comparison_total we can see that if a relationship between latitude, longitude and hail size exists, it is certainly not linear.\n\nA potential solution to these hurdles would be to fit a [quantile regression](https://en.wikipedia.org/wiki/Quantile_regression). Differently from conventional regression, quantile regression can be used for estimating any quantile in a distribution and not just the mean\n\n![Example of Quantile Regression](results/images/quantile_regression.png)\n\nUsing a regression approach would allow us to include the type of spatial and temporal effect we outlined during oour exploratory data analysis. Generically the regression would have the following formulation:\n\n$\\mathcal{Q}_{y_i|X_i}(\\tau) = \\alpha + \\beta X_i $\n\nwith $\\tau$ being the desired quantile, $X_i$ all the spatio-temporal covariates associated with hail events $y_i$ and $\\alpha$ and $\\beta$ parameters to be estimated. Expanding the formulation for quantile regression further, we would have\n\n$\\mathcal{Q}_{y_i|X_i}(\\tau) = \\alpha + \\phi(LatLon_i) \\beta_{LatLon} + \\phi(hour) \\beta_{Hour} + \\phi(month) \\beta_{month} + \\phi(year) \\beta_{year} $\n\nhere $\\phi$ indicate a cubic spline function that we apply in order to model non-linearity in the contribution of the various covariates. More precisly, $\\phi(LatLon_i)$ is a [tensor-product spline](https://stats.stackexchange.com/questions/45446/intuition-behind-tensor-product-interactions-in-gams-mgcv-package-in-r) generated from the latitude and longitude splines. $\\phi(hour)$ and $\\phi(month)$ can be thought as seasonal components while $\\phi(year)$ can be identified as a trend component.\n\nIn order to perform estimation on the entire surface of Florida we opted for an approximation of the spatial covariates by doing the following:\n\n1. Dividing the entire surface of Florida in a grid with 1 mile resolution (see @fig-estimation_grid).\n\n![The Estimation Grid Used for the Regression](results/images/estimation_grid.png){#fig-estimation_grid}\n\n3. For each square in the grid we obtain its centroid and extract the latitude, longitude and maximum observed hail size for that specific centroid obtaining something like the following table\n\n| latitude | longitude | hail_size |\n|:--------:|:---------:|:---------:|\n|    XXX   |    YYY    |     Z     |\n|    XXX   |    YYY    |     Z     |\n|    ...   |    ...    |    ...    |\n\nWe retained all measurements associated with a given square so that a single square might have more than one hail event associated to it. It goes without saying that the spatial resolution of our regression here would be entirely determined by the size of the squares.\n\n::: {#855c3a6c .cell execution_count=21}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\nfrom scipy.stats import iqr\n\nfrom sklearn.preprocessing import SplineTransformer, OrdinalEncoder, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom jax import numpy as jnp\nfrom jax import random\n\nfrom numpyro.infer.reparam import LocScaleReparam\n\nfrom numpyro.distributions import (\n    Normal,\n    AsymmetricLaplaceQuantile,\n    HalfNormal,\n    HalfCauchy,\n    Distribution,\n    Laplace,\n)\nfrom numpyro.infer import MCMC, NUTS, Predictive, SVI, Trace_ELBO, RenyiELBO\nfrom numpyro.infer.svi import SVIRunResult\nfrom numpyro.infer.autoguide import (\n    AutoDAIS,\n    AutoNormal,\n    AutoLowRankMultivariateNormal,\n    AutoMultivariateNormal,\n)\n\nRNG_KEY = random.key(seed=666)\n\ndef sample_using_mcmc(\n    rng_key: ArrayLike,\n    model: Callable,\n    model_kwargs: Dict[str, Any],\n    MCMC_kwargs: Dict[str, Any],\n) -> MCMC:\n    \"\"\"Sample from the model using MCMC\"\"\"\n    rng_key, sub_rng_key = random.split(rng_key)\n    kernel = NUTS(model=model)\n    mcmc = MCMC(kernel, progress_bar=True, **MCMC_kwargs)\n    mcmc.run(sub_rng_key, **model_kwargs)\n    mcmc.print_summary()\n    return mcmc\n\n\ndef sample_using_svi(\n    rng_key: ArrayLike,\n    model: Callable,\n    autoguide: Any,\n    model_kwargs: Dict[str, Any],\n    guide_kwargs: Dict[str, Any],\n    optimizer_kwargs: Dict[str, Any],\n    num_steps: int,\n    elbo_tracer: Any = Trace_ELBO,\n    num_particles: int = 2\n) -> Tuple[SVIRunResult, AutoDAIS]:\n    \"\"\"Sample from the model using variational inference\"\"\"\n    rng_key, sub_rng_key = random.split(rng_key)\n    guide = autoguide(model=model, **guide_kwargs)\n    optimizer = numpyro.optim.ClippedAdam(**optimizer_kwargs)\n    svi = SVI(model, guide, optimizer, loss=elbo_tracer(num_particles=num_particles))\n    svi_result = svi.run(sub_rng_key, num_steps, **model_kwargs)\n\n    fig, ax = plt.subplots()\n    ax.plot(svi_result.losses)\n    ax.set_title(\"ELBO loss\")\n    ax.grid(alpha=0.5)\n    plt.show()\n\n    return svi_result, guide\n\n\ndef sample_posterior_predictive_mcmc(\n    rng_key: ArrayLike,\n    model: Callable,\n    posterior_samples: Dict[str, Any],\n    model_kwargs: Dict[str, Any],\n) -> ArrayLike:\n    \"\"\"Sample from the posterior using MCMC  posterior samples\"\"\"\n    rng_key, sub_rng_key = random.split(rng_key)\n    predictive = Predictive(model, posterior_samples=posterior_samples)\n    posterior_predictive = predictive(rng_key=sub_rng_key, **model_kwargs)\n    return posterior_predictive\n\n\ndef sample_posterior_predictive_svi(\n    rng_key: ArrayLike,\n    model: Callable,\n    guide: AutoDAIS,\n    covariates_hat: Dict[str, ArrayLike],\n    svi_result: SVIRunResult,\n    num_samples: int,\n    model_kwargs: Dict[str, Any],\n    return_sites: List[str] = None,\n    target: ArrayLike = None,\n) -> ArrayLike:\n    \"\"\"Sample from the posterior using SVI inferred parameters\"\"\"\n    model_kwargs = {key: value for key, value in model_kwargs.items()}\n    model_kwargs[\"target\"] = target\n    model_kwargs[\"covariates\"] = covariates_hat\n    predictive = Predictive(\n        model=model,\n        guide=guide,\n        params=svi_result.params,\n        num_samples=num_samples,\n        exclude_deterministic=False,\n        return_sites=return_sites,\n    )\n    rng_key, sub_rng_key = random.split(rng_key)\n    posterior_predictive = predictive(rng_key=sub_rng_key, **model_kwargs)\n    return posterior_predictive\n\n\ndef transform_fitting_covariates(\n    covariates: Dict[str, ArrayLike],\n    transformers: Dict[str, Any],\n) -> Tuple[Dict[str, ArrayLike], Dict[str, Any]]:\n    \"\"\"Fit transformers and transform covariates\"\"\"\n    transformed_covariates = {}\n    for covariate_name, covariate_array in covariates.items():\n\n        transformers[covariate_name].fit(covariate_array)\n        transformed_covariates[covariate_name] = transformers[covariate_name].transform(\n            covariate_array\n        )\n\n    return transformed_covariates, transformers\n\n\ndef transform_estimation_covariates(\n    covariates: Dict[str, ArrayLike],\n    transformers: Dict[str, Any],\n) -> Dict[str, ArrayLike]:\n    \"\"\"Transform covariates using fitted transformers\"\"\"\n    transformed_covariates = {}\n    for covariate_name, covariate_array in covariates.items():\n\n        transformed_covariates[covariate_name] = transformers[covariate_name].transform(\n            covariate_array\n        )\n\n    return transformed_covariates\n\n\ndef jaxify_array_dictionary(\n    array_dictionary: Dict[str, ArrayLike],\n) -> Dict[str, ArrayLike]:\n    \"\"\"Turn arrays in a dictionary into JAX arrays\"\"\"\n    jaxified_array_dictionary = {}\n    for key, value in array_dictionary.items():\n\n        jaxified_array_dictionary[key] = jnp.array(value)\n\n    return jaxified_array_dictionary\n\n\ndef prepare_modelling_data(\n    covariates: Dict[str, ArrayLike],\n    covariates_hat: Dict[str, ArrayLike],\n    transformers: Dict[str, Pipeline],\n    target: ArrayLike,\n) -> Tuple[Dict[str, Pipeline], Dict[str, ArrayLike], Dict[str, ArrayLike], ArrayLike]:\n    transformed_covariates, transformers = transform_fitting_covariates(\n        covariates=covariates,\n        transformers=transformers,\n    )\n    transformed_covariates_hat = transform_estimation_covariates(\n        covariates=covariates_hat,\n        transformers=transformers,\n    )\n    transformed_covariates = jaxify_array_dictionary(\n        array_dictionary=transformed_covariates,\n    )\n    transformed_covariates_hat = jaxify_array_dictionary(\n        array_dictionary=transformed_covariates_hat,\n    )\n    target = jnp.array(target)\n\n    return transformers, transformed_covariates, transformed_covariates_hat, target\n\ndef generate_temporal_components(\n    posterior: Dict[str, ArrayLike],\n    transformers: Dict[str, Pipeline],\n    years: ArrayLike,\n    suffix: str = \"\",\n    parameter_transformer: Callable = None\n) -> Dict[str, ArrayLike]:\n    \"\"\"Generate the temporal components\"\"\"\n    mapped_dot = vmap(jnp.dot, in_axes=(None, 0))\n    covariates = {\n        \"year_covariates\": (years.reshape(-1, 1)),\n        \"month_covariates\": (np.arange(1, 13).reshape(-1, 1)),\n        \"hour_covariates\": (np.arange(24).reshape(-1, 1)),\n    }\n\n    covariates = transform_estimation_covariates(\n        covariates=covariates, transformers=transformers\n    )\n\n    year_component = mapped_dot(\n        covariates[\"year_covariates\"], posterior[f\"beta_year{suffix}\"]\n    )\n    month_component = mapped_dot(\n        covariates[\"month_covariates\"], posterior[f\"beta_month{suffix}\"]\n    )\n    hour_component = mapped_dot(\n        covariates[\"hour_covariates\"], posterior[f\"beta_hour{suffix}\"]\n    )\n\n    posterior_components = {\n        \"year_component\": year_component,\n        \"month_component\": month_component,\n        \"hour_component\": hour_component,\n    }\n\n    if parameter_transformer is not None:\n\n        posterior_components = {\n            key: parameter_transformer(value) for key, value in posterior_components.items()\n        }\n\n    return posterior_components\n\ndef visualize_geo_regression(\n    covariates_hat_df: gpd.GeoDataFrame, posterior: Dict[str, ArrayLike], parameter: str, parameter_transformer: Callable = None\n) -> Tuple[plt.Figure, Axes]:\n    \"\"\"Visualize the result from the regression on a geodataframe.\"\"\"\n    parameter_value = posterior[parameter]\n    if parameter_transformer is not None:\n        parameter_value = parameter_transformer(parameter_value)\n\n    covariates_hat_df[\"2.5%\"] = np.percentile(\n        parameter_value,\n        q=2.5,\n        axis=0,\n    )\n    covariates_hat_df[\"Median\"] = np.percentile(\n        parameter_value,\n        q=50,\n        axis=0,\n    )\n    covariates_hat_df[\"97.5%\"] = np.percentile(\n        parameter_value,\n        q=97.5,\n        axis=0,\n    )\n    fig, axs = plt.subplots(1, 3, figsize=(15, 10))\n    for ax, column in zip(axs, [\"2.5%\", \"Median\", \"97.5%\"]):\n\n        covariates_hat_df.plot(\n            column,\n            ax=ax,\n            cmap=COLORMAP_NAME,\n            legend=True,\n            legend_kwds={\"shrink\": 0.3},\n            vmin=np.percentile(parameter_value, q=1),\n            vmax=np.percentile(parameter_value, q=99),\n        )\n        ax.grid(alpha=0.5)\n        ax.set_title(column)\n        ax.set_ylabel(\"Longitude\")\n        ax.set_xlabel(\"Latitude\")\n\n    return fig, axs\n\n\ndef visualize_county_regression(\n    modelling_df: pd.DataFrame,\n    covariates_hat_df: gpd.GeoDataFrame,\n    posterior: Dict[str, ArrayLike],\n    parameter: str,\n    target: str,\n) -> Tuple[plt.Figure, Axes]:\n    \"\"\"Visualize the regression at county level\"\"\"\n    samples_df = pd.DataFrame(posterior[parameter].T)\n    samples_df[COUNTIES_INDEX] = covariates_hat_df[COUNTIES_INDEX].values\n    rows = 4\n    random_counties = np.random.choice(\n        samples_df[COUNTIES_INDEX].unique(), rows**2, replace=False\n    )\n    fig, axs = plt.subplots(rows, rows, figsize=(10, 10))\n    for county, ax in zip(random_counties, axs.flatten()):\n\n        county_target = modelling_df[modelling_df[COUNTIES_INDEX] == county][\n            target\n        ].values\n        county_samples = (\n            samples_df[samples_df[COUNTIES_INDEX] == county]\n            .drop([COUNTIES_INDEX], axis=1)\n            .values.flatten()\n        )\n        sns.histplot(\n            data=county_target,\n            element=\"step\",\n            fill=False,\n            stat=\"density\",\n            ax=ax,\n            color=COLORMAP(5),\n        )\n\n        ax.axvline(np.quantile(county_target, CRITICAL_QUANTILE), c=\"r\", linestyle=\"--\")\n        ax.axvspan(\n            np.quantile(county_samples, 0.025),\n            np.quantile(county_samples, 0.975),\n            alpha=0.5,\n            color=\"red\",\n        )\n        ax.grid(alpha=0.5)\n        ax.set_ylim(0, None)\n    plt.tight_layout()\n    return fig, axs\n\n\ndef visualize_temporal_components(temporal_components: Dict[str, ArrayLike]) -> Tuple[plt.Figure, Axes]:\n    \"\"\"Visualize the various temporal components\"\"\"\n    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n    for ax, component in zip(axs.flatten(), [\"year_component\", \"month_component\", \"hour_component\"]):\n        ax.plot(\n            np.percentile(\n                a=temporal_components[component],\n                q=50,\n                axis=0\n            ),\n            c=COLORMAP(5)\n        )\n        ax.fill_between(\n            np.arange(temporal_components[component].shape[1]),\n            np.percentile(\n                a=temporal_components[component],\n                q=2.5,\n                axis=0\n            ),\n            np.percentile(\n                a=temporal_components[component],\n                q=97.5,\n                axis=0\n            ),\n            color=COLORMAP(1),\n            alpha=0.5\n        )\n        ax.set_title(\" \".join(component.split(\"_\")))\n        ax.grid(alpha=0.5)\n        ax.set_xlabel(component.split(\"_\")[0])\n        ax.set_ylabel(\"Component Contribution\")\n\n    plt.tight_layout()\n    return fig, axs\n\n\ndef create_tensor_product_spline(first_spline, second_spline):\n    return np.tensordot(first_spline, second_spline, axes=0)\n```\n:::\n\n\n### Estimating Expected Extreme Values Using Quantile Regression{#sec-quantile_regression}\n\n::: {#82d58e2e .cell execution_count=22}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\ndef create_estimation_covariates_quantile_regression(\n    months: List[int],\n    hours: List[int],\n    years: List[int],\n    geometries_df: gpd.GeoDataFrame,\n    edge_size: float,\n    states: List[str],\n) -> Tuple[gpd.GeoDataFrame, Dict[str, ArrayLike]]:\n    \"\"\"Create the covariates for estimation\"\"\"\n    quantile_regression_covariates_hat_df = create_covariates_df(\n        geometries_df=geometries_df,\n        edge_size=edge_size,\n        state=states,\n        months=months,\n        hours=hours,\n        years=years,\n    )\n    quantile_regression_covariates_hat = {\n        \"latitude_covariates\": (\n            quantile_regression_covariates_hat_df[LAT_COVARIATES].values.reshape(-1, 1)\n        ),\n        \"longitude_covariates\": (\n            quantile_regression_covariates_hat_df[LON_COVARIATES].values.reshape(-1, 1)\n        ),\n        \"year_covariates\": (\n            quantile_regression_covariates_hat_df[YEAR_COVARIATES].values.reshape(-1, 1)\n        ),\n        \"month_covariates\": (\n            quantile_regression_covariates_hat_df[MONTH_COVARIATE].values.reshape(-1, 1)\n        ),\n        \"hour_covariates\": (\n            quantile_regression_covariates_hat_df[HOUR_COVARIATE].values.reshape(-1, 1)\n        ),\n        \"counties_index\": (\n            quantile_regression_covariates_hat_df[COUNTIES_INDEX].values.reshape(-1, 1)\n        ),\n    }\n    return quantile_regression_covariates_hat_df, quantile_regression_covariates_hat\n```\n:::\n\n\n::: {#6288c858 .cell execution_count=23}\n``` {.python .cell-code}\nquantile_regression_transformers = {\n    \"latitude_covariates\": Pipeline(\n        steps=[\n            (\n                \"spline_transformer\",\n                SplineTransformer(\n                    include_bias=False,\n                    extrapolation=\"periodic\",\n                    n_knots=20,\n                ),\n            )\n        ]\n    ),\n    \"longitude_covariates\": Pipeline(\n        steps=[\n            (\n                \"spline_transformer\",\n                SplineTransformer(\n                    include_bias=False,\n                    extrapolation=\"periodic\",\n                    n_knots=20,\n                ),\n            )\n        ]\n    ),\n    \"year_covariates\": Pipeline(\n        steps=[\n            (\n                \"ordinal_encoder\",\n                OrdinalEncoder(\n                    dtype=\"int\",\n                ),\n            ),\n            (\n                \"spline_transformer\",\n                SplineTransformer(\n                    include_bias=False,\n                ),\n            ),\n        ]\n    ),\n    \"month_covariates\": Pipeline(\n        steps=[\n            (\n                \"spline_transformer\",\n                SplineTransformer(\n                    include_bias=False,\n                ),\n            )\n        ]\n    ),\n    \"hour_covariates\": Pipeline(\n        steps=[\n            (\n                \"spline_transformer\",\n                SplineTransformer(\n                    include_bias=False,\n                ),\n            )\n        ]\n    ),\n    \"counties_index\": OrdinalEncoder(\n        dtype=\"int\",\n    ),\n}\nquantile_regression_covariates = {\n    \"latitude_covariates\": (\n        continuous_state_modelling_df[LAT_COVARIATES].values.reshape(-1, 1)\n    ),\n    \"longitude_covariates\": (\n        continuous_state_modelling_df[LON_COVARIATES].values.reshape(-1, 1)\n    ),\n    \"year_covariates\": (\n        continuous_state_modelling_df[YEAR_COVARIATES].values.reshape(-1, 1)\n    ),\n    \"month_covariates\": (\n        continuous_state_modelling_df[MONTH_COVARIATE].values.reshape(-1, 1)\n    ),\n    \"hour_covariates\": (\n        continuous_state_modelling_df[HOUR_COVARIATE].values.reshape(-1, 1)\n    ),\n    \"counties_index\": (\n        continuous_state_modelling_df[COUNTIES_INDEX].values.reshape(-1, 1)\n    ),\n}\nquantile_regression_target = continuous_state_modelling_df[CONTINUOUS_TARGET].values\n\nquantile_regression_covariates_hat_df, quantile_regression_covariates_hat = (\n    create_estimation_covariates_quantile_regression(\n        months=[5],\n        hours=[17],\n        years=[2024],\n        geometries_df=geometries_df,\n        edge_size=DEGREES_PRECISION,\n        states=SELECTED_ANALYSIS_STATES,\n    )\n)\n\n(\n    quantile_regression_transformers,\n    quantile_regression_covariates,\n    quantile_regression_covariates_hat,\n    target,\n) = prepare_modelling_data(\n    covariates=quantile_regression_covariates,\n    covariates_hat=quantile_regression_covariates_hat,\n    target=quantile_regression_target,\n    transformers=quantile_regression_transformers,\n)\n\nquantile_regression_covariates[\"latitude_longitude_tensor_covariates\"] = (\n    np.vstack(\n        [\n            create_tensor_product_spline(\n                quantile_regression_covariates[\"latitude_covariates\"][i, :], \n                quantile_regression_covariates[\"longitude_covariates\"][i, :]).flatten() for i in range(quantile_regression_covariates[\"latitude_covariates\"].shape[0]\n            )\n        ]\n    )\n)\nquantile_regression_covariates_hat[\"latitude_longitude_tensor_covariates\"] = (\n    np.vstack(\n        [\n            create_tensor_product_spline(\n                quantile_regression_covariates_hat[\"latitude_covariates\"][i, :], \n                quantile_regression_covariates_hat[\"longitude_covariates\"][i, :]).flatten() for i in range(quantile_regression_covariates_hat[\"latitude_covariates\"].shape[0]\n            )\n        ]\n    )\n)\n```\n:::\n\n\n**Partially Pooled Quantile Regression**\n\nBefore modelling the spatial components directly (which is a rather expensive process) we attempted an approximated solution by varying the intercept of the model for each county in the state. In order to pool information across the entire state and performing outlier regularization we opted for a [partially-pooled model](https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling) (an amazing resource for learning more about this topic is this Michael Betancourt [blog post](https://betanalpha.github.io/assets/case_studies/hierarchical_modeling.html)).\n\n$$\n\\begin{gather}\n\\color{RedOrange}\\sigma_{county} \\sim HalfCauchy(\\sigma=5) \\\\\n\\color{RedOrange}\\mu_{county} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\color{RedOrange}\\alpha_{county} \\sim \\mathcal{N}(\\mu_{county}, \\sigma_{county}) \\\\\n\\beta_{hour} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\beta_{month} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\sigma \\sim HalfNormal(1)\\\\\n\\mu = exp(\\alpha_{county} + \\beta_{hour}f(hour) + \\beta_{month}f(month) + \\beta_{year}f(year))\\\\\n\\color{RedOrange} y \\sim AsymmetricLapace(\\mu, \\sigma, \\tau=.95)\n\\end{gather}\n$$\n\nGiven the fomrulation above, it is iomportant to remeber that given the exponential link applied to $\\mu$ the relationship between covariates is multiplicative rather than additive.\n\nWe define the model in numpyro and visualize its graphical model representation\n\n::: {#2bfc8eb5 .cell execution_count=24}\n``` {.python .cell-code}\nreparam_config = {\n    \"alpha\": LocScaleReparam(0),\n}\n\n\n@numpyro.handlers.reparam(config=reparam_config)\ndef hierarchical_non_spatial_quantile_regression(\n    target: ArrayLike,\n    covariates: Dict[str, ArrayLike],\n    quantile: float,\n    prior_mu_alpha: Distribution,\n    prior_sigma_alpha: Distribution,\n    prior_scale: Distribution,\n    prior_beta_year: Distribution,\n    prior_beta_month: Distribution,\n    prior_beta_hour: Distribution,\n) -> None:\n    \"\"\"Quantile regression model with partially pooled intercept\"\"\"\n    n_groups = len(np.unique(covariates[\"counties_index\"]))\n    counties_index = covariates[\"counties_index\"].flatten()\n\n    mu_alpha = numpyro.sample(\n        \"mu_alpha\",\n        prior_mu_alpha,\n    )\n    sigma_alpha = numpyro.sample(\n        \"sigma_alpha\",\n        prior_sigma_alpha,\n    )\n\n    with numpyro.plate(\"counties\", n_groups):\n\n        alpha = numpyro.sample(\n            \"alpha\",\n            Normal(mu_alpha, sigma_alpha),\n        )\n\n    beta_hour = numpyro.sample(\n        \"beta_hour\",\n        prior_beta_hour.expand([covariates[\"hour_covariates\"].shape[1]]),\n    )\n    beta_month = numpyro.sample(\n        \"beta_month\",\n        prior_beta_month.expand([covariates[\"month_covariates\"].shape[1]]),\n    )\n    beta_year = numpyro.sample(\n        \"beta_year\",\n        prior_beta_year.expand([covariates[\"year_covariates\"].shape[1]]),\n    )\n    hour_component = numpyro.deterministic(\n        name=\"hour_component\",\n        value=jnp.dot(covariates[\"hour_covariates\"], beta_hour),\n    )\n    month_component = numpyro.deterministic(\n        name=\"month_component\",\n        value=jnp.dot(covariates[\"month_covariates\"], beta_month),\n    )\n    year_component = numpyro.deterministic(\n        name=\"year_component\",\n        value=jnp.dot(covariates[\"year_covariates\"], beta_year),\n    )\n    temporal_component = numpyro.deterministic(\n        name=\"temporal_component\",\n        value=year_component + month_component + hour_component,\n    )\n    spatial_component = numpyro.deterministic(\n        name=\"spatial_component\",\n        value=alpha[counties_index],\n    )\n    loc = numpyro.deterministic(\n        name=\"loc\", \n        value=jnp.exp(spatial_component + temporal_component)\n    )\n    scale = numpyro.sample(\n        \"scale\",\n        prior_scale,\n    )\n    obs = numpyro.sample(\n        \"obs\",\n        AsymmetricLaplaceQuantile(loc=loc, scale=scale, quantile=quantile),\n        obs=target,\n    )\n    if target is not None:\n        numpyro.deterministic(\n            \"log_likelihood\",\n            AsymmetricLaplaceQuantile(\n                loc=loc, \n                scale=scale, \n                quantile=quantile,\n            )\n            .log_prob(target)\n        )\n\nhierarchical_non_spatial_model_parameters = [\n    \"mu_alpha\",\n    \"sigma_alpha\",\n    \"beta_hour\",\n    \"beta_month\",\n    \"beta_year\",\n    \"loc\",\n    \"alpha\",\n    \"hour_component\",\n    \"month_component\",\n    \"year_component\",\n    \"temporal_component\",\n    \"spatial_component\",\n    \"scale\",\n    \"obs\",\n]\nhierarchical_non_spatial_model_kwargs = {\n    \"covariates\": quantile_regression_covariates,\n    \"quantile\": CRITICAL_QUANTILE,\n    \"target\": quantile_regression_target,\n    \"prior_mu_alpha\": Normal(loc=0.0, scale=5.0),\n    \"prior_sigma_alpha\": HalfCauchy(scale=2.0),\n    \"prior_beta_year\": Normal(loc=0.0, scale=1),\n    \"prior_beta_month\": Normal(loc=0.0, scale=1),\n    \"prior_beta_hour\": Normal(loc=0.0, scale=1),\n    \"prior_scale\": HalfNormal(scale=1),\n}\nnumpyro.render_model(\n    hierarchical_non_spatial_quantile_regression,\n    model_kwargs=hierarchical_non_spatial_model_kwargs,\n    render_distributions=False,\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n![](hail_risk_estimation_files/figure-html/cell-24-output-1.svg){}\n:::\n:::\n\n\nGiven the size of our data we then proceed at estimating the parameters using [variational inference](https://en.wikipedia.org/wiki/Variational_Bayesian_methods) (we recommend this amazing [primer](https://www.youtube.com/watch?v=Moo4-KR5qNg) on variational bayes by Tamara Broderick).\n\n::: {#801b5e0a .cell execution_count=25}\n``` {.python .cell-code}\nsvi_pooled_quantile_regression_parameters, svi_pooled_quantile_regression_guide = (\n    sample_using_svi(\n        rng_key=RNG_KEY,\n        model=hierarchical_non_spatial_quantile_regression,\n        model_kwargs=hierarchical_non_spatial_model_kwargs,\n        autoguide=AutoMultivariateNormal,\n        guide_kwargs={},\n        optimizer_kwargs={\"step_size\": 1e-4, \"clip_norm\": 5},\n        num_steps=NUMBER_ITERATIONS,\n        num_particles=NUMBER_PARTICLES,\n    )\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/100 [00:00<?, ?it/s]\r  1%|          | 1/100 [00:00<00:27,  3.56it/s]\r100%|██████████| 100/100 [00:00<00:00, 323.59it/s, init loss: 15671.9941, avg. loss [96-100]: 16196.0732]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-25-output-2.png){}\n:::\n:::\n\n\nAn then sampled from the posterior distribution using previously unseen data (i.e. the covariates we have generated)\n\n::: {#5e57fe17 .cell execution_count=26}\n``` {.python .cell-code}\nposterior_hierarchical_non_spatial_regression_svi = sample_posterior_predictive_svi(\n    rng_key=RNG_KEY,\n    covariates_hat=quantile_regression_covariates_hat,\n    model_kwargs=hierarchical_non_spatial_model_kwargs,\n    model=hierarchical_non_spatial_quantile_regression,\n    guide=svi_pooled_quantile_regression_guide,\n    svi_result=svi_pooled_quantile_regression_parameters,\n    num_samples=2000,\n    return_sites=hierarchical_non_spatial_model_parameters,\n)\n```\n:::\n\n\nThis plot shows the estimated intercept values for each county as a crude approxiamtion of a spatial component\n\n::: {#79c7ebb7 .cell execution_count=27}\n``` {.python .cell-code}\nfig, axs = visualize_geo_regression(\n    covariates_hat_df=quantile_regression_covariates_hat_df,\n    posterior=posterior_hierarchical_non_spatial_regression_svi,\n    parameter=\"spatial_component\",\n    parameter_transformer=lambda x: jnp.exp(x)\n\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-27-output-1.png){}\n:::\n:::\n\n\nWhile this other plots show both the seasonal and trend componets coming from the hour, month and year covariates.\n\n::: {#592b7a5c .cell execution_count=28}\n``` {.python .cell-code}\nfig, axs = visualize_temporal_components(\n    temporal_components=generate_temporal_components(\n        posterior=posterior_hierarchical_non_spatial_regression_svi,\n        transformers=quantile_regression_transformers,\n        years=continuous_state_modelling_df[YEAR_COVARIATES].unique(),\n        parameter_transformer=lambda x: jnp.exp(x)\n    )\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-28-output-1.png){}\n:::\n:::\n\n\n**Fully Pooled Spatial Quantile Regression**\n\nIn order to then obtain a finer grain spatial representation we then proceeded at substituting the intercept $\\alpha$ from the previous model with a tensor product spline obtained from latitude and longitude. In the case the $Laplace$ prior is applied in order to put a strong regularization component on the tensor product spline.\n\n$$\n\\begin{gather}\n\\color{RedOrange}\\beta_{spatial} \\sim Laplace(\\mu=0, \\sigma=1) \\\\\n\\beta_{hour} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\beta_{month} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\beta_{year} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\sigma \\sim HalfNormal(1)\\\\\n\\color{RedOrange}\\mu = exp(\\beta_{spatial}f(spatial) + \\beta_{hour}f(hour) + \\beta_{month}f(month) + \\beta_{year}f(year))\\\\\n\\ y \\sim AsymmetricLapace(\\mu, \\sigma, \\tau=.95)\n\\end{gather}\n$$\n\n::: {#c34b7663 .cell execution_count=29}\n``` {.python .cell-code}\ndef pooled_quantile_regression(\n    target: ArrayLike,\n    covariates: Dict[str, ArrayLike],\n    quantile: float,\n    prior_beta_spatial: Distribution,\n    prior_beta_month: Distribution,\n    prior_beta_year: Distribution,\n    prior_beta_hour: Distribution,\n    prior_scale: Distribution,\n) -> None:\n    \"\"\"Simple quantile regression model\"\"\"\n    beta_spatial = numpyro.sample(\n        \"beta_spatial\",\n        prior_beta_spatial.expand([covariates[\"latitude_longitude_tensor_covariates\"].shape[1]]),\n    )\n    beta_hour = numpyro.sample(\n        \"beta_hour\",\n        prior_beta_hour.expand([covariates[\"hour_covariates\"].shape[1]]),\n    )\n    beta_month = numpyro.sample(\n        \"beta_month\",\n        prior_beta_month.expand([covariates[\"month_covariates\"].shape[1]]),\n    )\n    beta_year = numpyro.sample(\n        \"beta_year\",\n        prior_beta_year.expand([covariates[\"year_covariates\"].shape[1]]),\n    )\n    scale = numpyro.sample(\n        \"scale\",\n        prior_scale,\n    )\n    hour_component = numpyro.deterministic(\n        name=\"hour_component\",\n        value=jnp.dot(covariates[\"hour_covariates\"], beta_hour),\n    )\n    month_component = numpyro.deterministic(\n        name=\"month_component\",\n        value=jnp.dot(covariates[\"month_covariates\"], beta_month),\n    )\n    year_component = numpyro.deterministic(\n        name=\"year_component\",\n        value=jnp.dot(covariates[\"year_covariates\"], beta_year),\n    )\n    spatial_component = numpyro.deterministic(\n        name=\"spatial_component\",\n        value=jnp.dot(covariates[\"latitude_longitude_tensor_covariates\"], beta_spatial),\n    )\n    temporal_component = numpyro.deterministic(\n        name=\"temporal_component\",\n        value=year_component + month_component + hour_component,\n    )\n    loc = numpyro.deterministic(\n        name=\"loc\",\n        value=jnp.exp(spatial_component + temporal_component),\n    )\n    obs = numpyro.sample(\n        \"obs\",\n        AsymmetricLaplaceQuantile(loc=loc, scale=scale, quantile=quantile),\n        obs=target,\n    )\n    if target is not None:\n        numpyro.deterministic(\n            \"log_likelihood\",\n            AsymmetricLaplaceQuantile(\n                loc=loc, \n                scale=scale, \n                quantile=quantile,\n            )\n            .log_prob(target)\n        )\n\npooled_quantile_model_parameters = [\n    \"beta_spatial\",\n    \"beta_hour\",\n    \"beta_month\",\n    \"beta_year\",\n    \"loc\",\n    \"spatial_component\",\n    \"hour_component\",\n    \"month_component\",\n    \"year_component\",\n    \"scale\",\n    \"obs\",\n]\npooled_quantile_model_kwargs = {\n    \"covariates\": quantile_regression_covariates,\n    \"quantile\": CRITICAL_QUANTILE,\n    \"target\": quantile_regression_target,\n    \"prior_beta_spatial\": Laplace(loc=0.0, scale=5),\n    \"prior_beta_year\": Normal(loc=0.0, scale=1),\n    \"prior_beta_month\": Normal(loc=0.0, scale=1),\n    \"prior_beta_hour\": Normal(loc=0.0, scale=1),\n    \"prior_scale\": HalfNormal(scale=1),\n}\nnumpyro.render_model(\n    pooled_quantile_regression,\n    model_kwargs=pooled_quantile_model_kwargs,\n    render_distributions=False,\n    render_params=True,\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n![](hail_risk_estimation_files/figure-html/cell-29-output-1.svg){}\n:::\n:::\n\n\nAgain we proceeded at estimating parameters using variational inference\n\n::: {#55db7a01 .cell execution_count=30}\n``` {.python .cell-code}\nsvi_pooled_quantile_regression_parameters, svi_pooled_quantile_regression_guide = (\n    sample_using_svi(\n        rng_key=RNG_KEY,\n        model=pooled_quantile_regression,\n        model_kwargs=pooled_quantile_model_kwargs,\n        autoguide=AutoMultivariateNormal,\n        guide_kwargs={},\n        optimizer_kwargs={\"step_size\": 1e-4, \"clip_norm\": 5},\n        num_steps=NUMBER_ITERATIONS,\n        num_particles=NUMBER_PARTICLES,\n    )\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/100 [00:00<?, ?it/s]\r  1%|          | 1/100 [00:00<00:27,  3.67it/s]\r 86%|████████▌ | 86/100 [00:00<00:00, 294.07it/s, init loss: 13984.2441, avg. loss [81-85]: 14533.4922]\r100%|██████████| 100/100 [00:00<00:00, 256.42it/s, init loss: 13984.2441, avg. loss [96-100]: 14279.2705]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-30-output-2.png){}\n:::\n:::\n\n\nAnd we then obtained the posterior samples\n\n::: {#c937c41f .cell execution_count=31}\n``` {.python .cell-code}\nposterior_pooled_quantile_regression_svi = sample_posterior_predictive_svi(\n    rng_key=RNG_KEY,\n    covariates_hat=quantile_regression_covariates_hat,\n    model_kwargs=pooled_quantile_model_kwargs,\n    model=pooled_quantile_regression,\n    guide=svi_pooled_quantile_regression_guide,\n    svi_result=svi_pooled_quantile_regression_parameters,\n    num_samples=2000,\n    return_sites=pooled_quantile_model_parameters,\n)\n```\n:::\n\n\nWe can see how the spatial component now varies smoothly across the entire state \n\n::: {#3b1f1c87 .cell execution_count=32}\n``` {.python .cell-code}\nfig, axs = visualize_geo_regression(\n    covariates_hat_df=quantile_regression_covariates_hat_df,\n    posterior=posterior_pooled_quantile_regression_svi,\n    parameter=\"spatial_component\",\n    parameter_transformer=lambda x: jnp.exp(x)\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-32-output-1.png){}\n:::\n:::\n\n\nWe also obtained the same temporal components as before\n\n::: {#b9bc9e28 .cell execution_count=33}\n``` {.python .cell-code}\nfig, axs = visualize_temporal_components(\n    temporal_components=generate_temporal_components(\n        posterior=posterior_pooled_quantile_regression_svi,\n        transformers=quantile_regression_transformers,\n        years=continuous_state_modelling_df[YEAR_COVARIATES].unique(),\n        parameter_transformer=lambda x: jnp.exp(x)\n    )\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-33-output-1.png){}\n:::\n:::\n\n\n**Partially Pooled Spatial Quantile Regression**\n\nAs a final iteration of the quantile regression model we wanted to combine the partially pooled model with the fully pooled spatial model by allowing the spatial component to vary within each county. Although rather expensive (the model needs to estimate `number_of_counties * number_spline_features` parameters) this approach allows to have spatial effects that are localized to each county. This is an attempt to model the geographical peculiarities that each county might have.\n\n$$\n\\begin{gather}\n\\color{RedOrange}\\sigma_{county} \\sim HalfCauchy(\\sigma=5) \\\\\n\\color{RedOrange}\\mu_{county} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\color{RedOrange}\\beta_{spatial,county} \\sim Laplace(\\mu_{county}, \\sigma_{county}) \\\\\n\\beta_{hour} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\beta_{month} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\beta_{year} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\sigma \\sim HalfNormal(1)\\\\\n\\color{RedOrange}\\mu = exp(\\beta_{spatial,county}f(spatial) + \\beta_{hour}f(hour) + \\beta_{month}f(month) + \\beta_{year}f(year))\\\\\n\\ y \\sim AsymmetricLapace(\\mu, \\sigma, \\tau=.95)\n\\end{gather}\n$$\n\nThe numpyro model here make use of a double plate notation in order to specify a different `beta_spatial` for each county\n\n::: {#8eb0d198 .cell execution_count=34}\n``` {.python .cell-code}\nreparam_config = {\n    \"beta_spatial\": LocScaleReparam(0),\n}\n@numpyro.handlers.reparam(config=reparam_config)\ndef hierarchical_quantile_regression(\n    target: ArrayLike,\n    covariates: Dict[str, ArrayLike],\n    quantile: float,\n    prior_mu_beta_spatial: Distribution,\n    prior_sigma_beta_spatial: Distribution,\n    prior_scale: Distribution,\n    prior_beta_year: Distribution,\n    prior_beta_month: Distribution,\n    prior_beta_hour: Distribution,\n) -> None:\n    \"\"\"Quantile regression model with partially pooled intercept\"\"\"\n    n_groups = len(np.unique(covariates[\"counties_index\"]))\n    n_spatial_covariates = covariates[\"latitude_longitude_tensor_covariates\"].shape[1]\n    counties_index = covariates[\"counties_index\"].flatten()\n\n    mu_beta_spatial = numpyro.sample(\n        \"mu_beta_spatial\",\n        prior_mu_beta_spatial,\n    )\n    sigma_beta_spatial = numpyro.sample(\n        \"sigma_beta_spatial\",\n        prior_sigma_beta_spatial,\n    )\n    with numpyro.plate(\"counties\", n_groups, dim=-2):\n\n        with numpyro.plate(\"spline_coefficients\", n_spatial_covariates, dim=-1):\n\n            beta_spatial = numpyro.sample(\n                \"beta_spatial\",\n                Laplace(mu_beta_spatial, sigma_beta_spatial),\n            )\n\n    beta_hour = numpyro.sample(\n        \"beta_hour\",\n        prior_beta_hour.expand([covariates[\"hour_covariates\"].shape[1]]),\n    )\n    beta_month = numpyro.sample(\n        \"beta_month\",\n        prior_beta_month.expand([covariates[\"month_covariates\"].shape[1]]),\n    )\n    beta_year = numpyro.sample(\n        \"beta_year\",\n        prior_beta_year.expand([covariates[\"year_covariates\"].shape[1]]),\n    )\n    spatial_component = numpyro.deterministic(\n        name=\"spatial_component\",\n        value=jnp.sum(\n            beta_spatial[counties_index, :] * covariates[\"latitude_longitude_tensor_covariates\"],\n            axis=1,\n        ),\n    )\n    hour_component = numpyro.deterministic(\n        name=\"hour_component\",\n        value=jnp.dot(covariates[\"hour_covariates\"], beta_hour),\n    )\n    month_component = numpyro.deterministic(\n        name=\"month_component\",\n        value=jnp.dot(covariates[\"month_covariates\"], beta_month),\n    )\n    year_component = numpyro.deterministic(\n        name=\"year_component\",\n        value=jnp.dot(covariates[\"year_covariates\"], beta_year),\n    )\n    temporal_component = numpyro.deterministic(\n        name=\"temporal_component\",\n        value=year_component + month_component + hour_component,\n    )\n\n    loc = numpyro.deterministic(\n        name=\"loc\",\n        value=jnp.exp(spatial_component + temporal_component),\n    )\n    scale = numpyro.sample(\n        \"scale\",\n        prior_scale,\n    )\n    obs = numpyro.sample(\n        \"obs\",\n        AsymmetricLaplaceQuantile(loc=loc, scale=scale, quantile=quantile),\n        obs=target,\n    )\n    if target is not None:\n        numpyro.deterministic(\n            \"log_likelihood\",\n            AsymmetricLaplaceQuantile(\n                loc=loc, \n                scale=scale, \n                quantile=quantile,\n            )\n            .log_prob(target)\n        )\n\nhierarchical_quantile_model_kwargs = {\n    \"covariates\": quantile_regression_covariates,\n    \"quantile\": CRITICAL_QUANTILE,\n    \"target\": quantile_regression_target,\n    \"prior_mu_beta_spatial\": Normal(loc=0.0, scale=5.0),\n    \"prior_sigma_beta_spatial\": HalfCauchy(scale=2.0),\n    \"prior_beta_year\": Normal(loc=0.0, scale=1),\n    \"prior_beta_month\": Normal(loc=0.0, scale=1),\n    \"prior_beta_hour\": Normal(loc=0.0, scale=1),\n    \"prior_scale\": HalfNormal(scale=1),\n}\nhierarchical_quantile_model_parameters = [\n    \"mu_beta_latitude\",\n    \"sigma_beta_latitude\",\n    \"mu_beta_longitude\",\n    \"sigma_beta_longitude\",\n    \"beta_hour\",\n    \"beta_month\",\n    \"beta_year\",\n    \"loc\",\n    \"latitude_component\",\n    \"longitude_component\",\n    \"temporal_component\",\n    \"spatial_component\",\n    \"hour_component\",\n    \"month_component\",\n    \"year_component\",\n    \"scale\",\n    \"obs\",\n]\nnumpyro.render_model(\n    hierarchical_quantile_regression,\n    model_kwargs=hierarchical_quantile_model_kwargs,\n    render_distributions=False,\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n![](hail_risk_estimation_files/figure-html/cell-34-output-1.svg){}\n:::\n:::\n\n\n::: {#1bfbba4a .cell execution_count=35}\n``` {.python .cell-code}\n(\n    svi_hierarchical_quantile_regression_parameters,\n    svi_hierarchical_quantile_regression_guide,\n) = sample_using_svi(\n    rng_key=RNG_KEY,\n    model=hierarchical_quantile_regression,\n    model_kwargs=hierarchical_quantile_model_kwargs,\n    autoguide=AutoLowRankMultivariateNormal,\n    guide_kwargs={},\n    optimizer_kwargs={\"step_size\": 1e-4, \"clip_norm\": 5},\n    num_steps=NUMBER_ITERATIONS,\n    num_particles=NUMBER_PARTICLES,\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/100 [00:00<?, ?it/s]\r  1%|          | 1/100 [00:00<00:40,  2.42it/s]\r  7%|▋         | 7/100 [00:00<00:05, 17.21it/s, init loss: 71826.6406, avg. loss [1-5]: 71922.1875]\r 14%|█▍        | 14/100 [00:00<00:02, 30.19it/s, init loss: 71826.6406, avg. loss [6-10]: 71910.7891]\r 21%|██        | 21/100 [00:00<00:02, 39.23it/s, init loss: 71826.6406, avg. loss [16-20]: 71963.5156]\r 28%|██▊       | 28/100 [00:00<00:01, 45.85it/s, init loss: 71826.6406, avg. loss [21-25]: 71821.6719]\r 35%|███▌      | 35/100 [00:00<00:01, 50.27it/s, init loss: 71826.6406, avg. loss [31-35]: 71905.7266]\r 42%|████▏     | 42/100 [00:01<00:01, 53.57it/s, init loss: 71826.6406, avg. loss [36-40]: 71917.4219]\r 49%|████▉     | 49/100 [00:01<00:00, 55.89it/s, init loss: 71826.6406, avg. loss [41-45]: 71900.5156]\r 56%|█████▌    | 56/100 [00:01<00:00, 57.43it/s, init loss: 71826.6406, avg. loss [51-55]: 71542.4844]\r 63%|██████▎   | 63/100 [00:01<00:00, 58.71it/s, init loss: 71826.6406, avg. loss [56-60]: 71994.7031]\r 70%|███████   | 70/100 [00:01<00:00, 59.60it/s, init loss: 71826.6406, avg. loss [66-70]: 71393.4297]\r 77%|███████▋  | 77/100 [00:01<00:00, 60.15it/s, init loss: 71826.6406, avg. loss [71-75]: 71649.0703]\r 84%|████████▍ | 84/100 [00:01<00:00, 60.69it/s, init loss: 71826.6406, avg. loss [76-80]: 71787.6797]\r 91%|█████████ | 91/100 [00:01<00:00, 60.97it/s, init loss: 71826.6406, avg. loss [86-90]: 71647.8906]\r 98%|█████████▊| 98/100 [00:01<00:00, 61.16it/s, init loss: 71826.6406, avg. loss [91-95]: 71602.4609]\r100%|██████████| 100/100 [00:02<00:00, 49.27it/s, init loss: 71826.6406, avg. loss [96-100]: 71488.1406]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-35-output-2.png){}\n:::\n:::\n\n\n::: {#2f1140f9 .cell execution_count=36}\n``` {.python .cell-code}\nposterior_hierarchical_quantile_regression_svi = sample_posterior_predictive_svi(\n    rng_key=RNG_KEY,\n    covariates_hat=quantile_regression_covariates_hat,\n    model_kwargs=hierarchical_quantile_model_kwargs,\n    model=hierarchical_quantile_regression,\n    guide=svi_hierarchical_quantile_regression_guide,\n    svi_result=svi_hierarchical_quantile_regression_parameters,\n    num_samples=2000,\n    return_sites=hierarchical_quantile_model_parameters,\n)\n```\n:::\n\n\nAs we can see, the spatial component is now much more heterogenous across counties. Although this is the effect we wanted to achieve, some of the sharp variation at counties' edges do not look very natural. This is a desirable effect when environmental conditions justify this behaviour (e.g., the presence of mountains or terrain depressions) but it might also be due to the fact that we did not put any constraint forcing coninuity between contiguous boundaries.\n\n::: {#bf16b0fa .cell execution_count=37}\n``` {.python .cell-code}\nvisualize_geo_regression(\n    covariates_hat_df=quantile_regression_covariates_hat_df,\n    posterior=posterior_hierarchical_quantile_regression_svi,\n    parameter=\"spatial_component\",\n    parameter_transformer=lambda x: jnp.exp(x),\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-37-output-1.png){}\n:::\n:::\n\n\n::: {#4d7c2650 .cell execution_count=38}\n``` {.python .cell-code}\nfig, axs = visualize_temporal_components(\n    temporal_components=generate_temporal_components(\n        posterior=posterior_pooled_quantile_regression_svi,\n        transformers=quantile_regression_transformers,\n        years=continuous_state_modelling_df[YEAR_COVARIATES].unique(),\n        parameter_transformer=lambda x: jnp.exp(x),\n    )\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-38-output-1.png){}\n:::\n:::\n\n\n### Estimating the Likelyhood of Hail Events Using Zero-Inflated Negative Binomial Regression\n\n::: {#3dcefcb3 .cell execution_count=39}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\ndef create_estimation_covariates_zero_inflated_regression(\n    quantile_regression_covariates_hat_df: pd.DataFrame,\n    geometries_df: gpd.GeoDataFrame,\n) -> Tuple[gpd.GeoDataFrame, Dict[str, ArrayLike]]:\n    \"\"\"Create the covariates for estimation\"\"\"\n    zero_inflated_regression_covariates_df_hat = quantile_regression_covariates_hat_df[\n        [YEAR_COVARIATES, MONTH_COVARIATE, HOUR_COVARIATE, COUNTIES_INDEX, STATE_INDEX]\n    ].drop_duplicates()\n\n    zero_inflated_regression_covariates_df_hat = gpd.GeoDataFrame(\n        pd.merge(\n            zero_inflated_regression_covariates_df_hat,\n            geometries_df[[COUNTIES_INDEX, STATE_INDEX, \"geometry\"]],\n            how=\"inner\",\n            on=[COUNTIES_INDEX, STATE_INDEX],\n        )\n    )\n    zero_inflated_regression_covariates_hat = {\n        \"year_covariates\": (\n            zero_inflated_regression_covariates_df_hat[YEAR_COVARIATES].values.reshape(\n                -1, 1\n            )\n        ),\n        \"month_covariates\": (\n            zero_inflated_regression_covariates_df_hat[MONTH_COVARIATE].values.reshape(\n                -1, 1\n            )\n        ),\n        \"hour_covariates\": (\n            zero_inflated_regression_covariates_df_hat[HOUR_COVARIATE].values.reshape(\n                -1, 1\n            )\n        ),\n        \"counties_index\": (\n            zero_inflated_regression_covariates_df_hat[COUNTIES_INDEX].values.reshape(\n                -1, 1\n            )\n        ),\n    }\n    return (\n        zero_inflated_regression_covariates_df_hat,\n        zero_inflated_regression_covariates_hat,\n    )\nzero_inflated_regression_transformers = {\n    \"year_covariates\": Pipeline(\n        steps=[\n            (\n                \"ordinal_encoder\",\n                OrdinalEncoder(\n                    dtype=\"int\",\n                ),\n            ),\n            (\n                \"spline_transformer\",\n                SplineTransformer(\n                    include_bias=False,\n                ),\n            ),\n        ]\n    ),\n    \"month_covariates\": Pipeline(\n        steps=[\n            (\n                \"spline_transformer\",\n                SplineTransformer(\n                    include_bias=False,\n                ),\n            )\n        ]\n    ),\n    \"hour_covariates\": Pipeline(\n        steps=[\n            (\n                \"spline_transformer\",\n                SplineTransformer(\n                    include_bias=False,\n                ),\n            )\n        ]\n    ),\n    \"counties_index\": OrdinalEncoder(\n        dtype=\"int\",\n    ),\n}\n\nzero_inflated_regression_covariates = {\n    \"year_covariates\": (\n        count_state_modelling_df[YEAR_COVARIATES].values.reshape(-1, 1)\n    ),\n    \"month_covariates\": (\n        count_state_modelling_df[MONTH_COVARIATE].values.reshape(-1, 1)\n    ),\n    \"hour_covariates\": (count_state_modelling_df[HOUR_COVARIATE].values.reshape(-1, 1)),\n    \"counties_index\": (count_state_modelling_df[COUNTIES_INDEX].values.reshape(-1, 1)),\n}\nzero_inflated_regression_covariates_hat_df, zero_inflated_regression_covariates_hat = (\n    create_estimation_covariates_zero_inflated_regression(\n        quantile_regression_covariates_hat_df=quantile_regression_covariates_hat_df,\n        geometries_df=geometries_df,\n    )\n)\n\n(\n    zero_inflated_regression_covariates,\n    zero_inflated_regression_transformers,\n) = transform_fitting_covariates(\n    covariates=zero_inflated_regression_covariates,\n    transformers=zero_inflated_regression_transformers,\n)\nzero_inflated_regression_covariates_hat = transform_estimation_covariates(\n    covariates=zero_inflated_regression_covariates_hat,\n    transformers=zero_inflated_regression_transformers,\n)\nzero_inflated_regression_target = count_state_modelling_df[COUNT_TARGET].values\n```\n:::\n\n\n**Fully Pooled**\n\n$$\n\\begin{gather}\n\\color{RedOrange}\\alpha_{Gate} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\color{NavyBlue}\\alpha_{Mean} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\\\\n\\color{RedOrange}\\beta_{GateHour} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\color{RedOrange}\\beta_{GateMonth} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\color{RedOrange}\\beta_{GateYear} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\color{NavyBlue}\\beta_{MeanHour} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\color{NavyBlue}\\beta_{MeanMonth} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\color{NavyBlue}\\beta_{MeanYear} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\\\\n\\color{RedOrange}p = expit(\\alpha_{Gate} + \\beta_{GateHour}f(hour) + \\beta_{GateMonth}f(month) + \\beta_{GateYear}f(year))\\\\\n\\color{NavyBlue}\\mu = exp(\\alpha_{Mean} + \\beta_{MeanHour}f(hour) + \\beta_{MeanMonth}f(month) + \\beta_{GateYear}f(year))\\\\\n\\\\\n\\lambda \\sim InverseGamma(0.3, 0.4)\\\\\n\n\\ y \\sim ZeroInflatedNegativeBinomial(p, \\mu, \\lambda)\n\\end{gather}\n$$\n\n::: {#2f77bd5d .cell execution_count=40}\n``` {.python .cell-code}\nfrom jax.scipy.special import expit\nfrom jax import numpy as jnp\n\nfrom numpyro.distributions import (\n    NegativeBinomial2,\n    InverseGamma,\n    ZeroInflatedDistribution,\n)\n\n\ndef pooled_zero_inflated_negative_binomial_regression(\n    target: ArrayLike,\n    covariates: Dict[str, ArrayLike],\n    prior_rate: Distribution,\n    prior_alpha_gate: Distribution,\n    prior_beta_year_gate: Distribution,\n    prior_beta_month_gate: Distribution,\n    prior_beta_hour_gate: Distribution,\n    prior_alpha_mean: Distribution,\n    prior_beta_year_mean: Distribution,\n    prior_beta_month_mean: Distribution,\n    prior_beta_hour_mean: Distribution,\n) -> None:\n    \"\"\"Simple zero-inflated negative binomial regression model\"\"\"\n    alpha_gate = numpyro.sample(\n        \"alpha_gate\",\n        prior_alpha_gate,\n    )\n    alpha_mean = numpyro.sample(\n        \"alpha_mean\",\n        prior_alpha_mean,\n    )\n\n    with numpyro.plate(\n        \"year_spline_coefficients\", size=covariates[\"year_covariates\"].shape[1]\n    ):\n        beta_year_gate = numpyro.sample(\n            \"beta_year_gate\",\n            prior_beta_year_gate,\n        )\n        beta_year_mean = numpyro.sample(\n            \"beta_year_mean\",\n            prior_beta_year_mean,\n        )\n\n    with numpyro.plate(\n        \"hour_spline_coefficients\", size=covariates[\"hour_covariates\"].shape[1]\n    ):\n        beta_hour_gate = numpyro.sample(\n            \"beta_hour_gate\",\n            prior_beta_hour_gate,\n        )\n        beta_hour_mean = numpyro.sample(\n            \"beta_hour_mean\",\n            prior_beta_hour_mean,\n        )\n\n    with numpyro.plate(\n        \"month_spline_coefficients\", size=covariates[\"month_covariates\"].shape[1]\n    ):\n        beta_month_gate = numpyro.sample(\n            \"beta_month_gate\",\n            prior_beta_month_gate,\n        )\n        beta_month_mean = numpyro.sample(\n            \"beta_month_mean\",\n            prior_beta_month_mean,\n        )\n\n    # Year component\n    year_component_gate = numpyro.deterministic(\n        name=\"year_component_gate\",\n        value=jnp.dot(covariates[\"year_covariates\"], beta_year_gate),\n    )\n    year_component_mean = numpyro.deterministic(\n        name=\"year_component_mean\",\n        value=jnp.dot(covariates[\"year_covariates\"], beta_year_mean),\n    )\n    # Month component\n    month_component_gate = numpyro.deterministic(\n        name=\"month_component_gate\",\n        value=jnp.dot(covariates[\"month_covariates\"], beta_month_gate),\n    )\n    month_component_mean = numpyro.deterministic(\n        name=\"month_component_mean\",\n        value=jnp.dot(covariates[\"month_covariates\"], beta_month_mean),\n    )\n    # Hour component\n    hour_component_gate = numpyro.deterministic(\n        name=\"hour_component_gate\",\n        value=jnp.dot(covariates[\"hour_covariates\"], beta_hour_gate),\n    )\n    hour_component_mean = numpyro.deterministic(\n        name=\"hour_component_mean\",\n        value=jnp.dot(covariates[\"hour_covariates\"], beta_hour_mean),\n    )\n\n    # Temporal components\n    temporal_component_gate = numpyro.deterministic(\n        name=\"temporal_component_gate\",\n        value=year_component_gate + month_component_gate + hour_component_gate,\n    )\n    temporal_component_mean = numpyro.deterministic(\n        name=\"temporal_component_mean\",\n        value=year_component_mean + month_component_mean + hour_component_mean,\n    )\n\n    gate = numpyro.deterministic(\n        name=\"gate\",\n        value=1 - expit(alpha_gate + temporal_component_gate),\n    )\n    mean = numpyro.deterministic(\n        name=\"mean\",\n        value=jnp.exp(alpha_mean + temporal_component_mean),\n    )\n    rate = numpyro.sample(\n        \"rate\",\n        prior_rate,\n    )\n\n    obs = numpyro.sample(\n        \"obs\",\n        ZeroInflatedDistribution(\n            base_dist=NegativeBinomial2(mean, rate),\n            gate=gate,\n        ),\n        obs=target,\n    )\n    if target is not None:\n        numpyro.deterministic(\n            \"log_likelihood\",\n            ZeroInflatedDistribution(\n                base_dist=NegativeBinomial2(mean, rate),\n                gate=gate,\n            )\n            .log_prob(target)\n        )\n\npooled_zero_inflated_regression_parameters = [\n    \"rate\",\n    \"alpha_gate\",\n    \"beta_year_gate\",\n    \"beta_month_gate\",\n    \"beta_hour_gate\",\n    \"alpha_mean\",\n    \"beta_year_mean\",\n    \"beta_month_mean\",\n    \"beta_hour_mean\",\n    \"gate\",\n    \"mean\",\n    \"obs\",\n]\npooled_zero_inflated_regression_kwargs = {\n    \"covariates\": zero_inflated_regression_covariates,\n    \"target\": zero_inflated_regression_target,\n    \"prior_rate\": InverseGamma(0.4, 0.3),\n    \"prior_alpha_gate\": Normal(loc=0.0, scale=1.),\n    \"prior_beta_year_gate\": Normal(loc=0.0, scale=1.),\n    \"prior_beta_month_gate\": Normal(loc=0.0, scale=1.),\n    \"prior_beta_hour_gate\": Normal(loc=0.0, scale=1.),\n    \"prior_alpha_mean\": Normal(loc=0.0, scale=1.),\n    \"prior_beta_year_mean\": Normal(loc=0.0, scale=1.),\n    \"prior_beta_month_mean\": Normal(loc=0.0, scale=1.),\n    \"prior_beta_hour_mean\": Normal(loc=0.0, scale=1.),\n}\nnumpyro.render_model(\n    pooled_zero_inflated_negative_binomial_regression,\n    model_kwargs=pooled_zero_inflated_regression_kwargs,\n    render_distributions=False,\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n![](hail_risk_estimation_files/figure-html/cell-40-output-1.svg){}\n:::\n:::\n\n\n::: {#56e62bda .cell execution_count=41}\n``` {.python .cell-code}\n(\n    svi_pooled_zero_inflated_regression_parameters,\n    svi_pooled_zero_inflated_regression_guide,\n) = sample_using_svi(\n    rng_key=RNG_KEY,\n    model=pooled_zero_inflated_negative_binomial_regression,\n    model_kwargs=pooled_zero_inflated_regression_kwargs,\n    autoguide=AutoLowRankMultivariateNormal,\n    guide_kwargs={},\n    optimizer_kwargs={\"step_size\": 1e-4, \"clip_norm\": 5},\n    num_steps=NUMBER_ITERATIONS,\n    num_particles=NUMBER_PARTICLES,\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/100 [00:00<?, ?it/s]\r  1%|          | 1/100 [00:00<01:08,  1.45it/s]\r 10%|█         | 10/100 [00:00<00:05, 16.40it/s, init loss: 26211.6211, avg. loss [6-10]: 26363.3594]\r 20%|██        | 20/100 [00:00<00:02, 32.13it/s, init loss: 26211.6211, avg. loss [16-20]: 26311.1816]\r 30%|███       | 30/100 [00:01<00:01, 45.70it/s, init loss: 26211.6211, avg. loss [26-30]: 27444.9961]\r 40%|████      | 40/100 [00:01<00:01, 57.00it/s, init loss: 26211.6211, avg. loss [36-40]: 27344.7129]\r 50%|█████     | 50/100 [00:01<00:00, 65.83it/s, init loss: 26211.6211, avg. loss [46-50]: 27010.3789]\r 60%|██████    | 60/100 [00:01<00:00, 72.68it/s, init loss: 26211.6211, avg. loss [56-60]: 26880.2246]\r 70%|███████   | 70/100 [00:01<00:00, 77.86it/s, init loss: 26211.6211, avg. loss [66-70]: 27138.0352]\r 80%|████████  | 80/100 [00:01<00:00, 81.87it/s, init loss: 26211.6211, avg. loss [76-80]: 26617.7383]\r 90%|█████████ | 90/100 [00:01<00:00, 84.24it/s, init loss: 26211.6211, avg. loss [86-90]: 26915.8555]\r 99%|█████████▉| 99/100 [00:01<00:00, 85.46it/s, init loss: 26211.6211, avg. loss [91-95]: 26201.1523]\r100%|██████████| 100/100 [00:01<00:00, 55.88it/s, init loss: 26211.6211, avg. loss [96-100]: 27439.7871]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-41-output-2.png){}\n:::\n:::\n\n\n::: {#b1365151 .cell execution_count=42}\n``` {.python .cell-code}\nposterior_pooled_zero_inflated_regression_svi = sample_posterior_predictive_svi(\n    rng_key=RNG_KEY,\n    model=pooled_zero_inflated_negative_binomial_regression,\n    guide=svi_pooled_zero_inflated_regression_guide,\n    covariates_hat=zero_inflated_regression_covariates_hat,\n    svi_result=svi_pooled_zero_inflated_regression_parameters,\n    num_samples=1500,\n    model_kwargs=pooled_zero_inflated_regression_kwargs,\n    return_sites=pooled_zero_inflated_regression_parameters,\n)\n```\n:::\n\n\n::: {#425146b5 .cell execution_count=43}\n``` {.python .cell-code}\nvisualize_geo_regression(\n    covariates_hat_df=zero_inflated_regression_covariates_hat_df,\n    posterior=posterior_pooled_zero_inflated_regression_svi,\n    parameter=\"alpha_gate\",\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-43-output-1.png){}\n:::\n:::\n\n\n::: {#f81b7b35 .cell execution_count=44}\n``` {.python .cell-code}\nvisualize_geo_regression(\n    covariates_hat_df=zero_inflated_regression_covariates_hat_df,\n    posterior=posterior_pooled_zero_inflated_regression_svi,\n    parameter=\"alpha_mean\",\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-44-output-1.png){}\n:::\n:::\n\n\n::: {#28ee6ca3 .cell execution_count=45}\n``` {.python .cell-code}\nfig, axs = visualize_temporal_components(\n    temporal_components=generate_temporal_components(\n        posterior=posterior_pooled_zero_inflated_regression_svi,\n        transformers=zero_inflated_regression_transformers,\n        years=count_state_modelling_df[YEAR_COVARIATES].unique(),\n        suffix=\"_gate\",\n    )\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-45-output-1.png){}\n:::\n:::\n\n\n::: {#772fea51 .cell execution_count=46}\n``` {.python .cell-code}\nfig, axs = visualize_temporal_components(\n    temporal_components=generate_temporal_components(\n        posterior=posterior_pooled_zero_inflated_regression_svi,\n        transformers=zero_inflated_regression_transformers,\n        years=count_state_modelling_df[YEAR_COVARIATES].unique(),\n        suffix=\"_mean\",\n    )\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-46-output-1.png){}\n:::\n:::\n\n\n**Partially Pooled**\n\n$$\n\\begin{gather}\n\\color{RedOrange}\\sigma_{Gate, County} \\sim HalfCauchy(\\sigma=5) \\\\\n\\color{RedOrange}\\mu_{Gate, County} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\color{RedOrange}\\alpha_{Gate, County} \\sim \\mathcal{N}(\\mu_{GateCounty}, \\sigma_{GateCounty}) \\\\\n\n\\color{NavyBlue}\\sigma_{Mean, County} \\sim HalfCauchy(\\sigma=5) \\\\\n\\color{NavyBlue}\\mu_{Mean, County} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\color{NavyBlue}\\alpha_{Mean, County} \\sim \\mathcal{N}(\\mu_{MeanCounty}, \\sigma_{MeanCounty}) \\\\\n\\\\\n\\color{RedOrange}\\beta_{GateHour} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\color{RedOrange}\\beta_{GateMonth} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\color{RedOrange}\\beta_{GateYear} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\color{NavyBlue}\\beta_{MeanHour} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\color{NavyBlue}\\beta_{MeanMonth} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\color{NavyBlue}\\beta_{MeanYear} \\sim \\mathcal{N}(\\mu=0, \\sigma=1) \\\\\n\\\\\n\\color{RedOrange}p = expit(\\alpha_{Gate, County} + \\beta_{GateHour}f(hour) + \\beta_{GateMonth}f(month) + \\beta_{GateYear}f(year))\\\\\n\\color{NavyBlue}\\mu = exp(\\alpha_{Mean, County} + \\beta_{MeanHour}f(hour) + \\beta_{MeanMonth}f(month) + \\beta_{GateYear}f(year))\\\\\n\\\\\n\\lambda \\sim InverseGamma(0.3, 0.4)\\\\\n\n\\ y \\sim ZeroInflatedNegativeBinomial(p, \\mu, \\lambda)\n\\end{gather}\n$$\n\n::: {#83c994bc .cell execution_count=47}\n``` {.python .cell-code}\nreparam_config = {\"alpha_gate\": LocScaleReparam(0), \"alpha_mean\": LocScaleReparam(0)}\n\n\n@numpyro.handlers.reparam(config=reparam_config)\ndef hierarchical_zero_inflated_negative_binomial_regression(\n    target: ArrayLike,\n    covariates: Dict[str, ArrayLike],\n    prior_rate: Distribution,\n    prior_beta_year_gate: Distribution,\n    prior_beta_month_gate: Distribution,\n    prior_beta_hour_gate: Distribution,\n    prior_beta_year_mean: Distribution,\n    prior_beta_month_mean: Distribution,\n    prior_beta_hour_mean: Distribution,\n    prior_mu_alpha_gate: Distribution,\n    prior_sigma_alpha_gate: Distribution,\n    prior_mu_alpha_mean: Distribution,\n    prior_sigma_alpha_mean: Distribution,\n) -> None:\n    \"\"\"Hierarchical zero-inflated Negative Binomial regression model\"\"\"\n    number_groups = len(np.unique(covariates[\"counties_index\"]))\n    counties_index = covariates[\"counties_index\"].flatten()\n\n    mu_alpha_gate = numpyro.sample(\n        \"mu_alpha_gate\",\n        prior_mu_alpha_gate,\n    )\n    sigma_alpha_gate = numpyro.sample(\n        \"sigma_alpha_gate\",\n        prior_sigma_alpha_gate,\n    )\n    mu_alpha_mean = numpyro.sample(\n        \"mu_alpha_mean\",\n        prior_mu_alpha_mean,\n    )\n    sigma_alpha_mean = numpyro.sample(\n        \"sigma_alpha_mean\",\n        prior_sigma_alpha_mean,\n    )\n\n    with numpyro.plate(\"counties\", size=number_groups):\n\n        alpha_gate = numpyro.sample(\n            \"alpha_gate\",\n            Normal(mu_alpha_gate, sigma_alpha_gate),\n        )\n        alpha_mean = numpyro.sample(\n            \"alpha_mean\",\n            Normal(mu_alpha_mean, sigma_alpha_mean),\n        )\n\n    with numpyro.plate(\n        \"year_spline_coefficients\", size=covariates[\"year_covariates\"].shape[1]\n    ):\n        beta_year_gate = numpyro.sample(\n            \"beta_year_gate\",\n            prior_beta_year_gate,\n        )\n        beta_year_mean = numpyro.sample(\n            \"beta_year_mean\",\n            prior_beta_year_mean,\n        )\n\n    with numpyro.plate(\n        \"hour_spline_coefficients\", size=covariates[\"hour_covariates\"].shape[1]\n    ):\n        beta_hour_gate = numpyro.sample(\n            \"beta_hour_gate\",\n            prior_beta_hour_gate,\n        )\n        beta_hour_mean = numpyro.sample(\n            \"beta_hour_mean\",\n            prior_beta_hour_mean,\n        )\n\n    with numpyro.plate(\n        \"month_spline_coefficients\", size=covariates[\"month_covariates\"].shape[1]\n    ):\n        beta_month_gate = numpyro.sample(\n            \"beta_month_gate\",\n            prior_beta_month_gate,\n        )\n        beta_month_mean = numpyro.sample(\n            \"beta_month_mean\",\n            prior_beta_month_mean,\n        )\n\n    # Year component\n    year_component_gate = numpyro.deterministic(\n        name=\"year_component_gate\",\n        value=jnp.dot(covariates[\"year_covariates\"], beta_year_gate),\n    )\n    year_component_mean = numpyro.deterministic(\n        name=\"year_component_mean\",\n        value=jnp.dot(covariates[\"year_covariates\"], beta_year_mean),\n    )\n    # Month component\n    month_component_gate = numpyro.deterministic(\n        name=\"month_component_gate\",\n        value=jnp.dot(covariates[\"month_covariates\"], beta_month_gate),\n    )\n    month_component_mean = numpyro.deterministic(\n        name=\"month_component_mean\",\n        value=jnp.dot(covariates[\"month_covariates\"], beta_month_mean),\n    )\n    # Hour component\n    hour_component_gate = numpyro.deterministic(\n        name=\"hour_component_gate\",\n        value=jnp.dot(covariates[\"hour_covariates\"], beta_hour_gate),\n    )\n    hour_component_mean = numpyro.deterministic(\n        name=\"hour_component_mean\",\n        value=jnp.dot(covariates[\"hour_covariates\"], beta_hour_mean),\n    )\n\n    # Temporal components\n    temporal_component_gate = numpyro.deterministic(\n        name=\"temporal_component_gate\",\n        value=year_component_gate + month_component_gate + hour_component_gate,\n    )\n    temporal_component_mean = numpyro.deterministic(\n        name=\"temporal_component_mean\",\n        value=year_component_mean + month_component_mean + hour_component_mean,\n    )\n\n    spatial_component_mean = numpyro.deterministic(\n        name=\"spatial_component_mean\",\n        value=alpha_mean[counties_index],\n    )\n\n    spatial_component_gate = numpyro.deterministic(\n        name=\"spatial_component_gate\",\n        value=alpha_gate[counties_index],\n    )\n\n    gate = numpyro.deterministic(\n        name=\"gate\",\n        value=1 - expit(spatial_component_gate + temporal_component_gate),\n    )\n    mean = numpyro.deterministic(\n        name=\"mean\",\n        value=jnp.exp(spatial_component_mean + temporal_component_mean),\n    )\n    rate = numpyro.sample(\n        \"rate\",\n        prior_rate,\n    )\n\n    obs = numpyro.sample(\n        \"obs\",\n        ZeroInflatedDistribution(\n            base_dist=NegativeBinomial2(mean, rate),\n            gate=gate,\n        ),\n        obs=target,\n    )\n\n    if target is not None:\n        numpyro.deterministic(\n            \"log_likelihood\",\n            ZeroInflatedDistribution(\n                base_dist=NegativeBinomial2(mean, rate),\n                gate=gate,\n            )\n            .log_prob(target)\n        )\n\nhierarchical_zero_inflated_regression_parameters = [\n    \"rate\",\n    \"mean\",\n    \"gate\",\n    \"mu_alpha_gate\",\n    \"sigma_alpha_gate\",\n    \"mu_alpha_mean\",\n    \"sigma_alpha_mean\",\n    \"alpha_gate\",\n    \"spatial_component_mean\",\n    \"spatial_component_gate\",\n    \"alpha_mean\",\n    \"beta_year_mean\",\n    \"beta_month_mean\",\n    \"beta_hour_mean\",\n    \"beta_year_gate\",\n    \"beta_month_gate\",\n    \"beta_hour_gate\",\n    \"obs\",\n]\nhierarchical_zero_inflated_regression_kwargs = {\n    \"covariates\": zero_inflated_regression_covariates,\n    \"target\": zero_inflated_regression_target,\n    \"prior_rate\": InverseGamma(0.4, 0.3),\n    \"prior_mu_alpha_gate\": Normal(loc=0.0, scale=1),\n    \"prior_sigma_alpha_gate\": HalfCauchy(5),\n    \"prior_mu_alpha_mean\": Normal(loc=0.0, scale=1),\n    \"prior_sigma_alpha_mean\": HalfCauchy(5),\n    \"prior_beta_year_gate\": Normal(loc=0.0, scale=1),\n    \"prior_beta_month_gate\": Normal(loc=0.0, scale=1),\n    \"prior_beta_hour_gate\": Normal(loc=0.0, scale=1),\n    \"prior_beta_year_mean\": Normal(loc=0.0, scale=1),\n    \"prior_beta_month_mean\": Normal(loc=0.0, scale=1),\n    \"prior_beta_hour_mean\": Normal(loc=0.0, scale=1),\n}\nnumpyro.render_model(\n    hierarchical_zero_inflated_negative_binomial_regression,\n    model_kwargs=hierarchical_zero_inflated_regression_kwargs,\n    render_distributions=False,\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=46}\n![](hail_risk_estimation_files/figure-html/cell-47-output-1.svg){}\n:::\n:::\n\n\n::: {#8f8e1b95 .cell execution_count=48}\n``` {.python .cell-code}\n(\n    svi_hierarchical_zero_inflated_regression_parameters,\n    svi_hierarchical_zero_inflated_regression_guide,\n) = sample_using_svi(\n    rng_key=RNG_KEY,\n    model=hierarchical_zero_inflated_negative_binomial_regression,\n    model_kwargs=hierarchical_zero_inflated_regression_kwargs,\n    autoguide=AutoLowRankMultivariateNormal,\n    guide_kwargs={},\n    optimizer_kwargs={\"step_size\": 1e-4, \"clip_norm\": 5},\n    num_steps=NUMBER_ITERATIONS,\n    num_particles=NUMBER_PARTICLES,\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/100 [00:00<?, ?it/s]\r  1%|          | 1/100 [00:00<01:18,  1.27it/s]\r  9%|▉         | 9/100 [00:00<00:06, 13.31it/s, init loss: 35326.1836, avg. loss [1-5]: 39163.5781]\r 18%|█▊        | 18/100 [00:00<00:03, 26.89it/s, init loss: 35326.1836, avg. loss [11-15]: 39138.2695]\r 27%|██▋       | 27/100 [00:01<00:01, 39.43it/s, init loss: 35326.1836, avg. loss [21-25]: 36557.6445]\r 36%|███▌      | 36/100 [00:01<00:01, 50.22it/s, init loss: 35326.1836, avg. loss [31-35]: 37389.9453]\r 45%|████▌     | 45/100 [00:01<00:00, 59.31it/s, init loss: 35326.1836, avg. loss [41-45]: 38235.6836]\r 54%|█████▍    | 54/100 [00:01<00:00, 66.32it/s, init loss: 35326.1836, avg. loss [46-50]: 36664.7383]\r 63%|██████▎   | 63/100 [00:01<00:00, 71.98it/s, init loss: 35326.1836, avg. loss [56-60]: 36305.8047]\r 72%|███████▏  | 72/100 [00:01<00:00, 76.32it/s, init loss: 35326.1836, avg. loss [66-70]: 38509.9609]\r 81%|████████  | 81/100 [00:01<00:00, 79.57it/s, init loss: 35326.1836, avg. loss [76-80]: 35461.6250]\r 90%|█████████ | 90/100 [00:01<00:00, 81.88it/s, init loss: 35326.1836, avg. loss [86-90]: 38346.7344]\r 99%|█████████▉| 99/100 [00:01<00:00, 83.04it/s, init loss: 35326.1836, avg. loss [91-95]: 36974.0664]\r100%|██████████| 100/100 [00:01<00:00, 51.59it/s, init loss: 35326.1836, avg. loss [96-100]: 37281.2969]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-48-output-2.png){}\n:::\n:::\n\n\n::: {#e93aa830 .cell execution_count=49}\n``` {.python .cell-code}\nposterior_hierarchical_zero_inflated_regression_svi = sample_posterior_predictive_svi(\n    rng_key=RNG_KEY,\n    model=hierarchical_zero_inflated_negative_binomial_regression,\n    guide=svi_hierarchical_zero_inflated_regression_guide,\n    covariates_hat=zero_inflated_regression_covariates_hat,\n    svi_result=svi_hierarchical_zero_inflated_regression_parameters,\n    num_samples=2000,\n    model_kwargs=hierarchical_zero_inflated_regression_kwargs,\n    return_sites=hierarchical_zero_inflated_regression_parameters,\n)\n```\n:::\n\n\n::: {#d7b902f5 .cell execution_count=50}\n``` {.python .cell-code}\nvisualize_geo_regression(\n    covariates_hat_df=zero_inflated_regression_covariates_hat_df,\n    posterior=posterior_hierarchical_zero_inflated_regression_svi,\n    parameter=\"spatial_component_gate\",\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-50-output-1.png){}\n:::\n:::\n\n\n::: {#633be085 .cell execution_count=51}\n``` {.python .cell-code}\nvisualize_geo_regression(\n    covariates_hat_df=zero_inflated_regression_covariates_hat_df,\n    posterior=posterior_hierarchical_zero_inflated_regression_svi,\n    parameter=\"spatial_component_mean\",\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-51-output-1.png){}\n:::\n:::\n\n\n::: {#ba1e6668 .cell execution_count=52}\n``` {.python .cell-code}\nfig, axs = visualize_temporal_components(\n    temporal_components=generate_temporal_components(\n        posterior=posterior_hierarchical_zero_inflated_regression_svi,\n        transformers=zero_inflated_regression_transformers,\n        years=count_state_modelling_df[YEAR_COVARIATES].unique(),\n        suffix=\"_gate\",\n    )\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-52-output-1.png){}\n:::\n:::\n\n\n::: {#eb6bf5af .cell execution_count=53}\n``` {.python .cell-code}\nfig, axs = visualize_temporal_components(\n    temporal_components=generate_temporal_components(\n        posterior=posterior_hierarchical_zero_inflated_regression_svi,\n        transformers=zero_inflated_regression_transformers,\n        years=count_state_modelling_df[YEAR_COVARIATES].unique(),\n        suffix=\"_mean\",\n    )\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hail_risk_estimation_files/figure-html/cell-53-output-1.png){}\n:::\n:::\n\n\n",
    "supporting": [
      "hail_risk_estimation_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}