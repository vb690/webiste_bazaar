{
  "hash": "dbe3a31a08449c96b787c6ce53270d01",
  "result": {
    "markdown": "---\ntitle: LinUCB for Contextual Multi-Armed Bandit\ndescription: 'This post how to implement the Linear Upper Confindence Bound [@li2010contextual] algorithm in [JAX](https://jax.readthedocs.io/en/latest/#) and applying it to a simulated contextual multi-armed bandit problem.'\ndate: '2023-09-10'\ncategories:\n  - JAX\n  - reinforcement learning\n  - LinUCB\n  - contextual bandits\nbibliography: bibliographies/lin_ucb_jax.bib\n---\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\nfrom typing import Tuple, List, Any\n\nfrom functools import partial\n\nimport numpy as np\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.manifold import TSNE\n\nfrom jax.typing import ArrayLike\nfrom jax.lax import scan\nfrom jax import vmap\nfrom jax import numpy as jnp\nfrom jax import random, jit\nfrom jax.scipy.linalg import inv\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\n@jit\ndef tempered_softmax(logits, temperature=1):\n    \"\"\"Produce a tempered softmax given logits.\n\n    Args:\n        logits (ArrayLike): logits to be turned into probability.\n        temperature (int, optional): parameter controlling the softness\n        of the function, the higher the value the more soft is the function. \n        Defaults to 1.\n\n    Returns:\n        ArrayLike: simplex derived from the input logits.\n    \"\"\"\n    nominator = jnp.exp(logits / temperature)\n    denominator = jnp.sum(nominator, axis=1).reshape(-1, 1)\n    return nominator / denominator\n```\n:::\n\n\n# Premise\n\nWe want to stress how what is presented here is not at all novel but rather an exercise for leveraging the nice features that JAX offers and applying them for making the solution of a specific problem more efficient. Alot of credit for this post goes to the autohor of the original LinUCB paper, the contributors to the JAX library and to Kenneth Foo Fangwei for [this](https://kfoofw.github.io/contextual-bandits-linear-ucb-disjoint/) very clear blogpost explaining the fundamentals of the algorithm.\n\n## What we will cover\n\n1. Very brief introduction to multi armed and contextual multi armed bandit problems.\n2. Very brief introduction to the LinUCB algorithm.\n3. Simulating a disjoint contentual multi armed bandit problem.\n4. Implementing the LinUCB algorithm in JAX.\n5. Testing the algorithm on simulated data.\n6. Accelerating testing and simulation using a GPU.\n7. Evaluating the performance of the algorithm.\n\n## What we will **not** cover\n\n1. JAX fundamentals.\n2. In depth expalantion of multi-armed and contextual multi-armed bandit problems.\n3. In depth expalantion of the LinUCB algorithm.\n\n# Introduction\n\n## Multi-Armed Bandit Problem\n\nThe multi-armed bandit problem describes a situation where an agent is faced with $K = (k_0, k_1, \\dots, k_n)$ different options (or arms), each one with an associated unknown payoff (or reward) $r_k$^[The assumption is that the payoff comes from a stationary distribution, meaning that at any point in time we can expect that $r_k \\sim \\mathcal{N}(\\mu_k, \\sigma_k)$ (or any other suitable probability distribution).] [@sutton2018reinforcement].\n\nThe goal of the agent is to select, over a finite sequence of interactions $T=(t_0, t_1, \\dots, t_n)$, the set of options that will maximize the expected total payoff over $T$ [@sutton2018reinforcement]. What our agent is \ninterested in then is the true value $q*$ of taking an action $a$ and selecting a given arm $k$, the action associated with highest value should be the go-to strategy for maximizing the cumulative payoff\n\nSince the true value is not known, our agent often has to rely on an estimate of such value which comes with an associated level of uncertainity. We can think of this in terms of the relationship between the mean of the distribution from which the rewards of a given arm are sampled and its empirical estimate with associated standard error.\n\nSelecting the best set of actions over a finite number of interactions then requires a balance between and **exploitative** and **explorative behaviour**\n\n1. Exploit the options with the highest associated estimated reward \n2. Allow the exploration of other options in case our exploitative behaviour has been biased by noisy estimates. \n\n## Contextual Multi-Armed Bandit Problem\n\nThe conventional multi-armed bandit scenario attempts to solve what is called a nonassociative task, meaning that the payoff of a given action (e.g. selecting one of the $k$ available arms) doesn't depend on any context. This means that as a measure of value for a given action, we are interested in \n\n$\\mathbb{E}[r | K=k]$\n\nIn a contextual multi-amred bandit scenario instead, the payoff of a given action is dependent on the context in which the action is performed. This implies that given a matrix of context vectors $X_{K\\times h}$ we try to estimate the value of a given action as\n\n$\\mathbb{E}[r | K=k, X=x_k]$\n\nIn order to get a better understanding of what we mean here, let's simulate a potential generating process that could give rise to data suitable for a contextual multi-armed bandit problem.\n\n### Create the Simulation Dataset\n\nWe will approximate the data generating process using sklearn's [`make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) function. The idea is to re-formulate this as a multi-class classification problem where the context are features able to influece the probability to pick one of `n_arms` classes.\n\nIn order to simulate some of the challenges we could face in a real world setting we will add the following hurdles:\n\n1. Only a small portion of the fetaures will actually have predictive power on which arm is the most promising.\n2. We will generate features that have a ceertain degree of overlap (immagine them being drawn from relatively spread-out distributions)\n3. We will enforce sparsity on the reward generated by each arm. This is like saying that the reward generated by pulling a given arm comes from a zero inflated distribution of the form\n\n$$\n\\begin{gather}\nhurdle \\sim Bernoulli(p_{hurdle}) \\\\\nP(r | K=k, X=x_k) = \\alpha x_k \\\\\nr = \\begin{cases} 0,& \\text{if hurdle} = 0 \\\\ \nBernoulli(p_{r}),& \\text{otherwise} \\end{cases}\n\\end{gather}\n$$\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=2}\n``` {.python .cell-code}\nUNIT_CONTEXT_SIZE = 1\nUNIT_ARMS = 2\n\nINFORMATIVE = 2\nREPEATED = 0\nREDUNDANT  = 0\nRANDOM  = 8\n\nCONTEXT_SIZE = INFORMATIVE + REPEATED + REDUNDANT + RANDOM\n\nREWARD_SPARSITY = 0.99\nREWARD_WEIGHTING = 1 / (1 - REWARD_SPARSITY)\n\nN_USERS = 1000\n\nN_ARMS = UNIT_ARMS ** 2 # We ensure we can always plot the arms in a squared grid\nCLASS_SEP = .9\n```\n:::\n\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\ndef generate_context(n_arms, n_users, context_size, redundant, repeated, informative, clusters_per_class, class_sep, with_intercept=True):\n    context, groups = make_classification(\n        n_classes=n_arms, \n        n_samples=n_users, \n        n_features=context_size,\n        n_redundant=redundant,\n        n_repeated=repeated,\n        n_informative=informative,\n        n_clusters_per_class=clusters_per_class,\n        class_sep=class_sep\n    )\n    context = (context - context.mean(0)) / context.std(0)\n    if with_intercept:\n\n        context = np.hstack([context, np.ones(shape=(context.shape[0], 1))])\n    \n    return context, groups\n\ndef compute_arms_probabilities(context, groups, with_intercept=True, temperature=5):\n    model = LogisticRegression(fit_intercept=not with_intercept).fit(context, groups)\n    weights = model.coef_.T\n\n    logits = context @ weights\n    arms_probabilities = tempered_softmax(logits=logits, temperature=temperature)\n    return arms_probabilities, model\n\ndef expand_context_to_arms(context, n_arms):\n    context = np.array([context for _ in range(n_arms)])\n    context = np.swapaxes(context, 0, 1)\n    return context\n```\n:::\n\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=4}\n``` {.python .cell-code}\nCONTEXT, GROUPS = generate_context(\n    n_arms=N_ARMS, \n    n_users=N_USERS, \n    context_size=CONTEXT_SIZE, \n    redundant=REDUNDANT, \n    repeated=REPEATED, \n    informative=INFORMATIVE, \n    clusters_per_class=1, \n    class_sep=CLASS_SEP, \n    with_intercept=True\n)\nARMS_PROBAILITIES, MODE = compute_arms_probabilities(\n    context=CONTEXT,\n    groups=GROUPS,\n    with_intercept=True,\n)\n\nCONTEXT = expand_context_to_arms(\n    context=CONTEXT,\n    n_arms=N_ARMS,\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCUDA backend failed to initialize: Unable to use CUDA because of the following issues with CUDA components:\nOutdated cuDNN installation found.\nVersion JAX was built against: 8907\nMinimum supported: 9100\nInstalled version: 8907\nThe local installation version must be no lower than 9100..(Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n```\n:::\n:::\n\n\nHere we will use TSNE for projecting the multidimensional context space on a 2D plane, this should allow us to get a better intuition of what is going on. What we expect to see are separate spheres or regions (this depends on how much noise we encode in our context space) with different colouring depening on which arms they are associated with\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=5}\n``` {.python .cell-code}\nembedding = TSNE().fit_transform(CONTEXT[:, 0, :])\n\nfig, axs = plt.subplots(UNIT_ARMS, UNIT_ARMS, figsize=(8, 8), sharex=True, sharey=True)\naxs = axs.flatten()\nfor arm in range(ARMS_PROBAILITIES.shape[1]):\n\n    axs[arm].scatter(\n        embedding[:, 0],\n        embedding[:, 1],\n        c=ARMS_PROBAILITIES[:, arm],\n        s=1,\n        cmap=\"RdBu_r\"\n    )\n    axs[arm].set_title(f\"Arm {arm}\")\n\nfig.supylabel(\"First Context Dimension\")\nfig.supxlabel(\"Second Context Dimension\")\nfig.suptitle(\"Arm Reward Probability \\n Visualized in Context Space\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lin_ucb_jax_files/figure-html/cell-6-output-1.png){width=757 height=764}\n:::\n:::\n\n\n## LinUCB Algorithm for a Multi-Armed Bandit Problem\n\nTo solve a multi-armed bandit problem with context we can leverage an algorithm called Linear Upper Confidence Bound (i.e. LinUCB).\n\nThe aim of the algorithm is to progressively obtain a reliable estimate of all the options provided by the multi-armed bandit and to do so efficiently (i.e. with the smallest number of interactions.)\n\nFor doing so LinUCB simply fit a multinomial regression to the context (i.e. the covariates) provided by each arm of the bandit in order to estimate the return value associated with each arm, more formally\n\n$$\n\\mathbb{E}[r_{t,k} | x_k] = x^\\intercal \\theta^*_k\n$$\n\nwhere $\\theta^*_k$ are the set of parameters associated with a given arm $k$ for which the algorithm is trying to find the optimal estimate.\n\n# Implementing the Algorithm\n\n## Parameters Initialisation\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=6}\n``` {.python .cell-code}\ndef init_matrices(context_size: int) -> Tuple[ArrayLike, ...]:\n    A = jnp.eye(N=context_size)\n    b = jnp.zeros(shape=(context_size, 1))\n    return A, b\n\ndef init_matrices_for_all_arms(\n        number_of_arms: int, \n        context_size: int,\n    )  -> Tuple[ArrayLike, ...]:\n\n    arms_A = []\n    arms_b = []\n    for _ in range(number_of_arms):\n\n        A, b = init_matrices(context_size=context_size)\n        arms_A.append(A)\n        arms_b.append(b)\n\n    arms_A = jnp.array(arms_A)\n    arms_b = jnp.array(arms_b)\n\n    return arms_A, arms_b\n```\n:::\n\n\n## Computations\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=7}\n``` {.python .cell-code}\n@jit\ndef compute_theta(\n        A_inverse: ArrayLike, \n        b: ArrayLike,\n    ) -> ArrayLike:\n\n    theta = jnp.dot(A_inverse, b)\n    return theta\n\n@jit\ndef compute_sigma(\n        context: ArrayLike, \n        A_inverse: ArrayLike,\n    ) -> ArrayLike:\n\n    sigma = jnp.sqrt(\n        jnp.dot(\n            jnp.transpose(context), \n            jnp.dot(A_inverse, context)\n        )\n    )\n    return sigma\n\n@jit\ndef compute_mu(\n        theta: ArrayLike, \n        context: ArrayLike,\n    )-> ArrayLike:\n\n    mu = jnp.dot(\n        jnp.transpose(theta),\n        context\n    )\n    return mu\n\n@jit\ndef compute_upper_bound(\n        A: ArrayLike, \n        b: ArrayLike, \n        context: ArrayLike, \n        alpha: float,\n    ) -> ArrayLike:\n    A_inverse = inv(A)\n    context_column = jnp.reshape(a=context, newshape=(-1, 1))\n\n    theta = compute_theta(\n        A_inverse=A_inverse, \n        b=b\n    )\n    sigma = compute_sigma(\n        context=context_column, \n        A_inverse=A_inverse\n    )\n    mu = compute_mu(\n        theta=theta, \n        context=context_column\n    )\n    upper_bound = mu + (sigma * alpha)\n    return upper_bound\n\n@jit\ndef execute_linucb(\n        alpha: float, \n        arms_A: ArrayLike, \n        arms_b:ArrayLike, \n        context: ArrayLike, \n        noise: ArrayLike,\n    ) -> ArrayLike:\n    partialized_compute_upper_bound = partial(\n        compute_upper_bound,\n        alpha=alpha\n    )\n    upper_bound = vmap(\n        fun=vmap( \n            fun=partialized_compute_upper_bound, \n            in_axes=(0, 0, 0)\n        ),\n        in_axes=(None, None, 0)\n    )(arms_A, arms_b, context).squeeze()\n    upper_bound += noise\n    return upper_bound\n```\n:::\n\n\n## Update parameters\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=8}\n``` {.python .cell-code}\n@jit\ndef update_parameters(\n    arms_A: ArrayLike, \n    arms_b: ArrayLike, \n    arms_context: ArrayLike, \n    policy: ArrayLike, \n    reward: ArrayLike\n) -> Tuple[ArrayLike, ArrayLike]:\n    new_A=arms_A[policy, :, :] \n    new_b=arms_b[policy, :, :]\n    context=arms_context[policy, :]\n\n    context_column = jnp.reshape(a=context, newshape=(-1, 1))\n    new_A += jnp.dot(context_column, jnp.transpose(context_column))\n    new_b += reward*context_column\n\n    arms_A = arms_A.at[policy, :, :].set(new_A)\n    arms_b = arms_b.at[policy, :, :].set(new_b)\n    return arms_A, arms_b\n```\n:::\n\n\n# Definining the simulation steps\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=9}\n``` {.python .cell-code}\n@jit\ndef step(\n        policy: ArrayLike, \n        arms_probabilities: ArrayLike, \n        step_key: ArrayLike, \n        reward_sparsity: ArrayLike,\n        reward_weighting: float,\n    ) -> ArrayLike:\n    sparsity_key, reward_key = random.split(step_key)\n    rows = jnp.arange(start=arms_probabilities.shape[0])\n\n    sparsity_factor = random.bernoulli(\n        key=sparsity_key,\n        p=1 - reward_sparsity,\n        shape=(arms_probabilities.shape[0],),\n    ) * 1\n    rewards = random.bernoulli(\n        key=reward_key,\n        p=arms_probabilities[rows, policy]\n    ) * 1\n    return (rewards * sparsity_factor) * reward_weighting\n\n@jit\ndef execute_policies(\n        upper_bound: ArrayLike, \n        random_arm_key: ArrayLike,\n    ) -> Tuple[ArrayLike, ArrayLike]:\n    random_policy  = random.choice(\n        key=random_arm_key, \n        a=jnp.arange(upper_bound.shape[1]),\n        shape=(upper_bound.shape[0],)\n    )\n    linucb_policy = upper_bound.argmax(axis=1)\n    return linucb_policy, random_policy\n\n@jit\ndef compute_rewards(\n        linucb_policy: ArrayLike, \n        random_policy: ArrayLike, \n        arms_probabilities: ArrayLike, \n        reward_sparsity: ArrayLike, \n        reward_weighting: float,\n        step_key\n    ) -> ArrayLike:\n    arms_rewards = []\n    for policy in [linucb_policy, random_policy]:\n\n        rewards = step(\n            policy=policy, \n            arms_probabilities=arms_probabilities, \n            reward_sparsity=reward_sparsity,\n            reward_weighting=reward_weighting,\n            step_key=step_key\n        )\n        arms_rewards.append(rewards)\n\n    return arms_rewards\n\n@jit\ndef compute_regrets(\n        linucb_policy: ArrayLike, \n        random_policy: ArrayLike, \n        arms_probabilities: ArrayLike\n    ) -> List[ArrayLike]:\n    policies_regrets = []\n    optimality = arms_probabilities.max(1)\n    rows = jnp.arange(start=arms_probabilities.shape[0])\n    for policy in [linucb_policy, random_policy]:\n\n        regret = optimality - arms_probabilities[rows, policy]\n        policies_regrets.append(regret)\n    \n    return policies_regrets\n\n@jit\ndef simulate_interaction(\n        carry: Tuple[ArrayLike, ...], \n        x: Any, \n        arms_probabilities: ArrayLike, \n        alpha: float, \n        reward_weighting: float,\n        reward_sparsity: float\n    ) -> Any:\n    split_key, arms_A, arms_b, context = carry\n    noise_key, random_arm_key, step_key, split_key = random.split(split_key, 4)  \n    noise = random.normal(\n        key=noise_key, \n        shape=(context.shape[0], arms_A.shape[0])\n    ) * 1e-5\n\n    upper_bound = execute_linucb(\n        alpha=alpha, \n        arms_A=arms_A, \n        arms_b=arms_b, \n        context=context, \n        noise=noise,\n    )\n    linucb_policy, random_policy= execute_policies(\n        upper_bound=upper_bound, \n        random_arm_key=random_arm_key\n    )\n    linucb_rewards, random_rewards = compute_rewards(\n        linucb_policy=linucb_policy, \n        random_policy=random_policy, \n        arms_probabilities=arms_probabilities, \n        reward_sparsity=reward_sparsity,\n        reward_weighting=reward_weighting,\n        step_key=step_key\n    )\n    linucb_regrets, random_regrets = compute_regrets(\n        linucb_policy=linucb_policy, \n        random_policy=random_policy, \n        arms_probabilities=arms_probabilities\n    )\n\n    arms_A, arms_b = vmap(\n        fun=update_parameters, \n        in_axes=(None, None, 0, 0, 0)\n    )(arms_A, arms_b, context, linucb_policy, linucb_rewards)\n    arms_A = arms_A.mean(0)\n    arms_b = arms_b.mean(0)\n\n    new_carry = (split_key, arms_A, arms_b, context)\n    diagnostics = {\n        \"parameters\": {\"A\": arms_A, \"b\": arms_b},\n        \"policies\": {\n            \"linucb\": {\"rewards\": linucb_rewards, \"regrets\": linucb_regrets}, \n            \"random\": {\"rewards\": random_rewards, \"regrets\": random_regrets}\n        }\n\n    }\n\n    return new_carry, diagnostics\n```\n:::\n\n\n# Running the simulation\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=10}\n``` {.python .cell-code}\nSIMULATION_STEPS = (24 * 4) * 7\n\nsplit_key = random.PRNGKey(666)\narms_A, arms_b = init_matrices_for_all_arms(\n    number_of_arms=N_ARMS, \n    context_size=CONTEXT.shape[-1]\n)\n\npartialized_simulate_interaction = partial(\n    simulate_interaction, \n    arms_probabilities=ARMS_PROBAILITIES, \n    alpha=1,\n    reward_sparsity=REWARD_SPARSITY,\n    reward_weighting=REWARD_WEIGHTING,\n)\ncarry, diagnostics = scan(\n    partialized_simulate_interaction, \n    init=(split_key, arms_A, arms_b, CONTEXT),\n    xs=jnp.arange(SIMULATION_STEPS)\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_13144/1682972371.py:44: DeprecationWarning: The newshape argument of jax.numpy.reshape is deprecated and setting it will soon raise an error. To avoid an error in the future, and to suppress this warning, please use the shape argument instead.\n  context_column = jnp.reshape(a=context, newshape=(-1, 1))\n/tmp/ipykernel_13144/688672908.py:13: DeprecationWarning: The newshape argument of jax.numpy.reshape is deprecated and setting it will soon raise an error. To avoid an error in the future, and to suppress this warning, please use the shape argument instead.\n  context_column = jnp.reshape(a=context, newshape=(-1, 1))\n```\n:::\n:::\n\n\n# Performance Visualisation\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=11}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\ndef compute_policy_diagnostics_summaries(diagnostics):\n    diagnostics_summaries = {}\n    for policy, diagnositcs_dict in diagnostics.items():\n\n        diagnostics_summaries[policy] = {}\n\n        for diagnostic, value in diagnositcs_dict.items():\n\n            cumsum_value = value.cumsum(0)\n\n            diagnostics_summaries[policy][diagnostic] = {\n                \"mean\": value.mean(1), \n                \"upper_percentile\": np.percentile(value, 2.5, axis=1), \n                \"lower_percentile\": np.percentile(value, 97.5, axis=1), \n                \"cumsum_mean\": cumsum_value.mean(1), \n                \"cumsum_upper_percentile\": np.percentile(cumsum_value, 2.5, axis=1),\n                \"cumsum_lower_percentile\": np.percentile(cumsum_value, 97.5, axis=1)\n            }\n    return diagnostics_summaries\n \ndef plot_policy_diagnositc(mean, lower_percentile, upper_percentile, ax, label):\n    ax.plot(\n        mean,\n        label=label\n    )\n    ax.fill_between(\n        x=np.arange(mean.shape[0]),\n        y1=lower_percentile,\n        y2=upper_percentile,\n        alpha=0.25\n    )\n    return ax\n\ndef plot_all_policy_diagnostics(diagnostics_summaries, figsize=(10, 20)):\n    fig, axs = plt.subplots(\n        nrows=2, \n        ncols=2, \n        figsize=figsize, \n        sharex=True\n    )\n    axs = axs.flatten()\n\n    for policy in list(diagnostics_summaries.keys()):\n\n        ax = plot_policy_diagnositc(\n            mean=diagnostics_summaries[policy][\"rewards\"][\"mean\"], \n            lower_percentile=diagnostics_summaries[policy][\"rewards\"][\"upper_percentile\"], \n            upper_percentile=diagnostics_summaries[policy][\"rewards\"][\"lower_percentile\"], \n            ax=axs[0], \n            label=policy\n        )\n        ax.set_ylabel(\"Reward\")\n        ax = plot_policy_diagnositc(\n            mean=diagnostics_summaries[policy][\"rewards\"][\"cumsum_mean\"], \n            lower_percentile=diagnostics_summaries[policy][\"rewards\"][\"cumsum_upper_percentile\"], \n            upper_percentile=diagnostics_summaries[policy][\"rewards\"][\"cumsum_lower_percentile\"], \n            ax=axs[1], \n            label=policy\n        )\n        ax.set_ylabel(\"Cumulative Reward\")\n\n        ax = plot_policy_diagnositc(\n            mean=diagnostics_summaries[policy][\"regrets\"][\"mean\"], \n            lower_percentile=diagnostics_summaries[policy][\"regrets\"][\"upper_percentile\"], \n            upper_percentile=diagnostics_summaries[policy][\"regrets\"][\"lower_percentile\"], \n            ax=axs[2], \n            label=policy\n        )\n        ax.set_ylabel(\"Regret\")\n        ax.set_xlabel(\"Simulation Step\")\n        \n        ax = plot_policy_diagnositc(\n            mean=diagnostics_summaries[policy][\"regrets\"][\"cumsum_mean\"], \n            lower_percentile=diagnostics_summaries[policy][\"regrets\"][\"cumsum_upper_percentile\"], \n            upper_percentile=diagnostics_summaries[policy][\"regrets\"][\"cumsum_lower_percentile\"], \n            ax=axs[3], \n            label=policy\n        )\n        ax.set_ylabel(\"Cumulative Regret\")\n        ax.set_xlabel(\"Simulation Step\")\n\n    for ax in axs:\n\n        ax.grid()\n        ax.legend()\n    \n    return fig\n```\n:::\n\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=12}\n``` {.python .cell-code}\ndiagnostics_summaries = compute_policy_diagnostics_summaries(\n    diagnostics=diagnostics[\"policies\"]\n)\nfig = plot_all_policy_diagnostics(\n    diagnostics_summaries=diagnostics_summaries, \n    figsize=(10, 5)\n)\nplt.suptitle(\"Performance of LinUCB and Random Policies\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lin_ucb_jax_files/figure-html/cell-13-output-1.png){width=949 height=473}\n:::\n:::\n\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=13}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\ndef plot_single_arm_parameters_dianoistics(arm, diagnostics, figsize, show_labels=True):\n    fig = plt.figure(figsize=figsize, tight_layout=True)\n    grid = GridSpec(nrows=2, ncols=3)\n\n    beta = diagnostics[\"b\"][:, arm, :].squeeze()\n    midpoint = int(beta.shape[0] // 2)\n    simulation_steps = np.arange(beta.shape[0])\n\n    ax_beta = fig.add_subplot(grid[0, :])\n    for parameter_index in np.arange(beta.shape[-1]):\n\n        ax_beta.plot(\n            beta[:, parameter_index],\n            label=f\"Parameter {parameter_index}\"            \n        )\n    \n    if show_labels:\n        ax_beta.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n    ax_beta.set_ylabel(\"Parameter Value\")\n    ax_beta.set_xlabel(\"Simulation Step\")\n    ax_beta.set_title(\"Evolution of b parameters\")\n    ax_beta.grid()\n\n    for column, simulation_step in enumerate(simulation_steps[[0, midpoint, -1]]): \n\n        ax_alpha = fig.add_subplot(grid[1, column])\n        ax_alpha.set_ylabel(\"b Parameters\")\n        ax_alpha.set_xlabel(\"b Parameters\")\n        ax_alpha.set_title(f\"Covariance b parameters \\n Simulation Step {simulation_step}\")\n        \n        \n        alpha_img = ax_alpha.imshow(\n            diagnostics[\"A\"][simulation_step, arm, :, :]\n        )\n        plt.colorbar(\n            alpha_img,\n            ax=ax_alpha,\n            fraction=0.046, \n            pad=0.04\n        )\n    \n    plt.suptitle(f\"Diagnostics for arm {arm}\")\n    \n    return fig\n```\n:::\n\n\n::: {.cell vscode='{\"languageId\":\"python\"}' execution_count=14}\n``` {.python .cell-code}\nfor arm in range(N_ARMS):\n\n    fig = plot_single_arm_parameters_dianoistics(\n        arm=arm, \n        diagnostics=diagnostics[\"parameters\"], \n        figsize=(9, 6)\n    )\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lin_ucb_jax_files/figure-html/cell-15-output-1.png){width=856 height=550}\n:::\n\n::: {.cell-output .cell-output-display}\n![](lin_ucb_jax_files/figure-html/cell-15-output-2.png){width=856 height=550}\n:::\n\n::: {.cell-output .cell-output-display}\n![](lin_ucb_jax_files/figure-html/cell-15-output-3.png){width=856 height=550}\n:::\n\n::: {.cell-output .cell-output-display}\n![](lin_ucb_jax_files/figure-html/cell-15-output-4.png){width=867 height=552}\n:::\n:::\n\n\n",
    "supporting": [
      "lin_ucb_jax_files"
    ],
    "filters": [],
    "includes": {}
  }
}