{
  "hash": "4ef2b5056a99710f6bd89f817f5276af",
  "result": {
    "markdown": "---\ntitle: 2 - Model specification and fitting\ndescription: This post introduces the general set-up that we will use in this tutorial for specifying models and fitting them to the data.\nauthor: Valerio Bonometti\ndate: '2023-08-24'\ncategories:\n  - JAX Tutorial\n  - model fitting\n  - model building\n---\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom jax.debug import print as jprint\n```\n:::\n\n\nIn order to specify models in JAX we first need to figure out what are the core functionalities that we need to implement. We will focus on specific set of models that given an input $X$, a target $y$ and parameters $\\theta$ aim to approximate functions of the form $f(X; \\theta) \\mapsto y$.\n\nWhat we need to specify are:\n\n1. Parameters-related functionalities:\n    - Storage, how to best keep records of our parameters.\n    - Initialisation, how to set our parameters to good starting points.\n    - Sharing, how to make the parameters available to the model.\n\n2. Model-related functionalities:\n    - Forward computations, how to move from an input to an estimate of the target.\n    - Objective computations, how to define suitable loss function along with any regularizing penalties.\n    - Backward computations, how to derive the gradient of the parameters with respect to the model's objective.\n\n3. Optimization-related functionalities:\n    - Optimization routines, how to find the optimal values for the parameters using suitable algorithms.\n    - Parameters update, how to use the information derived from the backward computations for updating the parameters.\n    - Fitting routines, how to connect the input, model and the optimization routines.\n\nWe also need to make sure that while developing these functionalities we leverage the optimisations provided by JAX while avoiding its sharp edges.\n\n# Parameters-related Functionalities\n\n## Parameters Container\n\nThe ideal way for storing parameters would be to create an immutable data structure (e.g., the named tuple example presented in our first post), registered as a pytree node, every time we need to update our parameters.\n\nIn this and future posts we will adopt a much simpler although more intuitive strategy and store our parameters in a dictionary.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom jax import numpy as jnp\n\nmy_parameters = {}\n```\n:::\n\n\nThe nice thing about dictionaries is that they are:\n\n1. Natively supported as a pytree.\n2. Easy to inspect.\n3. Naturally support nested structures.\n4. Easy to update using native JAX functionalities\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nmy_parameters[\"alpha\"] = 1.\n\n# more complicated structure\nmy_parameters[\"beta_1\"] = {\n    0: jnp.arange(5),\n    1: jnp.arange(5, 10),\n    2: jnp.arange(10, 15)\n}\nmy_parameters[\"beta_2\"] = {\n    0: {\n        \"beta_21\": jnp.arange(5),\n        \"beta_22\": jnp.arange(5, 10),\n        \"beta_23\": jnp.arange(10, 15)\n    },\n    1: 4.,\n    2: 5.\n}\n```\n:::\n\n\n## Parameters Initialisation\n\nOne of the first step when fitting a model to the data is to set the starting point for the optimization process for each of the considered parameters.\n\nWe can achieve this by defining functions that implement specific initialisation strategies\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef ones(init_state, random_key):\n    \"\"\"Initialise parameters with a vector of ones.\n\n    Args:\n        init_state (Tuple): state required to\n        initialise the parameters. For this initializer only the parameters shape is required\n\n        random_key (PRNG Key): random state used for generate the random numbers. Not used for this type of initialisation.\n\n    Returns:\n        param(DeviceArray): generated parameters\n    \"\"\"\n    params_shape, _ = init_state\n    params = jnp.ones(shape=params_shape)\n    return params\n```\n:::\n\n\none of the most straightforward strategies is to initialize all the parameters with the same constant value (a one in this case).\n\nIn this case we our function requires an `init_state` tuple containing all the information necessaries for initializing the parameters and a `random_key` used for setting the state of the random number generator. In this case we really do not need any random behaviour but we keep the signature for keeping compatibility with other initialisation strategies.\n\nAnother alternative is to generate starting values according to some statistical distribution, like a gaussian for instance\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom jax.random import PRNGKey\nfrom jax import random\n\nimport seaborn as sns\n\ndef random_gaussian(init_state, random_key, sigma=0.1):\n    \"\"\"Initialize parameters with a vector of random numbers drawn from a normal distribution with mean 0 and std sigma.\n\n    Args:\n        init_state (Tuple): state required to\n        initialize the parameters. For this initializer only the parameters shape is required\n\n        random_key (PRNG Key): random state used for generate the random numbers.\n\n    Returns:\n        param(DeviceArray): generated parameters\n    \"\"\"\n    params_shape, _ = init_state\n    params = random.normal(\n        key=random_key,\n        shape=params_shape\n    ) * sigma\n    return params\n\nmaster_key = PRNGKey(666)\n\nmy_parameters = random_gaussian(\n    init_state=((100, 2), None),\n    random_key=master_key,\n    sigma=0.1\n    )\n\ngrid = sns.jointplot(\n    x=my_parameters[:, 0],\n    y=my_parameters[:, 1],\n    kind=\"kde\",\n    height=4\n)\ngrid.ax_joint.set_ylabel(\"Parameter 2\")\ngrid.ax_joint.set_xlabel(\"Parameter 1\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=404 height=392}\n:::\n:::\n\n\n## Parameters Sharing\n\nSharing the parameters at this point is better understood as part of a **state manipulation** process. What do we mean by this? If we were to perform parameters update within a an object oriented framework we might do something among these lines\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nclass Model:\n    def __init__(self):\n        self._parameters = np.array([0, 0, 0])\n\n    def add(self, x):\n        self._parameters += x\n\n    def subtract(self, x):\n        self._parameters -= x\n\n    def get_parameters(self):\n        return self._parameters\n\nmodel = Model()\nmodel.add(10)\nmodel.subtract(5)\n\nprint(f\"Updated Parameters {model.get_parameters()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUpdated Parameters [5 5 5]\n```\n:::\n:::\n\n\nthe parameters are part of the state of `Model` an get updated according to the behavior of `add` and `subtract`.\n\nSince in JAX we have to stick to pure functions as much as we can, a viable option is to consider `parameters` as a state that is passed through a chain of transformation\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom jax import jit\n\ndef parameters_init():\n    return jnp.array([0., 0., 0.])\n\n@jit\ndef add(parameters, x):\n    return parameters + x\n\n@jit\ndef subtract(parameters, x):\n    return parameters - x\n\nparameters = parameters_init()\n# parameters are passed to transformations\n# and returned modified\nparameters = add(parameters=parameters, x=10.)\nparameters = subtract(parameters=parameters, x=5.)\n\nprint(f\"Updated Parameters {parameters}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUpdated Parameters [5. 5. 5.]\n```\n:::\n:::\n\n\ndifferently from the previous example, here the state (i.e., `parameters`) is made explicit and passed as argument to the functions in charge of doing the transformations.\n\n# Model-related Functionalities\n\nNow that we have outlined how to generate, store and share parameters we must focus on the scaffolding describing the transformations performed by our model.\n\nFor the sake of simplicity we will use [closures](https://realpython.com/inner-functions-what-are-they-good-for/) for being compliant with the functional requirements of JAX. We are aware that this might not be the optimal solution but it works just fine for the didactic purpose of this post. So let's see how we would structure our closure\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom jax import jit\n\ndef model(X, prng):\n\n    @jit\n    def init_params(X):\n        pass\n\n    @jit\n    def forward(X, current_state):\n        pass\n\n    @jit\n    def backward(X, y, current_state):\n\n        @jit\n        def compute_loss(y, y_hat):\n            pass\n\n        pass\n\n    return init_params, forward, backward\n```\n:::\n\n\nIn this case our `model` function would take a set of parameters (these are supposed to be constant and used by all the downstream functions) and return a collection functions in charge of performing parameters initialisation, forward and backward computations.\n\nLet's now go in more details of these specific functions.\n\n## Forward Computations\n\nThe `forward` function is in charge of performing all the the \"forward computations\" in the model. This naming, that we borrow from the various deep learning framework around, might not be the optimal one as we won't necessarely need or have a complementary \"backward\" function but it is functional to the needs of this tutorial. \n\nLet's look at how the forward functiona for a simple linear model might look like\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n@jit\ndef forward(X, current_state):\n    \"\"\"Perform the forward computations for a linear model\n\n    Args:\n        x: covaruates of the model.\n        current_state: current state of the model, containing \n        parameters and state for the pseudo random number generator.\n\n    Returns:\n        yhat: estimate of the target variable.\n    \"\"\"\n    current_params, random_state = current_state\n    beta = current_params[\"beta\"]\n    alphas = current_params[\"alphas\"]\n    yhat =  beta + jnp.dot(X, alphas)\n    return yhat\n```\n:::\n\n\nHere, `forward` receive as input some data `x` and a variable we call `current_state`, as we see this last one it then gets unpacked in some `current_parameters` and a `random_state`. The idea here is that we are passing around the current state of the model (parameters or random number generator seed) instead of accessing it as we would with the attributes of a class.\n\nOnce unpacked all the necessary variables (here random state is spurious, but we could use it in case we need some stochastic behaviour from our function), the `forward` function simply execute all the logic necessary for mapping the input `x` to an estimate of our target variable `yhat`.\n\nOnce we obtain an estimate of our target variable, we should evaluate it with rispect to a given objective function, let's see how we can do that with the `compute_loss` function.\n\n## Objective Computations\n\nThe computation of the objective function will require in our case four components: the parameters, the forward function, the loss function (along with any function used for computing any regularizing term) ad the input to the model.\n\nWe'll see that we will pass all the components except the paramenters to the objective function using a closure and that the actual loss function need to be wrapped inside a `compute_loss` function.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrom jax import tree_leaves\n\nreg_strength = 0.001\n\nX = np.random.normal(size=(100, 10))\ny = np.random.normal(size=(100,))\n\nparams = {\n    \"alphas\": np.random.normal(size=(10, )),\n    \"beta\": np.random.normal(),\n}\n\n@jit\ndef root_mean_squared_error(y, yhat):\n    \"\"\"Compute the the root mean squared error between two vectors.\n\n    Args:\n        - y: ground truth value\n        - yhat: estimate of ground truth\n    \n    Returns:\n        squared_error: the squared mean error between y and y hat\n    \"\"\"\n    squared_error = jnp.square(y - yhat)\n    mean_squared_error = jnp.mean(squared_error)\n    return jnp.sqrt(mean_squared_error)\n\n@jit\ndef l1_reg_loss(params):\n    \"\"\"Compute the l1 norm of the parameters.\n\n    Args:\n        - y: ground truth value\n        - yhat: estimate of ground truth\n    \n    Returns:\n        squared_error: the squared mean error between y and y hat\n    \"\"\"\n    loss = sum([jnp.sum(jnp.abs(leave)) for leave in tree_leaves(params)])\n    return loss\n\n\ndef model(X, y, reg_strength):\n\n    @jit\n    def compute_loss(current_params):\n        \"\"\"Perform the computations required for deriving the loss given the current parameters.\n\n        This inlcudes a call to the forward function, the loss computation and the regularisation\n        loss.\n\n        Args:\n            current_params: the current state of the parameters of the model\n        \"\"\"\n        yhat = forward(X=X, current_state=(current_params, None))\n        raw_loss = root_mean_squared_error(y=y, yhat=yhat)\n        reg_loss = l1_reg_loss(params=current_params) * reg_strength\n        return raw_loss + reg_loss\n    \n    return compute_loss\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_21206/2913352642.py:1: DeprecationWarning: jax.tree_leaves is deprecated: use jax.tree_util.tree_leaves.\n  from jax import tree_leaves\n```\n:::\n:::\n\n\nOf course there are are more intuitive ways for doing this but in this case we require this convoluted game of chinese boxes because when we apply the `grad` transformation we want it to give us the derivative of the objective function (`root_mean_squared_error` plus `l1_reg_loss`) with respect to the `current_parameters`. So in this case in order to obtain the derivative we can simple apply the relevant transformation `value_and_grad` to the `compute_loss` function\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfrom jax import value_and_grad\n\ncompute_loss = model(X=X, y=y, reg_strength=reg_strength)\ncompute_loss_derivative = value_and_grad(compute_loss)\n\ncompute_loss_derivative(params)\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\n(Array(2.1123009, dtype=float32),\n {'alphas': Array([-0.4701293 , -0.29670703,  0.43866345, -0.18365346,  0.36222848,\n         -0.07263832, -0.1072477 ,  0.12307326,  0.08663466,  0.2632417 ],      dtype=float32),\n  'beta': Array(0.325346, dtype=float32, weak_type=True)})\n```\n:::\n:::\n\n\nWhat is particularly convinient in the functions defined above is the `tree_leaves` we use we computing the regularisation term. This utility function provided by jax allows us to traverse an antire PyTree in order to get all its leaves values, let's see an example in action\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ncomplex_nested_pytree = {\n    \"a\": (1, 2, 3),\n    \"b\": 4,\n    \"c\": {\n        \"c_1\": 5,\n        \"c_2\": 6,\n        \"c_3\": {\n            \"c_3_1\": 7,\n            \"c_3_2\": 8,\n            \"c_3_3\": 9\n        }\n    }\n}\n\ntree_leaves(complex_nested_pytree)\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n```\n:::\n:::\n\n\nAs you can see, this can become a convenient way for flattenting even the most complicated tree structure (such as those we have when specifying the parameters of some complex model).\n\n## Backward Computations\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}