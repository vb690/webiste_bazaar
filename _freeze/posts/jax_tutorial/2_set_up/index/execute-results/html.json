{
  "hash": "09f9f30e9a52c63e569650f764725a4f",
  "result": {
    "markdown": "---\ntitle: 2 - Model specification and fitting\ndescription: This post introduces the general set-up that we will use in this tutorial for specifying models and fitting them to the data.\nauthor: Valerio Bonometti\ndate: '2023-02-05'\ncategories:\n  - JAX Tutorial\n  - model fitting\n  - model building\n---\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom jax.debug import print as jprint\n```\n:::\n\n\nIn order to specify models in JAX we first need to figure out what are the core functionalities that we need to implement. We will focus on specific set of models that given an input $X$, a target $y$ and parameters $\\theta$ aim to approximate functions of the form $f(X; \\theta) \\mapsto y$.\n\nWhat we need to specify are:\n\n1. Parameters-related functionalities:\n    - Storage, how to best keep records of our parameters.\n    - Initialisation, how to set our parameters to good starting points.\n    - Sharing, how to make the parameters available to the model.\n\n2. Model-related functionalities:\n    - Forward computations, how to move from some input to an estimate of the target.\n    - Objective computations, how to define suitable loss function along with any regularizing penalties.\n    - Backward computations, how to derive the gradient of the parameters with respect to the model's objective.\n\n3. Optimization-related functionalities:\n    - Optimization routines, how to find the optimal values for the parameters using suitable algorithms.\n    - Parameters update, how to use the information derived from the backward computations for updating the parameters.\n    - Fitting routines, how to connect the input, model and the optimization routines.\n\nWe also need to make sure that while developing these functionalities we leverage the optimisations provided by JAX while avoiding its sharp edges.\n\n# Parameters Functionalities\n\n## Parameters Container\n\nThe ideal way for storing parameters would be to create an immutable data structure (e.g., the named tuple example presented in our first post), registered as a pytree node, every time we need to update our parameters. \n\nIn this and future posts we will adopt a much simpler although more intuitive strategy and store our parameters in a dictionary.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom jax import numpy as jnp\n\nmy_parameters = {}\n```\n:::\n\n\nThe nice thing about dictionaries is that they are:\n\n1. Natively supported as a pytree.\n2. Easy to inspect.\n3. Naturally support nested structures.\n4. Easy to update using native JAX functionalities\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nmy_parameters[\"alpha\"] = 1.\n\n# more complicated structure\nmy_parameters[\"beta_1\"] = {\n    0: jnp.arange(5),\n    1: jnp.arange(5, 10),\n    2: jnp.arange(10, 15)\n}\nmy_parameters[\"beta_2\"] = {\n    0: {\n        \"beta_21\": jnp.arange(5),\n        \"beta_22\": jnp.arange(5, 10),\n        \"beta_23\": jnp.arange(10, 15)\n    },\n    1: 4.,\n    2: 5.\n}\n```\n:::\n\n\n## Parameters Initialisation\n\nOne of the first step when fitting a model to the data is to set the starting point for the optimization process for each of the considered parameters.\n\nWe can achieve this by defining functions that implement specific initialisation strategies\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef ones(init_state, random_key):\n    \"\"\"Initialise parameters with a vector of ones.\n\n    Args:\n        init_state (Tuple): state required to\n        initialise the parameters. For this initializer only the parameters shape is required\n        \n        random_key (PRNG Key): random state used for generate the random numbers. Not used for this type of initialisation.\n\n    Returns:\n        param(DeviceArray): generated parameters\n    \"\"\"\n    params_shape, _ = init_state\n    params = jnp.ones(shape=params_shape)\n    return params\n```\n:::\n\n\none of the most straightforward strategies is to initialize all the parameters with the same constant value (a one in this case). \n\nIn this case we our function requires an `init_state` tuple containing all the information necessaries for initializing the parameters and a `random_key` used for setting the state of the random number generator. In this case we really do not need any random behaviour but we keep the signature for keeping compatibility with other initialisation strategies.\n\nAnother alternative is to generate starting values according to some statistical distribution, like a gaussian for instance\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom jax.random import PRNGKey\nfrom jax import random\n\nimport seaborn as sns\n\ndef random_gaussian(init_state, random_key, sigma=0.1):\n    \"\"\"Initialize parameters with a vector of random numbers drawn from a normal distribution with mean 0 and std sigma.\n\n    Args:\n        init_state (Tuple): state required to\n        initialize the parameters. For this initializer only the parameters shape is required\n        \n        random_key (PRNG Key): random state used for generate the random numbers.\n\n    Returns:\n        param(DeviceArray): generated parameters\n    \"\"\"\n    params_shape, _ = init_state\n    params = random.normal(\n        key=random_key, \n        shape=params_shape\n    ) * sigma\n    return params\n\nmaster_key = PRNGKey(666)\n\nmy_parameters = random_gaussian(\n    init_state=((100, 2), None), \n    random_key=master_key, \n    sigma=0.1\n    )\n\ngrid = sns.jointplot(\n    x=my_parameters[:, 0], \n    y=my_parameters[:, 1], \n    kind=\"kde\",\n    height=4\n)\ngrid.ax_joint.set_ylabel(\"Parameter 2\")\ngrid.ax_joint.set_xlabel(\"Parameter 1\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=404 height=392}\n:::\n:::\n\n\n## Parameters Sharing\n\nSharing the parameters at this point is better understood as part of a **state manipulation** process. What do we mean by this? If we were to perform parameters update within a an object oriented framework we might do something among these lines\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nclass Model:\n    def __init__(self):\n        self._parameters = np.array([0, 0, 0])\n\n    def add(self, x):\n        self._parameters += x\n    \n    def subtract(self, x):\n        self._parameters -= x\n\n    def get_parameters(self):\n        return self._parameters\n\n\nmodel = Model()\nmodel.add(10)\nmodel.subtract(5)\n\nprint(f\"Updated Parameters {model.get_parameters()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUpdated Parameters [5 5 5]\n```\n:::\n:::\n\n\nthe parameters are part of the state of `Model` an get updated according to the behavior of `add` and `subtract`. \n\nSince in JAX we have to stick to pure functions as much as we can, the obvious choice is to consider `parameters` as a state that is passed through a chain of transformation\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom jax import jit\n\ndef parameters_init():\n    return jnp.array([0., 0., 0.])\n\n@jit\ndef add(parameters, x):\n    return parameters + x\n\n@jit\ndef subtract(parameters, x):\n    return parameters - x\n\nparameters = parameters_init()\n# parameters are passed to transformations\n# and returned modified\nparameters = add(parameters=parameters, x=10.)\nparameters = subtract(parameters=parameters, x=5.)\n\nprint(f\"Updated Parameters {parameters}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUpdated Parameters [5. 5. 5.]\n```\n:::\n:::\n\n\n# Model Functionalities\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom jax import jit \n\ndef model(X, prng):\n\n    @jit\n    def init_params(X):\n        pass\n    \n    @jit\n    def forward(X, current_params, random_state):\n        pass\n\n    @jit\n    def backward(X, y, current_params, random_state):\n        \n        @jit\n        def compute_loss(params):\n            pass\n        \n        pass\n\n    return init_params, forward, backward\n```\n:::\n\n\n# Forward Computations\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n@jit\ndef forward(X, current_params, random_state):\n    pass\n```\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}