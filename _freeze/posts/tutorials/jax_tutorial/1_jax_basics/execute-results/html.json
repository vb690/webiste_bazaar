{
  "hash": "4a542db50132d0d15ddc6b8b55563d27",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"1 - The Basic Building Blocks\" \ndescription: \"This post introduces some of JAX basic building blocks. It is not an exhaustive collection but will include most components used in this series of blog posts.\"\nauthor: \"Valerio Bonometti\"\ndate: \"2023-08-24\"\ncategories: [JAX, Tutorial, basics]\njupyter: python3\n---\n\n::: {#30f0414e .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\n%load_ext watermark\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef visualize_univariate_time_series(\n    time_series, ax=None, figsize=(8, 4), **plot_kwargs\n):\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.plot(\n        np.arange(len(time_series)), \n        time_series, \n        **plot_kwargs\n    )\n\n    ax.tick_params(direction=\"in\", top=True, axis=\"x\", rotation=45)\n    ax.grid(\n        visible=True, \n        which=\"major\", \n        axis=\"x\", \n        color=\"k\", \n        alpha=0.25, \n        linestyle=\"--\"\n    )\n    return ax\n```\n:::\n\n\n# JAX functions unraveled\n\nIn order to leverage the speedup granted by the XLA compiler JAX first needs to transform python code in a set of lower-lever and strictier set of instructions: a **jax expression**. Let' s see how this expression would look like\n\n::: {#cf716c2d .cell execution_count=2}\n``` {.python .cell-code}\nimport jax \nimport jax.numpy as jnp\n\ndef my_foo(x):\n    x_1 = x\n    x_2 = jnp.square(x)\n    x_3 = jnp.power(x, 3)\n    out = x_1 + x_2 + x_3\n    return out\n\nprint(jax.make_jaxpr(my_foo)(3.0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{ lambda ; a:f32[]. let\n    b:f32[] = square a\n    c:f32[] = integer_pow[y=3] a\n    d:f32[] = add a b\n    e:f32[] = add d c\n  in (e,) }\n```\n:::\n:::\n\n\nas we can see our variables have now an explicit type (`f32`) and the functions `jnp.square` and `jnp.power` have been replaced by the `lax` equivalent `integer_pow`. `lax` can be thought as a sort of JAX low-level back-end. This expression will be then sent to the XLA compiler for being transformed in efficient machine code. \n\nBut what happens to our **jax expression** if we try to poke one of JAX [sharp edges](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html)? Let's introduce a side effect in our function and see\n\n::: {#74002e4b .cell execution_count=3}\n``` {.python .cell-code}\nimport jax \nimport jax.numpy as jnp\n\naccumulator = [] # global variable\n\ndef my_foo_with_side_effect(x):\n    x_1 = x\n    x_2 = jnp.square(x)\n    x_3 = jnp.power(x, 3)\n    out = x_1 + x_2 + x_3\n    accumulator.append(jnp.power(out, 2)) # side effect\n    return out\n\nprint(jax.make_jaxpr(my_foo_with_side_effect)(3.0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{ lambda ; a:f32[]. let\n    b:f32[] = square a\n    c:f32[] = integer_pow[y=3] a\n    d:f32[] = add a b\n    e:f32[] = add d c\n    _:f32[] = integer_pow[y=2] e\n  in (e,) }\n```\n:::\n:::\n\n\nas we can see our `accumulator` variable is not tracked in the **jax expression** neither are its associated computations! They will not be tracked by the compiler nor retrieved when the cached version of `foo` is executed a second time.\n\nWe know that most JAX functionalities (e.g. computing gradients) are achieved by applying the appropriated transformations to the functions of interest. These transformed functions will go down the same route of being translated into the relevant **jax expression** and compiled by XLA. Let's unravel what the gradient of `foo` would look like\n\n::: {#70e8c0d0 .cell execution_count=4}\n``` {.python .cell-code}\nfrom jax import grad\n\ngrad_my_foo = grad(my_foo) # we derive the gradient function of my_foo\n\nprint(jax.make_jaxpr(grad_my_foo)(3.0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{ lambda ; a:f32[]. let\n    b:f32[] = square a\n    c:f32[] = mul 2.0 a\n    d:f32[] = integer_pow[y=3] a\n    e:f32[] = integer_pow[y=2] a\n    f:f32[] = mul 3.0 e\n    g:f32[] = add a b\n    _:f32[] = add g d\n    h:f32[] = mul 1.0 f\n    i:f32[] = add_any h 1.0\n    j:f32[] = mul 1.0 c\n    k:f32[] = add_any i j\n  in (k,) }\n```\n:::\n:::\n\n\nWe can see how the derived **jax expression** now tracks all the required computations for computing the gradient pf `my_foo` with respect to its input `x`. We will expand more on the `grad` function later on this post. Let's move now to another very important feature of JAX: random number generation.\n\n# ARRRRGH!!!! explicit PRNG states\n\nGenerate random numbers using numpy it is a relatively straightforward matter:\n\n::: {#203d932d .cell execution_count=5}\n``` {.python .cell-code}\nx_1 = np.random.normal() # numpy is both easy...\nprint(x_1)\n\nx_2 = np.random.normal() # ...and intuitive!\nprint(x_1 == x_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.0415928805928754\nFalse\n```\n:::\n:::\n\n\nunder the hood numpy will do [alot of wizardry for us](https://numpy.org/doc/stable/reference/random/index.html) implicitly setting the state of the Pseudo-Random Number Generator (PRNG) every time we ask to simulate sampling from a given distribution (e.g. uniform, normal etc...). With JAX we must do a bit more  work as the library requires to explicitly pass a state whenever we call the PRNG\n\n::: {#b708236e .cell execution_count=6}\n``` {.python .cell-code}\nfrom jax import random\n\n# jax is less easy\nmaster_key = random.PRNGKey(666) # set the state with a seed\nprint(f\"Key {master_key}\")\nrandom.normal(key=master_key) # sample from the standard normal\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKey [  0 666]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\nArray(-0.10761635, dtype=float32)\n```\n:::\n:::\n\n\nthis implies that the numbers are **deterministically** generated at random given a certain state. Hence if we do not make sure to generate fresh new states whenever we require a new random behaviour we might incur in some rather nasty side effects\n\n::: {#097da82e .cell execution_count=7}\n``` {.python .cell-code}\naccumulator = 0\nfor _ in range(100):\n\n    x_1 = random.normal(key=master_key) # generate two numbers using the same state\n    x_2 = random.normal(key=master_key)\n    accumulator += int(x_1 == x_2)\n\nprint(accumulator)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n100\n```\n:::\n:::\n\n\nWithout modifying the state, calling `random.normal` will always generate the exact same sequence of random numbers! What we need to do in this case is to leverage the `split` function in the `random` module for splitting the original state (or key) in one or more sub-states (or sub-keys)\n\n::: {#d5a682bc .cell execution_count=8}\n``` {.python .cell-code}\naccumulator = 0\nseed_key = master_key\nfor _ in range(100):\n\n    seed_key, consume_key_1, consume_key_2 = random.split(seed_key, 3) # one key always left for generation\n    x_1 = random.normal(key=consume_key_1) # generate two numbers using different states\n    x_2 = random.normal(key=consume_key_2)\n    accumulator += int(x_1 == x_2)\n\nprint(accumulator)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0\n```\n:::\n:::\n\n\ndespite this behaviour might look as a big annoyance at first, it offers us a greater degree on control of when and where we want to see randomness.\n\n# Just In Time Compilation\n\nOne of the advantages of JAX is its ability to Just In Time (JIT) compile python code to different types of accelerating devices, be them CPU, GPU or TPU. By compiling and caching slow python code to optimized machine code. So let's see a simple example\n\n::: {#e6b53ff3 .cell execution_count=9}\n``` {.python .cell-code}\ndef silly_expensive_loopy_function(x):\n    \"\"\"A silly function, it does many completely useless computations.\n    However it is very loopy and expensive.\n    \n    Args:\n        x  (float): starting point of the silly function\n        \n    Returns:\n        x (float): output of the silly function\n    \"\"\"\n    for i in range(10):\n        for j in range(10):    \n            for k in range(10): \n                x += i + j + k + i*j + j*k + j**2 + k**2\n    return x\n\nprint(\"Pure Python\")\n%timeit silly_expensive_loopy_function(10.)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPure Python\n54.7 μs ± 202 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n```\n:::\n:::\n\n\nas we can see we have a quite hefty execution time, but what happens if we JIT compile our function through the relevant JAX transformation?\n\n::: {#688f87c4 .cell execution_count=10}\n``` {.python .cell-code}\nfrom jax import jit\n\njitted_silly_expensive_loopy_function = jit(silly_expensive_loopy_function)\n\nprint(\"Jitted Python with Compilation Time\")\n%timeit jitted_silly_expensive_loopy_function(10.).block_until_ready()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nJitted Python with Compilation Time\n3.06 μs ± 42 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n```\n:::\n:::\n\n\nas we can see execution time is almost 2 orders of magnitude lower for the JIT compiled function. The function of `.block_until_ready()` is to time not just compilation but also computation. \n\nIf compilation time can be avoided, through caching for example, we can achieve even further speed-up. This because once a piece of potentially slow python code is compiled and cached by JAX, it can be skipped altogether for subsequent computations.\n\n::: {#c8f7f2f8 .cell execution_count=11}\n``` {.python .cell-code}\njitted_silly_expensive_loopy_function(10.).block_until_ready()\n\nprint(\"Jitted Python without Compilation Time\")\n%timeit jitted_silly_expensive_loopy_function(666.).block_until_ready()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nJitted Python without Compilation Time\n3.05 μs ± 11.9 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n```\n:::\n:::\n\n\n# Looping vs Scanning\n\nJIT compilation is a great way for optimizing our potentially slow python code, however it comes with few gotchas and looping is one of them. If we have a function executing some relatively demanding computations over a long sequence \n\n::: {#cfa1f315 .cell execution_count=12}\n``` {.python .cell-code}\nstart_x_est=160.\ndx=1.\nh=1./10.\ng=.5/3.\ndt=1.\n\nX = 160. + (jnp.arange(1, 51) * 1.) + random.normal(shape=(50, ), key=master_key)\n\n@jit\ndef step(carry, x):\n    \"\"\" Step function for the g-h filter\n\n    Args:\n        carry (tupler): values to be carried over\n        x (float): data\n\n    Returns:\n        carry (tuple): updated components\n        x_est_update (float): updated estimate for the state\n    \"\"\"\n    previous_x_est, dx, h, g, dt = carry # h, g, and dt are fixed parameters\n    \n    x_pred = previous_x_est + (dx * dt) # system dynamics\n\n    residual = x - x_pred\n    dx = dx + h * (residual) / dt\n    updated_x_est = x_pred + g * residual\n    \n    return (updated_x_est, dx, h, g, dt ), updated_x_est\n```\n:::\n\n\nfor example, the above function illustrates the computations used by a [gh-filter](https://en.wikipedia.org/wiki/Alpha_beta_filter). This function is supposed to \n\n1. Step over a signal\n2. Generate an estimate of the state underlying the signal. This is given by a clever combination of the previous estimate and the current observed signal.\n3. Finally, provide the current estimate along with other relative parameters needed in the next step.\n\nIn this case, the most straightforward way to move the `step` function over the signal would be using a for loop\n\n::: {#012d202d .cell execution_count=13}\n``` {.python .cell-code}\n@jit\ndef loopy_function(X, start_x_est, dx, h, g, dt=1.):\n    \"\"\"Gh filter logic implmented with for loop\n\n    Args:\n        X (Device Array): Input data\n        start_x_est (float): Start values for the estimated state\n        dx (float): Rate of change in the system dynamics\n        h (float): Update value\n        g (float): Gain value\n        dt (float): Frequency \n\n    Returns:\n        output (Device Array): Estimate state value\n    \"\"\"\n    output = []\n    carry = (start_x_est, dx, h, g, dt)\n    for x in X:\n        \n        carry, yhat = step(carry=carry, x=x)\n        output.append(yhat)\n    \n    return jnp.array(output)\n\nprint(\"Jitted for loop\")\n%timeit loopy_function(X=X, start_x_est=start_x_est, dx=dx, h=h, g=g, dt=dt).block_until_ready()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nJitted for loop\n40 μs ± 7.2 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```\n:::\n:::\n\n\nJIT compiling both our function gives us a convenient speedup as we have seen before. But what happens if we increase the length of the signal over which we want to step?\n\n::: {#da011669 .cell execution_count=14}\n``` {.python .cell-code}\nX = 160. + (jnp.arange(1, 101) * 1.) + random.normal(shape=(100, ), key=master_key)\n\nprint(\"Jitted for loop over a long sequence\")\n%timeit loopy_function(X=X, start_x_est=start_x_est, dx=dx, h=h, g=g, dt=dt).block_until_ready()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nJitted for loop over a long sequence\n70.3 μs ± 12.4 μs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n```\n:::\n:::\n\n\nthat is a considerable increase in computational time which doesn't seem to increase linearly with the number of steps in our signal. What is happening under the hood is that XLA has to unroll all the computations included in our for loop in order to compile them, would our signal be much longer we would wait until the end of times for the compiler to do its job.\n\nThankfully, JAX offers a solution to this through its the lower-level API `lax` using `scan`.\n\n::: {#9627486e .cell execution_count=15}\n``` {.python .cell-code}\nfrom jax.lax import scan\n\n@jit\ndef scan_function(X, start_x_est, dx, h, g, dt=1.):\n    \"\"\"Gh filter logic implemented with lax scan\n\n    Args:\n        X (Device Array): Input data\n        start_x_est (float): Start values for the estimated state\n        dx (float): Rate of change in the system dynamics\n        h (float): Update value\n        g (float): Gain value\n        dt (float): Frequency \n\n    Returns:\n        output (Device Array): Estimate state value\n    \"\"\"\n    carry, output= scan(\n        step, # this function is going to be moved along the input series,\n        (start_x_est, dx, h, g, dt), # these are the initial values of the carry,\n        X # this is the series over which step is moved\n    )\n    return output\n```\n:::\n\n\nthe syntax of `scan` might looks a bit un-intuitive if you are used to for loops but it is actually  quite simple. It will iterate our `step` function over all the values of `X` and compute both the output relative to the current value of `X` and a `carry`. As the name suggests the `carry` will carry over any information that might be required by `step` in the future, be those parameters or computed values. In our example the carry is made of fixed parameters and state variables computed inside `step`. But let's look at a quick perfromance benchmark now\n\n::: {#85a410a9 .cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\nscan_timings = []\nloop_timings = []\nfor length in [10, 20, 40, 80, 100]:\n\n    X = (\n        160. + \n        (jnp.arange(1, length+1) * 1.) + \n        (random.normal(shape=(length, ), key=master_key) * 10)\n    )\n\n    kwargs = {\n        \"X\": X,\n        \"start_x_est\": start_x_est, \n        \"dx\": dx, \n        \"h\": h, \n        \"g\": g, \n        \"dt\": dt\n    }\n\n    loopy_result = %timeit -o -n300 loopy_function(**kwargs).block_until_ready()\n    scan_result = %timeit -o -n300 scan_function(**kwargs).block_until_ready()\n\n    scan_timings.append(\n        scan_result.best\n    )\n    loop_timings.append(\n        loopy_result.best\n    )\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 3))\nax.plot(\n    [10, 20, 40, 80, 100],\n    loop_timings,\n    label=\"For Loop\"\n)\nax.plot(\n    [10, 20, 40, 80, 100],\n    scan_timings,\n    label=\"Scan\"\n)\nax.legend()\nax.set_xlabel(\"Input Legth\")\nax.set_ylabel(\"Compilation + Execution Time\\nSeconds\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe slowest run took 14.66 times longer than the fastest. This could mean that an intermediate result is being cached.\n26.2 μs ± 41.8 μs per loop (mean ± std. dev. of 7 runs, 300 loops each)\nThe slowest run took 9.48 times longer than the fastest. This could mean that an intermediate result is being cached.\n18.5 μs ± 24.1 μs per loop (mean ± std. dev. of 7 runs, 300 loops each)\nThe slowest run took 18.78 times longer than the fastest. This could mean that an intermediate result is being cached.\n54.1 μs ± 92.8 μs per loop (mean ± std. dev. of 7 runs, 300 loops each)\nThe slowest run took 8.59 times longer than the fastest. This could mean that an intermediate result is being cached.\n17.8 μs ± 22.1 μs per loop (mean ± std. dev. of 7 runs, 300 loops each)\nThe slowest run took 31.41 times longer than the fastest. This could mean that an intermediate result is being cached.\n163 μs ± 324 μs per loop (mean ± std. dev. of 7 runs, 300 loops each)\nThe slowest run took 7.23 times longer than the fastest. This could mean that an intermediate result is being cached.\n21.2 μs ± 24 μs per loop (mean ± std. dev. of 7 runs, 300 loops each)\nThe slowest run took 100.99 times longer than the fastest. This could mean that an intermediate result is being cached.\n777 μs ± 1.77 ms per loop (mean ± std. dev. of 7 runs, 300 loops each)\nThe slowest run took 7.84 times longer than the fastest. This could mean that an intermediate result is being cached.\n24.5 μs ± 26.5 μs per loop (mean ± std. dev. of 7 runs, 300 loops each)\n63.1 μs ± 864 ns per loop (mean ± std. dev. of 7 runs, 300 loops each)\nThe slowest run took 6.38 times longer than the fastest. This could mean that an intermediate result is being cached.\n24.4 μs ± 24.5 μs per loop (mean ± std. dev. of 7 runs, 300 loops each)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](1_jax_basics_files/figure-html/cell-17-output-2.png){}\n:::\n:::\n\n\nas you can see the difference is negligible for short sequences (scan might even require more time!) but increases massively for longer sequences. Lets's look at the result of our scanned function now\n\n::: {#dedb5e95 .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\nyhat = scan_function(\n    X=X, \n    start_x_est=start_x_est, \n    dx=dx, \n    h=h, \n    g=g, \n    dt=dt\n)\n\nax = visualize_univariate_time_series(\n    time_series=X,\n    label=\"Data\"\n)\n\nax = visualize_univariate_time_series(\n    time_series=yhat,\n    ax=ax,\n    label=\"State Estimate\"\n)\nax.plot(\n    np.arange(1, 101),\n    160. + (jnp.arange(1, 101) * 1.),\n    linestyle=\"--\",\n    c=\"k\",\n    alpha=0.5,\n    label=\"System\"\n)\nax.legend()\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"Value\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](1_jax_basics_files/figure-html/cell-18-output-1.png){}\n:::\n:::\n\n\nall seems to be in order and executed in record time!\n\n# Computing Gradients\n\nLet's come now to the other central transformation offered by JAX: `grad`. With `grad` we can transform python functions in gradient functions, what do we mean by this? Let's take the square function as an example\n\n::: {#0f7eb854 .cell execution_count=18}\n``` {.python .cell-code}\ndef square(x):\n    \"\"\"Return the square of x\n    \n    Args:\n        x  (float): values to be squared\n        \n    Returns:\n        ssq_ (float): square of x\n    \"\"\"\n    sq_ = x ** 2\n    return sq_\n\ndx = grad(square)\n```\n:::\n\n\nby passing this function to the `grad` transformation, we can obtain a new function that will evaluate the gradient of `x` with respect to `sq_` for us, pretty convenient.\n\n::: {#b1e6ad28 .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\nfig, axs = plt.subplots(1, 2, figsize=(8, 4), sharex=True)\nfor x in np.linspace(-10, 10, 100):\n\n    axs[0].scatter(\n        x, \n        square(x),\n        s=1,\n        c=\"k\"\n    )\n    axs[1].scatter(\n        x, \n        dx(x),\n        s=1,\n        c=\"r\"\n    )\n\nfor ax in axs:\n    ax.set_xlabel(\"x\")\n\naxs[0].set_ylabel(\"$x^2$\")\naxs[1].set_ylabel(\"$\\dfrac{\\partial f(x)}{\\partial x}$\")\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:21: SyntaxWarning: invalid escape sequence '\\d'\n<>:21: SyntaxWarning: invalid escape sequence '\\d'\n/var/folders/h_/lq2hvf816xs9ffng6570sblh0000gn/T/ipykernel_3597/2664859271.py:21: SyntaxWarning: invalid escape sequence '\\d'\n  axs[1].set_ylabel(\"$\\dfrac{\\partial f(x)}{\\partial x}$\")\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](1_jax_basics_files/figure-html/cell-20-output-2.png){}\n:::\n:::\n\n\nObtaining the derivative of a single variable function however is not that impressive, `scipy.optimize.approx_fprime` can easily achieve the same result although with a slightly more laborious approach. So is the advantage of `grad` only to provide some syntactic sugar? Well, not really.\n\n1. Differently from `scipy.optimize.approx_fprime` which relies on finite difference for approximating gradients, `grad` leverages [automatic differentiation](https://www.google.com/search?client=safari&rls=en&q=automatic+differtiation&ie=UTF-8&oe=UTF-8#fpstate=ive&vld=cid:cbd8dd04,vid:wG_nF1awSSY) for obtaining more numerically stable results.\n\n2. The use of automatic differentiation allows us to compute gradients of very complex and composite functions.\n\n3. The syntactic sugar capabilities of `grad` allow us to customize which gradients we are interested to and to compute them with respect to many different data structures.\n\nLet's look at the two variables function `sum_of_squares`\n\n::: {#11429574 .cell execution_count=20}\n``` {.python .cell-code}\ndef sum_of_squares(x_1, x_2):\n    \"\"\"Return the sum of squares of x_1 and x_2\n    \n    Args:\n        x_1  (float): first variable to be squared\n        x_2  (float): second variable to be squared\n        \n    Returns:\n        ssq_ (float): square of x\n    \"\"\"\n    ssq_ = jnp.square(x_1) + jnp.square(x_2)\n    return ssq_\n\ndxx = grad(sum_of_squares, argnums=[0, 1])\n```\n:::\n\n\nIn this case `sum_of_squares` takes two variables as inputs so we have to specify for which one we want to compute the partial derivative, we do that using the `argnums` argument. Let's see at the results produced by `dxx`\n\n::: {#7b97d1b8 .cell execution_count=21}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show supplementary code\"}\nssq_results = []\ngrads_x_1 = []\ngrads_x_2 = []\nspace = np.linspace(-10, 10, 10)\nfig, axs = plt.subplots(1, 2, figsize=(8, 4))\n\nfor x_1 in space:\n\n    for x_2 in space:\n\n        grads = dxx(x_1, x_2) # gradients come as a tuple of device arrays\n\n        grads_x_1.append(grads[0])\n        grads_x_2.append(grads[1])\n\n        ssq_results.append(sum_of_squares(x_1, x_2))\n\nx, y = np.meshgrid(\n    space, \n    space\n)\n\naxs[0].scatter(\n    x.flatten(),\n    y.flatten(), \n    c=ssq_results,\n    cmap=\"viridis\"\n)\naxs[1].scatter(\n    grads_x_1, \n    grads_x_2, \n    c=ssq_results,\n    cmap=\"viridis\"\n)\n\naxs[0].set_xlabel(\"$x_1$\")\naxs[0].set_ylabel(\"$x_2$\")\n\naxs[1].set_xlabel(\"$\\dfrac{\\partial f(x_1, x_2)}{\\partial x_1}$\")\naxs[1].set_ylabel(\"$\\dfrac{\\partial f(x_1, x_2)}{\\partial x_2}$\")\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:39: SyntaxWarning: invalid escape sequence '\\d'\n<>:40: SyntaxWarning: invalid escape sequence '\\d'\n<>:39: SyntaxWarning: invalid escape sequence '\\d'\n<>:40: SyntaxWarning: invalid escape sequence '\\d'\n/var/folders/h_/lq2hvf816xs9ffng6570sblh0000gn/T/ipykernel_3597/1542072068.py:39: SyntaxWarning: invalid escape sequence '\\d'\n  axs[1].set_xlabel(\"$\\dfrac{\\partial f(x_1, x_2)}{\\partial x_1}$\")\n/var/folders/h_/lq2hvf816xs9ffng6570sblh0000gn/T/ipykernel_3597/1542072068.py:40: SyntaxWarning: invalid escape sequence '\\d'\n  axs[1].set_ylabel(\"$\\dfrac{\\partial f(x_1, x_2)}{\\partial x_2}$\")\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](1_jax_basics_files/figure-html/cell-22-output-2.png){}\n:::\n:::\n\n\nthe best way to describe `grad` is to consider it as a tranformation able to compute gradients with respect to almost any type of data structure. The only requirement is that such data structure is in the form of a [Pytree](https://jax.readthedocs.io/en/latest/pytrees.html). A Pytree is a *\"...a tree-like structure built out of container-like Python objects...\"*, it usually have the following form\n\n```\npytree\n|\n|_node_1\n|      |_leaf_1.1\n|      |_leaf_1.2\n|\n|_node_2\n|      |_leaf_2.1\n|      |_leaf_2.2\n|      |_node_2.1\n|               |_leaf_2.1.1\n|               |_...\n|\n|_...                \n```\neach node and leaf can by default be any python data structures among lists, tuples and dicts however JAX allows to register others as valid pytree. The tree-like structure offers a great deal of flexibility for specifying things like the parameters of a model. Let's see a concrete example with a linear regression\n\n::: {#33f3b26b .cell execution_count=22}\n``` {.python .cell-code}\nfrom jax import value_and_grad\n\nX = random.normal(key=master_key, shape=(1000, 40))\ny = random.normal(key=master_key, shape=(1000,))\n\n# a dictionary as a pytree with 2 nodes and 41 leaves\nmy_parameters = {\n    \"alpha\": random.normal(key=master_key, shape=(1,)), \n    \"beta\": random.normal(key=master_key, shape=(X.shape[1],))\n}\n```\n:::\n\n\nonce we have defined our parameters, we can pass them to an appropriate function and let `grad` do its magic for deriving the gradient. \n\n::: {#b747fe7b .cell execution_count=23}\n``` {.python .cell-code}\n@jit\ndef sum_of_squared_errors(y, yhat):\n    \"\"\"Return the square of x\n    \n    Args:\n        y  (DeviceArray): ground truth values\n        yhat  (DeviceArray): model predictions\n        \n    Returns:\n        ssq (float): sum of the squares of the difference between y and yhat.\n    \"\"\"\n    return jnp.sum(jnp.square(y - yhat))\n\n@jit\ndef linear_regression_loss(X, y, parameters):\n    \"\"\"Compute the loss,  sum_of_squared_errors, for a linear regression model.\n\n    Args:\n        X  (DeviceArray): model covariates.\n        y  (DeviceArray): ground truth values.\n        parameters (dictionary): model's parameters.\n        \n    Returns:\n        loss (float): loss for the linear regression model.\n\n    \"\"\"\n    yhat = jnp.dot(X, parameters[\"beta\"]) + parameters[\"alpha\"]\n    loss = sum_of_squared_errors(y=y, yhat=yhat)\n    return loss\n\nlinear_regression_grad = value_and_grad(\n    fun=linear_regression_loss,\n    argnums=2 # w.r.t. parameters\n)\n```\n:::\n\n\nin this case we used a variation of `grad` named `value_and_grad` that returns not just the gradient but also the output of the function, which in this case is whatever comes out of `sum_of_squared_errors`.\n\nThe covenience of `grad` (and its variations) is that it will return the partial derivates of our parameters with respect to the output of `sum_of_squared_errors` keeping the same pytree structure!\n\n::: {#0bba997c .cell execution_count=24}\n``` {.python .cell-code}\nsse, gradients = linear_regression_grad(X, y, my_parameters)\n\ngrad_alpha = gradients['alpha']\ngrad_beta = gradients['beta']\n\nprint(f\"SSE: {sse}\")\nprint(f\"Partial Derivative alpha: {grad_alpha}\")\nprint(f\"Partial Derivatives beta: {grad_beta}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSSE: 53050.79296875\nPartial Derivative alpha: [-478.6121]\nPartial Derivatives beta: [  280.32477   272.2938   7183.9194   1950.6066  -2001.7474  -1227.3281\n   520.00366  1380.3948   1599.383    -864.306   -2723.1323   2912.9316\n  2502.141     300.33267   818.23303  1697.8423  -2095.8745   4386.4863\n  1800.3965    320.25903 -1798.7013   1561.4841  -2179.1855  -1486.0132\n  1934.0378   1267.4854  -4054.1572   -267.9964   1781.633    1066.2424\n  1163.5311    385.65964  -921.5457   1163.6954  -2004.7834   3002.1978\n  2724.7102  -2948.397   -4152.8105   3641.4666 ]\n```\n:::\n:::\n\n\nThis behaviour is extensible to virtually any python object as far as it is registered as a pytree. So how can we register a python object as a pytree? Let's take a `NamedTuple` as an example\n\n::: {#ade21133 .cell execution_count=25}\n``` {.python .cell-code}\nfrom collections import namedtuple\n\nLinearRegressionParameters = namedtuple(\n    \"LinearRegressionParameters\", \n    [\"alpha\", \"beta\"]\n)\n```\n:::\n\n\nwhen we register a pytree we have to tell JAX how to unpack the leaves into an iterable and pack them back in the original tree structure\n\n::: {#39b9dbc9 .cell execution_count=26}\n``` {.python .cell-code}\nfrom jax.tree_util import register_pytree_node\n\nregister_pytree_node(\n    LinearRegressionParameters,\n    lambda xs: (tuple(xs), None),  # tell JAX how to unpack to an iterable\n    lambda _, xs: LinearRegressionParameters(*xs)       # tell JAX how to pack back into LinearRegressionPArameters\n)\n```\n:::\n\n\nwe now just need to modify our `linear_regression_loss` function slightly in order to use the `LinearRegressionParameters` instead of a dictionary.\n\n::: {#5cb9baa9 .cell execution_count=27}\n``` {.python .cell-code}\n@jit\ndef linear_regression_loss(X, y, parameters):\n    \"\"\"Compute the loss,  sum_of_squared_errors, for a linear regression model.\n\n    Args:\n        X  (DeviceArray): model covariates.\n        y  (DeviceArray): ground truth values.\n        parameters (NamedTuple): model's parameters.\n        \n    Returns:\n        loass (float): loss for the linear regression model.\n    \"\"\"\n    yhat = jnp.dot(X, parameters.beta) + parameters.alpha\n    loss = sum_of_squared_errors(y=y, yhat=yhat)\n    return loss\n\nlinear_regression_grad = value_and_grad(\n    fun=linear_regression_loss,\n    argnums=2\n)\n\nmy_parameters = LinearRegressionParameters(\n    random.normal(key=master_key, shape=(1,)),\n    random.normal(key=master_key, shape=(X.shape[1],))\n)\n\nsse, gradients = linear_regression_grad(X, y, my_parameters)\n\ngrad_alpha = gradients.alpha\ngrad_beta = gradients.beta\n\nprint(f\"SSE: {sse}\")\nprint(f\"Partial Derivative alpha: {grad_alpha}\")\nprint(f\"Partial Derivatives beta: {grad_beta}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSSE: 53050.79296875\nPartial Derivative alpha: [-478.6121]\nPartial Derivatives beta: [  280.32477   272.2938   7183.9194   1950.6066  -2001.7474  -1227.3281\n   520.00366  1380.3948   1599.383    -864.306   -2723.1323   2912.9316\n  2502.141     300.33267   818.23303  1697.8423  -2095.8745   4386.4863\n  1800.3965    320.25903 -1798.7013   1561.4841  -2179.1855  -1486.0132\n  1934.0378   1267.4854  -4054.1572   -267.9964   1781.633    1066.2424\n  1163.5311    385.65964  -921.5457   1163.6954  -2004.7834   3002.1978\n  2724.7102  -2948.397   -4152.8105   3641.4666 ]\n```\n:::\n:::\n\n\nHere we conclude this first introductory post on the basics of JAX. We want to stress that this is just a **small** selection of the features offered by JAX.\n\nWe can think of it as a distillation of some of the contents reported in the JAX [online documentation](https://jax.readthedocs.io/en/latest/)\n\nThat said, what we outlined so far should equip us with enough knowledge to develop some simple models in the next posts.\n\n# Hardware and Requirements\n\nHere you can find the hardware and python requirements used for building this post.\n\n::: {#fc003cd1 .cell execution_count=28}\n``` {.python .cell-code}\n%watermark\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLast updated: 2025-03-28T09:22:09.149860+00:00\n\nPython implementation: CPython\nPython version       : 3.13.2\nIPython version      : 9.0.2\n\nCompiler    : Clang 18.1.8 \nOS          : Darwin\nRelease     : 24.3.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 14\nArchitecture: 64bit\n\n```\n:::\n:::\n\n\n::: {#40955703 .cell execution_count=29}\n``` {.python .cell-code}\n%watermark --iversions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnumpy     : 2.2.4\njax       : 0.5.2\nmatplotlib: 3.10.1\n\n```\n:::\n:::\n\n\n",
    "supporting": [
      "1_jax_basics_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}