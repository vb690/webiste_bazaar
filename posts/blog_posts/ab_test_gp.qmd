---
title: Analyzing A/B Test Data using Gaussian Processes
description: 'This post illustrates how to analyze longitudinal A/B test data using Gaussian Process.'
date: '2025-03-29'
categories:
  - JAX
  - Numpyro
  - Bayesian Statistics
  - A/B Test
  - Gaussian Process
bibliography: bibliographies/ab_test_gp.bib
jupyter: python3
---

```{python}
#| code-fold: true
#| code-summary: Show supplementary code
#| vscode: {languageId: python}

%load_ext watermark

NUM_CHAINS = 4

from typing import Dict, Any, Callable, Tuple

from functools import partial

import pandas as pd
import numpy as np

import scipy
from scipy.stats import lognorm, median_abs_deviation

import jax
from jax.typing import ArrayLike
import jax.random as random
from jax import jit
from jax import numpy as jnp
from jax import vmap

import numpyro
numpyro.set_host_device_count(NUM_CHAINS)
import numpyro.distributions as dist
from numpyro.infer import (
    MCMC,
    NUTS,
    Predictive
)

import matplotlib.pyplot as plt
from matplotlib.figure import Figure
from matplotlib import gridspec

import seaborn as sns

def plot_univariate_series(
        series_data: Dict[Any, Any], 
        ax: plt.Axes, 
        **plot_kwargs: Any
    ) -> plt.Axes:
    ax.scatter(
        series_data["x"]["data"],
        series_data["y"]["data"],
        **plot_kwargs
    )
    ax.plot(
        series_data["x"]["data"],
        series_data["y"]["data"],
        **plot_kwargs
    )
    ax.set_xlabel(series_data["x"]["label"])
    ax.set_ylabel(series_data["y"]["label"])

    ax.tick_params(
        direction="in",
        top=True, axis="x",
        rotation=45,
    )
    ax.grid(
        visible=True,
        which="major",
        axis="x",
        color="k",
        alpha=0.25,
        linestyle="--",
    )
    return ax
```

# Premise

What will be illustrated in this post is strongly in spired by the content of the books "Statistical Rethinking" [@mcelreath2018statistical] and "Bayesian Data Analysis 3rd Edition" [@gelman1995bayesian]. In particular, the idea to separately code and illustrate the behavior of different covariance kernel functions comes from the amazing ["Kernel Cookbook"](https://www.cs.toronto.edu/~duvenaud/cookbook/) and PhD thesis of David Duvenaud [@duvenaud2014automatic].

This post assumes some level of knowledge in bayesian statistics and probabilistic programming.

## What we will cover

1. Very brief illustration of longitudinal A/B test within and observational paradigm.
2. Very brief illustration of gaussian processes and their application to analyzing A/B test data.
3. Overview of how to implement a gaussian process model using [Numpyro](https://num.pyro.ai/en/stable/index.html) and [JAX](https://docs.jax.dev/en/latest/index.html).
4. Simulating A/B test data.
5. Analyzing A/B test data within an modelling setting.

## What we will **not** cover

1. Detailed coverage of A/B test (e.g., sampling, randomization etc...).
2. Hypothesis testing (we will focus on modelling).
3. Fundamentals of bayesian statistics.
4. Detailed overview of Gaussian processes.
5. Probabilistic programming and sampling algorithms.

# Introduction

## Longitudinal A/B tests in observational settings

When we talk about A/B test we usually refer to a research method used for evaluating if a given intervention is having an impact on a pre-defined outcome variable measured inside a sample. For doing so, we can draw two distinct samples (the A and B group) from a population of interest, subject one of the two to the intervention and then measure observed differences in the outcome variable. Subject to **several** assumptions and pre-conditions (we suggest reading part V of "Regression and Other Stories" [@gelman2021regression]), if we observe a difference between the two groups when can conclude that our intervention might have had an impact on our outcome variable.

## Gaussian Process

The Gaussian Process $GP$ can be thought as the continuous generalization of basis function regression (see Chapter 20 of [@gelman1995bayesian]). We can think of it as a stochastic process where any point drawn from it, $x_1, \dots, x_n$, comes from a multi-dimensional gaussian. In other words, it is as a prior distribution over an un-known function $\mu(x)$ defined as

$$
\mu(x_1), \dots, \mu(x_n) \sim \mathcal{N}((m(x_1), \dots, m(x_n)), k(x, \dots, x_n))
$$

or more compactly

$$
\mu(x) \sim GP(m, k)
$$

where $m$ is a mean function and $k$ is a covariance function. We can already have an intuition of how defining the $GP$ in terms mean and covariance **functions** gives us quite some flexibility as it allows us to produce a model than can interpolate for all the value of $x$.

The $m$ function provides the most likely guess for the $GP$ like the mean vector of a multi-dimensional Gaussian, deviation from this expected model are then handled by the covariance function $k$.

The $k$ function (often called Kernel) allows to structurally define the $GP$ behavior at any two points by producing an $n \times n$ covariance function given by evaluating $k(x, x')$ for every $x_1, \dots, x_n$.

One convenient property of $GP$ is that the sum and multiplication of two or more $GP$ is itself a $GP$, this allows to combine different types of Kernels for imposing specific structural constrains.

## Gaussian Process for A/B test

Although there aren't many papers illustrating how $GP$ can be used for analyzing A/B test data we found this interesting work by [@benavoli2015gaussian] from [IDSIA](https://www.idsia.usi-supsi.ch/) that we decided to adapt to our use-case.



# Implementing a Gaussian Process Model in Numpyro

In this section we will illustrate how we can implement a $GP$ model using Numpyro. Numpyro offers a Numpy-like Backend for [Pyro](https://pyro.ai/) a Probabilistic Programming Language (PPL). Other than offering the flexibility of specifying models using the familiar Numpy interface, Numpyro is perfectly integrated with JAX allowing us to tap into its JIT compilation capabilities.

```{python}
BLUE = (0, 83, 159)
BLUE = tuple(value / 255. for value in BLUE)

RED = (238, 28, 46)
RED = tuple(value / 255. for value in RED)

TIME_IDX = np.arange(7*8)
DATES = pd.date_range(
    start="01-01-2023",
    periods=len(TIME_IDX)
).values
TIME = "date"
MODELS = [
    "Branch A",
    "Branch B",
    "Branch C",
    "Branch D",
    "Branch E",
]
```

## Kernel Functions

```{python}
#| code-fold: true
#| code-summary: Show supplementary code
#| vscode: {languageId: python}

def simulate_kernel(
    X: ArrayLike, 
    X_prime:ArrayLike, 
    kernel_function: Callable, 
    samples: int,
    mean: float = 0.
) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:
    generated_covariance = kernel_function(
        X=X,
        X_prime=X_prime,
    )
    distance = generated_covariance[:, len(X) // 2]
    sampled_functions = np.random.multivariate_normal(
        mean=np.zeros(
            shape=generated_covariance.shape[0]
        ) + mean,
        cov=generated_covariance,
        size=samples
    )
    return generated_covariance, distance, sampled_functions

def visualize_kernel(
    X: ArrayLike, 
    generated_covariance: ArrayLike, 
    distance: ArrayLike, 
    sampled_functions: ArrayLike, 
    kernel_name: str
) -> Figure:

    fig = plt.figure(
        figsize=(8, 8),
        tight_layout=True
    )
    grid = gridspec.GridSpec(
        nrows=2,
        ncols=2
    )
    ax_functions = fig.add_subplot(grid[0, :])
    ax_distance = fig.add_subplot(grid[1, 0])
    ax_covariance = fig.add_subplot(grid[1, 1])

    for index in range(sampled_functions.shape[0]):

        ax_functions = plot_univariate_series(
            series_data={
                "x": {
                    "data": X,
                    "label": "x"
                },
                "y": {
                    "data": sampled_functions[index, :],
                    "label": "y"
                },

            },
            ax=ax_functions,
            alpha=0.5
        )

    ax_functions.set_title("Sampled Functions \n from MvNormal")
    ax_functions.axhline(0, linestyle="--", c="k")

    ax_distance.plot(
        X - (len(X) // 2),
        distance
    )
    ax_distance.grid(alpha=0.5)
    ax_distance.set_title(f"Distance Function \n Determined by {kernel_name} Kernel")
    ax_covariance.set_ylabel("Similarity")
    ax_covariance.set_xlabel("Distance")

    ax_covariance.imshow(
        generated_covariance
    )
    ax_covariance.set_ylabel("x'")
    ax_covariance.set_xlabel("x")
    ax_covariance.set_title(f"Covariance \n Determined by {kernel_name} Kernel")


    plt.suptitle(f"{kernel_name} Kernel")
    return fig
```

### Radial Basis Function Kernel

```{python}
@jit
def RBF_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    variance: float, 
    length: float, 
    noise: float,
    jitter: float =1.0e-6, 
    include_noise: bool =True
) -> ArrayLike:
    squared_differences = jnp.power(
        (X[:, None] - X_prime),
        2.0
    )
    squared_length_scale = 2 * jnp.power(
        length,
        2.0
    )
    covariance_matrix = jnp.exp(
        - (squared_differences / squared_length_scale)
    )
    scaled_covariance_matrix = variance * covariance_matrix

    if include_noise:
        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])

    return scaled_covariance_matrix
```


```{python}
kernel_function = partial(
    RBF_kernel,
    variance=1,
    length=10,
    noise=0.001
)
generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="RBF"
)
plt.show()
```

### Matern Kernel

```{python}
@jit
def rational_quadratic_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    variance: float, 
    length: float, 
    noise: float, 
    alpha: float, 
    jitter: float =1.0e-6, 
    include_noise: bool =True
) -> ArrayLike:

    squared_differences = jnp.power(
        (X[:, None] - X_prime),
        2.0
    )
    squared_length_scale = 2 * alpha * jnp.power(
        length,
        2.0
    )
    covariance_matrix = jnp.power(
        1 + (squared_differences / squared_length_scale),
        - alpha
    )
    scaled_covariance_matrix = variance * covariance_matrix

    if include_noise:
        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])

    return scaled_covariance_matrix
```

```{python}
kernel_function = partial(
    rational_quadratic_kernel,
    variance=1,
    length=10,
    alpha=3,
    noise=0.001
)
generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3

)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="Rational Quadratic"
)
plt.show()
```

### Periodic Kernel

```{python}
@jit
def periodic_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    variance: float, 
    length: float, 
    noise: float, 
    period: float, 
    jitter: float = 1.0e-6, 
    include_noise: bool =True
) -> ArrayLike:

    periodic_difference = jnp.pi * jnp.abs(X[:, None] - X_prime) / period
    sine_squared_difference = 2 * jnp.power(
        jnp.sin(periodic_difference),
        2.0
    )
    squared_length_scale = jnp.power(
        length,
        2.0
    )
    covariance_matrix = jnp.exp(
        - (sine_squared_difference / squared_length_scale)
    )
    scaled_covariance_matrix = variance * covariance_matrix

    if include_noise:
        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])

    return scaled_covariance_matrix
```

```{python}
kernel_function = partial(
    periodic_kernel,
    variance=1,
    length=5,
    period=10,
    noise=0.001
)

generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="Periodic"
)
plt.show()
```

### Linear Kernel

```{python}
@jit
def linear_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    variance: float, 
    noise: float, 
    jitter: float = 1.0e-6, 
    include_noise: bool = True
) -> ArrayLike:
    scaled_covariance_matrix = variance + (X[:, None] * X_prime)
    if include_noise:
        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])

    return scaled_covariance_matrix
```

```{python}
kernel_function = partial(
    linear_kernel,
    variance=0.01,
    noise=0.001
)
generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="Linear"
)
plt.show()
```

### White Noise Kernel

```{python}
@jit
def white_noise_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    variance: float
) -> ArrayLike:
    covariance_matrix = variance * jnp.eye(X.shape[0])
    return covariance_matrix
```

```{python}
kernel_function = partial(
    white_noise_kernel,
    variance=0.01,
)
generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="White Noise"
)
plt.show()
```

### Combining Kernels

```{python}
@partial(jit, static_argnums=(2,3))
def additive_combined_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    first_kernel: Callable, 
    second_kernel: Callable
) -> ArrayLike:
    return first_kernel(X=X, X_prime=X_prime) + second_kernel(X=X, X_prime=X_prime)

@partial(jit, static_argnums=(2,3))
def multiplicative_combined_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    first_kernel: Callable, 
    second_kernel: Callable
) -> ArrayLike:
    return first_kernel(X=X, X_prime=X_prime) * second_kernel(X=X, X_prime=X_prime)
```

```{python}
kernel_function_linear = partial(
    linear_kernel,
    variance=0.0001,
    noise=0.000
)
kernel_function_rbf = partial(
    RBF_kernel,
    variance=30,
    length=2,
    noise=10
)

kernel_function = partial(
    additive_combined_kernel,
    first_kernel=kernel_function_linear,
    second_kernel=kernel_function_rbf
)

generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="Additive Linear+RBF"
)
plt.show()
```

```{python}
kernel_function_periodic = partial(
    periodic_kernel,
    variance=5,
    length=5,
    period=10,
    noise=0.0001
)
kernel_function_rbf = partial(
    RBF_kernel,
    variance=.1,
    length=50,
    noise=0.0001
)

kernel_function = partial(
    multiplicative_combined_kernel,
    first_kernel=kernel_function_periodic,
    second_kernel=kernel_function_rbf
)

generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="Multiplicative Periodic RBF"
)
plt.show()
```

## The Gaussian Process Model

```{python}
def RBF_gaussian_process(
        trend_kernel_variance_dist,
        trend_kernel_noise_dist,
        trend_kernel_length_dist,

        seasonal_kernel_variance_dist,
        seasonal_kernel_noise_dist,
        seasonal_kernel_length_dist,
        seasonal_kernel_period_dist,


        day_dist,
        effect_mean_dist,
        effect_mu_dist,
        effect_sigma_dist,
    ):

    def get_model(X, y, days, arm_indexes):
        # The parameters of the kernel function are sampled from their
        # associated prior distributions
        trend_kernel_variance = numpyro.sample(
            "trend_kernel_variance",
            trend_kernel_variance_dist
        )
        trend_kernel_noise = numpyro.sample(
            "trend_kernel_noise",
            trend_kernel_noise_dist
        )
        trend_kernel_length = numpyro.sample(
            "trend_kernel_length",
            trend_kernel_length_dist
        )

        seasonal_kernel_variance = numpyro.sample(
            "seasonal_kernel_variance",
            seasonal_kernel_variance_dist
        )
        seasonal_kernel_noise = numpyro.sample(
            "seasonal_kernel_noise",
            seasonal_kernel_noise_dist
        )
        seasonal_kernel_length = numpyro.sample(
            "seasonal_kernel_length",
            seasonal_kernel_length_dist
        )
        seasonal_kernel_period = numpyro.sample(
            "seasonal_kernel_length_periodic",
            seasonal_kernel_period_dist
        )

        coefficients_days = numpyro.sample(
            "coefficients_days",
            day_dist
        )
        noise = numpyro.sample(
            "noise_dist",
            noise_dist
        )

        effect_mu = numpyro.sample(
            "effect_mu",
            effect_mu_dist,
        )
        effect_sigma = numpyro.sample(
            "effect_sigma",
            effect_sigma_dist,
        )
        with numpyro.plate("number_arms", X.shape[1]):
            effect = numpyro.sample(
                "effect_dist",
                dist.Normal(
                    effect_mu,
                    effect_sigma,
                )
            )

        # Compute the covariance matrix using the RBF kernel
        covariance_matrix_trend = RBF_kernel(
            X=X,
            X_prime=X,
            variance=trend_kernel_variance,
            length=trend_kernel_length,
            noise=trend_kernel_noise
        )
        covariance_matrix_seasonal = periodic_kernel(
            X=X,
            X_prime=X,
            variance=seasonal_kernel_variance,
            length=seasonal_kernel_length,
            period=seasonal_kernel_period,
            noise=seasonal_kernel_noise
        )
        trend_intercept =  numpyro.sample(
            "trend_intercept",
            dist.MultivariateNormal(
                loc=jnp.zeros(X.shape[0]),
                covariance_matrix=covariance_matrix_trend
            )
        )
        seasonal_intercept =  numpyro.sample(
            "seasonal_intercept",
            dist.MultivariateNormal(
                loc=jnp.zeros(X.shape[0]),
                covariance_matrix=covariance_matrix_seasonal,
            )
        )

        global_intercept = numpyro.deterministic(
            "global_intercept", 
            trend_intercept + seasonal_intercept,
        )
        days_effect = numpyro.deterministic(
            "days_effect", 
            jnp.dot(days, coefficients_days),
        )

        # Sample y from a MV Normal distribution of mean 0
        # and covariance determined by the kernel function
        numpyro.sample(
            "y",
            dist.Normal(
                global_intercept + days_effect + effect[arm_indexes],
                noise
            ),
            obs=y,
        )

    return get_model
```

# Simulating the data

## Generating the underlying process

```{python}
def generate_underlying(
    time_index, 
    periodic_variance, 
    periodic_length, 
    period, 
    rbf_variance, 
    rbf_length
):
    kernel_function_seasonality = partial(
        periodic_kernel,
        variance=periodic_variance,
        length=periodic_length,
        period=period, 
        noise=0.0001
    )
    kernel_function_slow_variation = partial(
        RBF_kernel,
        variance=rbf_variance,
        length=rbf_length,
        noise=0.0001
    )

    kernel_function_underlying = partial(
        additive_combined_kernel,
        first_kernel=kernel_function_seasonality,
        second_kernel=kernel_function_slow_variation
    )

    generated_covariance, distance, sampled_function_underlying = simulate_kernel(
        X=time_index,
        X_prime=time_index,
        kernel_function=kernel_function_underlying,
        samples=1,
    )
    return generated_covariance, distance, sampled_function_underlying


generated_covariance, distance, sampled_based_function = generate_underlying(
    time_index=TIME_IDX, 
    periodic_variance=0.25, 
    periodic_length=14, 
    period=7, # every seven days we have weekly variations
    rbf_variance=0.125, 
    rbf_length=14,  # slow variation with 21 days decay
)

sampled_based_function = (
    (sampled_based_function - sampled_based_function.min()) / 
    (sampled_based_function.max() - sampled_based_function.min())
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_based_function,
    kernel_name="Simulated Underlying Process"
)
plt.show()
```

## Adding the experimental effects

```{python}
# We assume constant effect as a fraction of standard deviation
# This is somehow unrealistic
models_parameters = {
    "Branch B": {
        "experimental_delta": sampled_based_function.std() * .5, 
        "periodic_length": 6, 
        "period": 3, 
        "rbf_length": 5,
    }, 
    "Branch C": {
        "experimental_delta": sampled_based_function.std() * .75, 
        "periodic_length": 10, 
        "period": 5, 
        "rbf_length": 14,
    }, 
    "Branch D": {
        "experimental_delta": sampled_based_function.std() * 1., 
        "periodic_length": 10, 
        "period": 14, 
        "rbf_length": 28,
    }, 
    "Branch E": {
        "experimental_delta": sampled_based_function.std() * 1.25, 
        "periodic_length": 14, 
        "period": 7, 
        "rbf_length": 3,
    }, 
}

sampled_functions_underlying = [sampled_based_function.flatten()]
for arm in ["Branch B", "Branch C", "Branch D", "Branch E"]:

    kernel_function = partial(
        white_noise_kernel,
        variance=.01,

    )
    _, _, sampled_function_underlying = generate_underlying(
        time_index=TIME_IDX, 
        periodic_variance=0.25, 
        periodic_length=models_parameters[arm]["periodic_length"], 
        period=models_parameters[arm]["period"],
        rbf_variance=0.125, 
        rbf_length=models_parameters[arm]["rbf_length"],
    )
    _, _, sampled_experimental_effect_function = simulate_kernel(
        X=TIME_IDX,
        X_prime=TIME_IDX,
        kernel_function=kernel_function,
        mean=models_parameters[arm]["experimental_delta"],
        samples=1
    )
    
    sampled_functions_underlying.append(
        (
            sampled_function_underlying + sampled_experimental_effect_function
        ).flatten()
    )

sampled_functions_underlying = np.array(sampled_functions_underlying)
```


```{python}
fig, ax = plt.subplots(1, 1, figsize=(8, 4))
for idx, series in enumerate(range(sampled_functions_underlying.shape[0])):

    ax = plot_univariate_series(
        series_data={
            "x": {
                "data": TIME_IDX,
                "label": "x"
            },
            "y": {
                "data":  sampled_functions_underlying[series, :],
                "label": "y"
            },

        },
        ax=ax,
        alpha=0.25 if idx != 0 else 1.
    )
plt.show()
```

## Adding WeekDay Effects

```{python}
weekday_effect = np.array(
    [
        0.,
        0.,
        0.,
        0.,
        0.15,
        -0.1,
        -0.05
    ]
)
weekday_effect = np.hstack([weekday_effect for _ in range(TIME_IDX.shape[0] // 6)])
weekday_effect = weekday_effect[:TIME_IDX.shape[0]]

fig, ax = plt.subplots(1, 1, figsize=(8, 4))
ax = plot_univariate_series(
    series_data={
        "x": {
            "data": TIME_IDX,
            "label": "x"
        },
        "y": {
            "data":  np.ones(shape=(TIME_IDX.shape[0])) * weekday_effect,
            "label": "y"
        },

    },
    ax=ax,
    alpha=0.5
)
plt.show()
```

```{python}
weekday_deltas = (sampled_functions_underlying) * weekday_effect
simulated_series = sampled_functions_underlying + weekday_deltas

fig, ax = plt.subplots(1, 1, figsize=(8, 4))
for idx, series in enumerate(range(simulated_series.shape[0])):

    ax = plot_univariate_series(
        series_data={
            "x": {
                "data": TIME_IDX,
                "label": "x"
            },
            "y": {
                "data":  simulated_series[series, :],
                "label": "y"
            },

        },
        ax=ax,
        alpha=0.25 if idx != 0 else 1.
    )
plt.show()
```

# Fitting Gaussian Process Models to A/B test data

```{python}
df = pd.DataFrame(
    simulated_series.T,
    # the names here indicates the 5 branches of an AB test.
    columns=MODELS
)
df["date"] = DATES
df["day_week"] = df["date"].dt.day_name()
df.head()
```

```{python}
#| code-fold: true
#| code-summary: "Show supplementary code"
def _set_x_axis_grid(ax):
    ax.tick_params(
        direction="in",
        top=True,
        axis="x"
    )
    ax.grid(
        visible=True,
        which="major",
        axis="x",
        color="k",
        alpha=0.25,
        linestyle="--"
    )
    return ax

def plot_arms_time_series(time, arm_a, arm_b, df, ax, y_label, title):
    ax.scatter(
        df[time].values,
        df[arm_a].values,
        marker="x",
        c="blue",
        label=arm_a
    )
    ax.scatter(
        df[time].values,
        df[arm_b].values,
        marker="*",
        c="red",
        label=arm_b
    )
    ax = _set_x_axis_grid(ax=ax)
    ax.set_title(title)
    ax.set_xlabel("Time")
    ax.set_ylabel(y_label)
    return ax
```

```{python}
plt.figure(figsize=(15, 5))
sns.violinplot(
    data=df.melt(
        id_vars=("date"),
        value_vars=MODELS,
        var_name="Arm",
        value_name="Dependent Variable"
    ),
    x="Arm",
    y="Dependent Variable",
    hue="Arm",
)
plt.xlabel("AB Test Arms")
plt.grid(alpha=0.5)
plt.show()
```

```{python}
plt.figure(figsize=(8, 4))
sns.violinplot(
    data=df.melt(
        id_vars=("date", "day_week"),
        value_vars=MODELS,
        var_name="Arm",
        value_name="Dependent Variable"
    ),
    x="day_week",
    y="Dependent Variable",
    order=[
        "Monday",
        "Tuesday",
        "Wednesday",
        "Thursday",
        "Friday",
        "Saturday",
        "Sunday"
    ],
    hue="day_week"
)
plt.xlabel("Day of the Week")
plt.grid(alpha=0.5)
plt.show()
```

```{python}
fig = plt.figure(figsize=(8, 4))
ax = sns.violinplot(
    data=df.melt(
        id_vars=("date", "day_week"),
        value_vars=MODELS,
        var_name="Arm",
        value_name="Dependent Variable"
    ),
    x="day_week",
    y="Dependent Variable",
    hue="Arm",
    order=[
        "Monday",
        "Tuesday",
        "Wednesday",
        "Thursday",
        "Friday",
        "Saturday",
        "Sunday"
    ]
)
sns.move_legend(ax, "upper left", bbox_to_anchor=(1, 1))
plt.xlabel("Day of the Week")
plt.grid(alpha=0.5)
plt.show()
```

```{python}
fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 6), sharex=True, sharey=True)
arms = MODELS.copy()
arms.remove("Branch A")

for arm_b, ax in zip(arms, axs.flatten()):

    ax = plot_arms_time_series(
        time="date",
        arm_a="Branch A",
        arm_b=arm_b,
        df=df,
        ax=ax,
        y_label="Dependent Variable",
        title="AB Test"
    )
    ax.legend()

plt.tight_layout()
plt.show()
```

```{python}
arms = MODELS.copy()
arms.remove("Branch A")

fig, axs = plt.subplots(
    nrows=2,
    ncols=2,
    figsize=(12, 6),
    sharex=True,
    sharey=True
)
for arm_b, ax in zip(arms, axs.flatten()):

    delta = df[arm_b] - df["Branch A"]
    time = df[TIME]

    semed = median_abs_deviation(delta) / np.sqrt(len(time))

    ax.scatter(
        time,
        delta,
        marker="o",
        facecolors='none',
        edgecolors='r',
        linestyle=":"
    )
    ax.set_title(arm_b)
    ax.plot(
        time,
        [np.median(delta)] * len(time),
        c='r',
    )
    ax.fill_between(
        time,
        [np.median(delta) - (1.96 * semed)] * len(time),
        [np.median(delta) + (1.96 * semed)] * len(time),
        color='r',
        alpha=0.1
    )
    ax.axhline(
        0,
        linestyle=":",
        c="k"
    )
    ax.set_xlabel("Date")
    ax.set_ylabel("$ \Delta $\nDependent Variable")

plt.tight_layout()
plt.show()
```

# Visualize the results

# Conclusion

# Hardware and Requirements
Here you can find the hardware and python requirements used for building this post.

```{python}
%watermark
```

```{python}
%watermark --iversions
```