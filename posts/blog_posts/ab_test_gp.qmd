---
title: Analyzing A/B Test Data using Gaussian Processes
description: 'This post illustrates how to analyze longitudinal A/B test data using Gaussian Process.'
date: '2025-03-29'
categories:
  - JAX
  - Numpyro
  - Bayesian Statistics
  - A/B Test
  - Gaussian Process
bibliography: bibliographies/ab_test_gp.bib
jupyter: python3
---

```{python}
#| code-fold: true
#| code-summary: Show supplementary code
#| vscode: {languageId: python}

%load_ext watermark

NUM_CHAINS = 4

from typing import Dict, Any, Callable, Tuple

from functools import partial

import pandas as pd
import numpy as np

import scipy
from scipy.stats import lognorm, median_abs_deviation

import jax
from jax.typing import ArrayLike
import jax.random as random
from jax import jit
from jax import numpy as jnp
from jax import vmap

import numpyro
numpyro.set_host_device_count(NUM_CHAINS)
import numpyro.distributions as dist
from numpyro.infer import (
    MCMC,
    NUTS,
    Predictive
)

import matplotlib.pyplot as plt
from matplotlib.figure import Figure
from matplotlib import gridspec

import seaborn as sns

def plot_univariate_series(
        series_data: Dict[Any, Any], 
        ax: plt.Axes, 
        **plot_kwargs: Any
    ) -> plt.Axes:
    ax.scatter(
        series_data["x"]["data"],
        series_data["y"]["data"],
        **plot_kwargs
    )
    ax.plot(
        series_data["x"]["data"],
        series_data["y"]["data"],
        **plot_kwargs
    )
    ax.set_xlabel(series_data["x"]["label"])
    ax.set_ylabel(series_data["y"]["label"])

    ax.tick_params(
        direction="in",
        top=True, axis="x",
        rotation=45,
    )
    ax.grid(
        visible=True,
        which="major",
        axis="x",
        color="k",
        alpha=0.25,
        linestyle="--",
    )
    return ax
```

# Premise

## What we will cover

## What we will **not** cover

# Introduction

```{python}
BLUE = (0, 83, 159)
BLUE = tuple(value / 255. for value in BLUE)

RED = (238, 28, 46)
RED = tuple(value / 255. for value in RED)

TIME_IDX = np.arange(7*6)
DATES = pd.date_range(
    start="01-01-2023",
    periods=len(TIME_IDX)
).values
TIME = "date"
MODELS = [
    "Branch A",
    "Branch B",
    "Branch C",
    "Branch D",
    "Branch E",
]
```

## A/B tests and longitudinal A/B tests

## Gaussian Process

## Gaussian Process for A/B test

# Implementing a Gaussian Process Model in Numpyro

## Kernel Functions

```{python}
#| code-fold: true
#| code-summary: Show supplementary code
#| vscode: {languageId: python}

def simulate_kernel(
    X: ArrayLike, 
    X_prime:ArrayLike, 
    kernel_function: Callable, 
    samples: int,
    mean: float = 0.
) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:
    generated_covariance = kernel_function(
        X=X,
        X_prime=X_prime,
    )
    distance = generated_covariance[:, len(X) // 2]
    sampled_functions = np.random.multivariate_normal(
        mean=np.zeros(
            shape=generated_covariance.shape[0]
        ) + mean,
        cov=generated_covariance,
        size=samples
    )
    return generated_covariance, distance, sampled_functions

def visualize_kernel(
    X: ArrayLike, 
    generated_covariance: ArrayLike, 
    distance: ArrayLike, 
    sampled_functions: ArrayLike, 
    kernel_name: str
) -> Figure:

    fig = plt.figure(
        figsize=(8, 8),
        tight_layout=True
    )
    grid = gridspec.GridSpec(
        nrows=2,
        ncols=2
    )
    ax_functions = fig.add_subplot(grid[0, :])
    ax_distance = fig.add_subplot(grid[1, 0])
    ax_covariance = fig.add_subplot(grid[1, 1])

    for index in range(sampled_functions.shape[0]):

        ax_functions = plot_univariate_series(
            series_data={
                "x": {
                    "data": X,
                    "label": "x"
                },
                "y": {
                    "data": sampled_functions[index, :],
                    "label": "y"
                },

            },
            ax=ax_functions,
            alpha=0.5
        )

    ax_functions.set_title("Sampled Functions \n from MvNormal")
    ax_functions.axhline(0, linestyle="--", c="k")

    ax_distance.plot(
        X - (len(X) // 2),
        distance
    )
    ax_distance.grid(alpha=0.5)
    ax_distance.set_title(f"Distance Function \n Determined by {kernel_name} Kernel")
    ax_covariance.set_ylabel("Similarity")
    ax_covariance.set_xlabel("Distance")

    ax_covariance.imshow(
        generated_covariance
    )
    ax_covariance.set_ylabel("x'")
    ax_covariance.set_xlabel("x")
    ax_covariance.set_title(f"Covariance \n Determined by {kernel_name} Kernel")


    plt.suptitle(f"{kernel_name} Kernel")
    return fig
```

### Radial Basis Function Kernel

```{python}
@jit
def RBF_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    variance: float, 
    length: float, 
    noise: float,
    jitter: float =1.0e-6, 
    include_noise: bool =True
) -> ArrayLike:
    squared_differences = jnp.power(
        (X[:, None] - X_prime),
        2.0
    )
    squared_length_scale = 2 * jnp.power(
        length,
        2.0
    )
    covariance_matrix = jnp.exp(
        - (squared_differences / squared_length_scale)
    )
    scaled_covariance_matrix = variance * covariance_matrix

    if include_noise:
        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])

    return scaled_covariance_matrix
```


```{python}
kernel_function = partial(
    RBF_kernel,
    variance=1,
    length=10,
    noise=0.001
)
generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="RBF"
)
plt.show()
```

### Matern Kernel

```{python}
@jit
def rational_quadratic_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    variance: float, 
    length: float, 
    noise: float, 
    alpha: float, 
    jitter: float =1.0e-6, 
    include_noise: bool =True
) -> ArrayLike:

    squared_differences = jnp.power(
        (X[:, None] - X_prime),
        2.0
    )
    squared_length_scale = 2 * alpha * jnp.power(
        length,
        2.0
    )
    covariance_matrix = jnp.power(
        1 + (squared_differences / squared_length_scale),
        - alpha
    )
    scaled_covariance_matrix = variance * covariance_matrix

    if include_noise:
        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])

    return scaled_covariance_matrix
```

```{python}
kernel_function = partial(
    rational_quadratic_kernel,
    variance=1,
    length=10,
    alpha=3,
    noise=0.001
)
generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3

)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="Rational Quadratic"
)
plt.show()
```

### Periodic Kernel

```{python}
@jit
def periodic_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    variance: float, 
    length: float, 
    noise: float, 
    period: float, 
    jitter: float = 1.0e-6, 
    include_noise: bool =True
) -> ArrayLike:

    periodic_difference = jnp.pi * jnp.abs(X[:, None] - X_prime) / period
    sine_squared_difference = 2 * jnp.power(
        jnp.sin(periodic_difference),
        2.0
    )
    squared_length_scale = jnp.power(
        length,
        2.0
    )
    covariance_matrix = jnp.exp(
        - (sine_squared_difference / squared_length_scale)
    )
    scaled_covariance_matrix = variance * covariance_matrix

    if include_noise:
        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])

    return scaled_covariance_matrix
```

```{python}
kernel_function = partial(
    periodic_kernel,
    variance=1,
    length=5,
    period=10,
    noise=0.001
)

generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="Periodic"
)
plt.show()
```

### Linear Kernel

```{python}
@jit
def linear_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    variance: float, 
    noise: float, 
    jitter: float = 1.0e-6, 
    include_noise: bool = True
) -> ArrayLike:
    scaled_covariance_matrix = variance + (X[:, None] * X_prime)
    if include_noise:
        scaled_covariance_matrix += (noise + jitter) * jnp.eye(X.shape[0])

    return scaled_covariance_matrix
```

```{python}
kernel_function = partial(
    linear_kernel,
    variance=0.01,
    noise=0.001
)
generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="Linear"
)
plt.show()
```

### White Noise Kernel

```{python}
@jit
def white_noise_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    variance: float
) -> ArrayLike:
    covariance_matrix = variance * jnp.eye(X.shape[0])
    return covariance_matrix
```

```{python}
kernel_function = partial(
    white_noise_kernel,
    variance=0.01,
)
generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="White Noise"
)
plt.show()
```

### Combining Kernels

```{python}
@partial(jit, static_argnums=(2,3))
def additive_combined_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    first_kernel: Callable, 
    second_kernel: Callable
) -> ArrayLike:
    return first_kernel(X=X, X_prime=X_prime) + second_kernel(X=X, X_prime=X_prime)

@partial(jit, static_argnums=(2,3))
def multiplicative_combined_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    first_kernel: Callable, 
    second_kernel: Callable
) -> ArrayLike:
    return first_kernel(X=X, X_prime=X_prime) * second_kernel(X=X, X_prime=X_prime)
```

```{python}
kernel_function_linear = partial(
    linear_kernel,
    variance=0.0001,
    noise=0.000
)
kernel_function_rbf = partial(
    RBF_kernel,
    variance=30,
    length=2,
    noise=10
)

kernel_function = partial(
    additive_combined_kernel,
    first_kernel=kernel_function_linear,
    second_kernel=kernel_function_rbf
)

generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="Additive Linear+RBF"
)
plt.show()
```

```{python}
kernel_function_periodic = partial(
    periodic_kernel,
    variance=5,
    length=5,
    period=10,
    noise=0.0001
)
kernel_function_rbf = partial(
    RBF_kernel,
    variance=.1,
    length=50,
    noise=0.0001
)

kernel_function = partial(
    multiplicative_combined_kernel,
    first_kernel=kernel_function_periodic,
    second_kernel=kernel_function_rbf
)

generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="Multiplicative Periodic RBF"
)
plt.show()
```

## The Gaussian Process Model

```{python}
def RBF_gaussian_process(
        trend_kernel_variance_dist,
        trend_kernel_noise_dist,
        trend_kernel_length_dist,

        seasonal_kernel_variance_dist,
        seasonal_kernel_noise_dist,
        seasonal_kernel_length_dist,
        seasonal_kernel_period_dist,


        day_dist,
        noise_dist
    ):

    def get_model(X, y, days):
        # The parameters of the kernel function are sampled from their
        # associated prior distributions
        trend_kernel_variance = numpyro.sample(
            "trend_kernel_variance",
            trend_kernel_variance_dist
        )
        trend_kernel_noise = numpyro.sample(
            "trend_kernel_noise",
            trend_kernel_noise_dist
        )
        trend_kernel_length = numpyro.sample(
            "trend_kernel_length",
            trend_kernel_length_dist
        )

        seasonal_kernel_variance = numpyro.sample(
            "seasonal_kernel_variance",
            seasonal_kernel_variance_dist
        )
        seasonal_kernel_noise = numpyro.sample(
            "seasonal_kernel_noise",
            seasonal_kernel_noise_dist
        )
        seasonal_kernel_length = numpyro.sample(
            "seasonal_kernel_length",
            seasonal_kernel_length_dist
        )
        seasonal_kernel_period = numpyro.sample(
            "seasonal_kernel_length_periodic",
            seasonal_kernel_period_dist
        )

        coefficients_days = numpyro.sample(
            "coefficients_days",
            day_dist
        )
        noise = numpyro.sample(
            "noise",
            noise_dist
        )

        # Compute the covariance matrix using the RBF kernel
        covariance_matrix_trend = RBF_kernel(
            X=X,
            X_prime=X,
            variance=trend_kernel_variance,
            length=trend_kernel_length,
            noise=trend_kernel_noise
        )
        covariance_matrix_seasonal = periodic_kernel(
            X=X,
            X_prime=X,
            variance=seasonal_kernel_variance,
            length=seasonal_kernel_length,
            period=seasonal_kernel_period,
            noise=seasonal_kernel_noise
        )
        trend_intercept =  numpyro.sample(
            "trend_intercept",
            dist.MultivariateNormal(
                loc=jnp.zeros(X.shape[0]),
                covariance_matrix=covariance_matrix_trend
            )
        )
        seasonal_intercept =  numpyro.sample(
            "seasonal_intercept",
            dist.MultivariateNormal(
                loc=jnp.zeros(X.shape[0]),
                covariance_matrix=covariance_matrix_seasonal
            )
        )

        intercept = trend_intercept + seasonal_intercept


        # Sample y from a MV Normal distribution of mean 0
        # and covariance determined by the kernel function
        numpyro.sample(
            "y",
            dist.Normal(
                intercept + jnp.dot(days, coefficients_days),
                noise
            ),
            obs=y,
        )

    return get_model
```

# Simulating the data

# Fitting Gaussian Process Models to A/B test data

# Visualize the results

# Conclusion

# Hardware and Requirements
Here you can find the hardware and python requirements used for building this post.

```{python}
%watermark
```

```{python}
%watermark --iversions
```