---
title: Analyzing A/B Test Data using Gaussian Processes
description: This post illustrates how to analyze longitudinal A/B test data using Gaussian Process.
date: '2025-03-29'
categories:
  - JAX
  - Numpyro
  - Bayesian Statistics
  - A/B Test
  - Gaussian Process
bibliography: bibliographies/ab_test_gp.bib
jupyter: python3
---

```{python}
#| code-fold: true
#| code-summary: Show supplementary code

%load_ext watermark

NUM_CHAINS = 4

from typing import Dict, Any, Callable, Tuple

from functools import partial

import pandas as pd
import numpy as np

import scipy
from scipy.stats import lognorm, median_abs_deviation

import jax
from jax.typing import ArrayLike
import jax.random as random
from jax import jit
from jax import numpy as jnp
from jax import vmap

import numpyro
numpyro.set_host_device_count(NUM_CHAINS)
import numpyro.distributions as dist
from numpyro.infer import (
    MCMC,
    NUTS,
    Predictive
)

import matplotlib.pyplot as plt
from matplotlib.figure import Figure
from matplotlib import gridspec

import seaborn as sns

def plot_univariate_series(
        series_data: Dict[Any, Any], 
        ax: plt.Axes, 
        **plot_kwargs: Any
    ) -> plt.Axes:
    ax.scatter(
        series_data["x"]["data"],
        series_data["y"]["data"],
        **plot_kwargs
    )
    ax.plot(
        series_data["x"]["data"],
        series_data["y"]["data"],
        **plot_kwargs
    )
    ax.set_xlabel(series_data["x"]["label"])
    ax.set_ylabel(series_data["y"]["label"])

    ax.tick_params(
        direction="in",
        top=True, axis="x",
        rotation=45,
    )
    ax.grid(
        visible=True,
        which="major",
        axis="x",
        color="k",
        alpha=0.25,
        linestyle="--",
    )
    return ax
```

# Premise

What will be illustrated in this post is strongly in spired by the content of the books "Statistical Rethinking" [@mcelreath2018statistical] and "Bayesian Data Analysis 3rd Edition" [@gelman1995bayesian]. In particular, the idea to separately code and illustrate the behavior of different covariance kernel functions comes from the amazing ["Kernel Cookbook"](https://www.cs.toronto.edu/~duvenaud/cookbook/) and PhD thesis of David Duvenaud [@duvenaud2014automatic].

This post assumes some level of knowledge in bayesian statistics and probabilistic programming.

## What we will cover

1. Very brief illustration of longitudinal A/B test within and observational paradigm.
2. Very brief illustration of gaussian processes and their application to analyzing A/B test data.
3. Overview of how to implement a gaussian process model using [Numpyro](https://num.pyro.ai/en/stable/index.html) and [JAX](https://docs.jax.dev/en/latest/index.html).
4. Simulating A/B test data.
5. Analyzing A/B test data within an modelling setting.

## What we will **not** cover

1. Detailed coverage of A/B test (e.g., sampling, randomization etc...).
2. Hypothesis testing (we will focus on modelling).
3. Fundamentals of bayesian statistics.
4. Detailed overview of Gaussian processes.
5. Probabilistic programming and sampling algorithms.

# Introduction

## Longitudinal A/B tests in observational settings

When we talk about A/B test we usually refer to a research method used for evaluating if a given intervention is having an impact on a pre-defined outcome variable measured inside a sample. For doing so, we can draw two distinct samples (the A and B group) from a population of interest, subject one of the two to the intervention and then measure observed differences in the outcome variable. Subject to **several** assumptions and pre-conditions (we suggest reading part V of "Regression and Other Stories" [@gelman2021regression]), if we observe a difference between the two groups when can conclude that our intervention might have had an impact on our outcome variable.

## Gaussian Process

The Gaussian Process $GP$ can be thought as the continuous generalization of basis function regression (see Chapter 20 of [@gelman1995bayesian]). We can think of it as a stochastic process where any point drawn from it, $x_1, \dots, x_n$, comes from a multi-dimensional gaussian. In other words, it is as a prior distribution over an un-known function $\mu(x)$ defined as

$$
\mu(x_1), \dots, \mu(x_n) \sim \mathcal{N}((m(x_1), \dots, m(x_n)), k(x, \dots, x_n))
$$

or more compactly

$$
\mu(x) \sim GP(m, k)
$$

where $m$ is a mean function and $k$ is a covariance function. We can already have an intuition of how defining the $GP$ in terms mean and covariance **functions** gives us quite some flexibility as it allows us to produce a model than can interpolate for all the value of $x$.

The $m$ function provides the most likely guess for the $GP$ like the mean vector of a multi-dimensional Gaussian, deviation from this expected model are then handled by the covariance function $k$.

The $k$ function (often called Kernel) allows to structurally define the $GP$ behavior at any two points by producing an $n \times n$ covariance function given by evaluating $k(x, x')$ for every $x_1, \dots, x_n$.

One convenient property of $GP$ is that the sum and multiplication of two or more $GP$ is itself a $GP$, this allows to combine different types of Kernels for imposing specific structural constrains.

## Gaussian Process for A/B test

Although there aren't many papers illustrating how $GP$ can be used for analyzing A/B test data we found this interesting work by [@benavoli2015gaussian] from [IDSIA](https://www.idsia.usi-supsi.ch/) that we decided to adapt to our use-case.



# Implementing a Gaussian Process Model in Numpyro

In this section we will illustrate how we can implement a $GP$ model using Numpyro. Numpyro offers a Numpy-like Backend for [Pyro](https://pyro.ai/) a Probabilistic Programming Language (PPL). Other than offering the flexibility of specifying models using the familiar Numpy interface, Numpyro is perfectly integrated with JAX allowing us to tap into its JIT compilation capabilities.

```{python}
BLUE = (0, 83, 159)
BLUE = tuple(value / 255. for value in BLUE)

RED = (238, 28, 46)
RED = tuple(value / 255. for value in RED)

TIME_IDX = np.arange(7*4)
DATES = pd.date_range(
    start="01-01-2023",
    periods=len(TIME_IDX)
).values
TIME = "date"
MODELS = [
    "Branch A",
    "Branch B",
    "Branch C",
    "Branch D",
    "Branch E",
]
```

## Kernel Functions

```{python}
#| code-fold: true
#| code-summary: Show supplementary code

def simulate_kernel(
    X: ArrayLike, 
    X_prime:ArrayLike, 
    kernel_function: Callable, 
    samples: int,
    mean: float = 0.
) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:
    generated_covariance = kernel_function(
        X=X,
        X_prime=X_prime,
    )
    distance = generated_covariance[:, len(X) // 2]
    sampled_functions = np.random.multivariate_normal(
        mean=np.zeros(
            shape=generated_covariance.shape[0]
        ) + mean,
        cov=generated_covariance,
        size=samples
    )
    return generated_covariance, distance, sampled_functions

def visualize_kernel(
    X: ArrayLike, 
    generated_covariance: ArrayLike, 
    distance: ArrayLike, 
    sampled_functions: ArrayLike, 
    kernel_name: str
) -> Figure:

    fig = plt.figure(
        figsize=(8, 8),
        tight_layout=True
    )
    grid = gridspec.GridSpec(
        nrows=2,
        ncols=2
    )
    ax_functions = fig.add_subplot(grid[0, :])
    ax_distance = fig.add_subplot(grid[1, 0])
    ax_covariance = fig.add_subplot(grid[1, 1])

    for index in range(sampled_functions.shape[0]):

        ax_functions = plot_univariate_series(
            series_data={
                "x": {
                    "data": X,
                    "label": "x"
                },
                "y": {
                    "data": sampled_functions[index, :],
                    "label": "y"
                },

            },
            ax=ax_functions,
            alpha=0.5
        )

    ax_functions.set_title("Sampled Functions \n from MvNormal")
    ax_functions.axhline(0, linestyle="--", c="k")

    ax_distance.plot(
        X - (len(X) // 2),
        distance
    )
    ax_distance.grid(alpha=0.5)
    ax_distance.set_title(f"Distance Function \n Determined by {kernel_name} Kernel")
    ax_covariance.set_ylabel("Similarity")
    ax_covariance.set_xlabel("Distance")

    ax_covariance.imshow(
        generated_covariance
    )
    ax_covariance.set_ylabel("x'")
    ax_covariance.set_xlabel("x")
    ax_covariance.set_title(f"Covariance \n Determined by {kernel_name} Kernel")


    plt.suptitle(f"{kernel_name} Kernel")
    return fig
```

### Radial Basis Function Kernel

```{python}
@jit
def RBF_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    variance: float, 
    length: float, 
    noise: float,
    jitter: float =1.0e-6, 
    include_noise: bool =True
) -> ArrayLike:
    squared_differences = jnp.power(
        (X[:, None] - X_prime),
        2.0
    )
    squared_length_scale = 2 * jnp.power(
        length,
        2.0
    )
    covariance_matrix = jnp.exp(
        - (squared_differences / squared_length_scale)
    )
    scaled_covariance_matrix = variance * covariance_matrix

    scaled_covariance_matrix = jnp.where(
        include_noise, 
        scaled_covariance_matrix + (noise + jitter) * jnp.eye(X.shape[0]), 
        scaled_covariance_matrix,
    )

    return scaled_covariance_matrix
```

```{python}
kernel_function = partial(
    RBF_kernel,
    variance=1,
    length=10,
    noise=0.001
)
generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="RBF"
)
plt.show()
```

### Matern Kernel

```{python}
@jit
def rational_quadratic_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    variance: float, 
    length: float, 
    noise: float, 
    alpha: float, 
    jitter: float =1.0e-6, 
    include_noise: bool =True
) -> ArrayLike:

    squared_differences = jnp.power(
        (X[:, None] - X_prime),
        2.0
    )
    squared_length_scale = 2 * alpha * jnp.power(
        length,
        2.0
    )
    covariance_matrix = jnp.power(
        1 + (squared_differences / squared_length_scale),
        - alpha
    )
    scaled_covariance_matrix = variance * covariance_matrix

    scaled_covariance_matrix = jnp.where(
        include_noise, 
        scaled_covariance_matrix + (noise + jitter) * jnp.eye(X.shape[0]), 
        scaled_covariance_matrix,
    )

    return scaled_covariance_matrix
```

```{python}
kernel_function = partial(
    rational_quadratic_kernel,
    variance=1,
    length=10,
    alpha=3,
    noise=0.001
)
generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3

)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="Rational Quadratic"
)
plt.show()
```

### Periodic Kernel

```{python}
@jit
def periodic_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    variance: float, 
    length: float, 
    noise: float, 
    period: float, 
    jitter: float = 1.0e-6, 
    include_noise: bool =True
) -> ArrayLike:

    periodic_difference = jnp.pi * jnp.abs(X[:, None] - X_prime) / period
    sine_squared_difference = 2 * jnp.power(
        jnp.sin(periodic_difference),
        2.0
    )
    squared_length_scale = jnp.power(
        length,
        2.0
    )
    covariance_matrix = jnp.exp(
        - (sine_squared_difference / squared_length_scale)
    )
    scaled_covariance_matrix = variance * covariance_matrix

    scaled_covariance_matrix = jnp.where(
        include_noise, 
        scaled_covariance_matrix + (noise + jitter) * jnp.eye(X.shape[0]), 
        scaled_covariance_matrix,
    )
    return scaled_covariance_matrix
```

```{python}
kernel_function = partial(
    periodic_kernel,
    variance=1,
    length=5,
    period=10,
    noise=0.001
)

generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="Periodic"
)
plt.show()
```

### Linear Kernel

```{python}
@jit
def linear_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    variance: float, 
    noise: float, 
    jitter: float = 1.0e-6, 
    include_noise: bool = True
) -> ArrayLike:
    scaled_covariance_matrix = variance + (X[:, None] * X_prime)
    scaled_covariance_matrix = jnp.where(
        include_noise, 
        scaled_covariance_matrix + (noise + jitter) * jnp.eye(X.shape[0]), 
        scaled_covariance_matrix,
    )

    return scaled_covariance_matrix
```

```{python}
kernel_function = partial(
    linear_kernel,
    variance=0.01,
    noise=0.001
)
generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="Linear"
)
plt.show()
```

### White Noise Kernel

```{python}
@jit
def white_noise_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    variance: float
) -> ArrayLike:
    covariance_matrix = variance * jnp.eye(X.shape[0])
    return covariance_matrix
```

```{python}
kernel_function = partial(
    white_noise_kernel,
    variance=0.01,
)
generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="White Noise"
)
plt.show()
```

### Combining Kernels

```{python}
@partial(jit, static_argnums=(2,3))
def additive_combined_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    first_kernel: Callable, 
    second_kernel: Callable
) -> ArrayLike:
    return first_kernel(X=X, X_prime=X_prime) + second_kernel(X=X, X_prime=X_prime)

@partial(jit, static_argnums=(2,3))
def multiplicative_combined_kernel(
    X: ArrayLike, 
    X_prime: ArrayLike, 
    first_kernel: Callable, 
    second_kernel: Callable
) -> ArrayLike:
    return first_kernel(X=X, X_prime=X_prime) * second_kernel(X=X, X_prime=X_prime)
```

```{python}
kernel_function_linear = partial(
    linear_kernel,
    variance=0.0001,
    noise=0.000
)
kernel_function_rbf = partial(
    RBF_kernel,
    variance=30,
    length=2,
    noise=10
)

kernel_function = partial(
    additive_combined_kernel,
    first_kernel=kernel_function_linear,
    second_kernel=kernel_function_rbf
)

generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="Additive Linear+RBF"
)
plt.show()
```

```{python}
kernel_function_periodic = partial(
    periodic_kernel,
    variance=5,
    length=5,
    period=10,
    noise=0.0001
)
kernel_function_rbf = partial(
    RBF_kernel,
    variance=.1,
    length=50,
    noise=0.0001
)

kernel_function = partial(
    multiplicative_combined_kernel,
    first_kernel=kernel_function_periodic,
    second_kernel=kernel_function_rbf
)

generated_covariance, distance, sampled_functions = simulate_kernel(
    X=TIME_IDX,
    X_prime=TIME_IDX,
    kernel_function=kernel_function,
    samples=3,
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_functions,
    kernel_name="Multiplicative Periodic RBF"
)
plt.show()
```

# Simulating the data

## Generating the underlying process

```{python}
1 / 14 
```

```{python}
def generate_underlying(
    time_index, 
    periodic_variance, 
    periodic_length, 
    period, 
    rbf_variance, 
    rbf_length
):
    kernel_function_seasonality = partial(
        periodic_kernel,
        variance=periodic_variance,
        length=periodic_length,
        period=period, 
        noise=0.0001
    )
    kernel_function_slow_variation = partial(
        RBF_kernel,
        variance=rbf_variance,
        length=rbf_length,
        noise=0.0001
    )

    kernel_function_underlying = partial(
        additive_combined_kernel,
        first_kernel=kernel_function_seasonality,
        second_kernel=kernel_function_slow_variation
    )

    generated_covariance, distance, sampled_function_underlying = simulate_kernel(
        X=time_index,
        X_prime=time_index,
        kernel_function=kernel_function_underlying,
        samples=1,
    )
    return generated_covariance, distance, sampled_function_underlying


generated_covariance, distance, sampled_based_function = generate_underlying(
    time_index=TIME_IDX, 
    periodic_variance=0.25, 
    periodic_length=7, 
    period=7, # every seven days we have weekly variations
    rbf_variance=0.1, 
    rbf_length=14,  # slow variation with 21 days decay
)

sampled_based_function = (
    (sampled_based_function - sampled_based_function.mean()) / sampled_based_function.std()
)
fig = visualize_kernel(
    TIME_IDX,
    generated_covariance=generated_covariance,
    distance=distance,
    sampled_functions=sampled_based_function,
    kernel_name="Simulated Underlying Process"
)
plt.show()
```

## Adding the experimental effects

```{python}
# We assume constant effect as a fraction of standard deviation
# This is somehow unrealistic

@partial(jit, static_argnums=2)
def exponential_decay_function(initial_value, rate_decay, max_t):
    decayed_function = jnp.repeat(initial_value, max_t)
    return decayed_function * jnp.power(1 - rate_decay, jnp.arange(max_t))

models_parameters = {
    "Branch A": {
        "experimental_delta": 0, 
    }, 
    "Branch B": {
        "experimental_delta": sampled_based_function.std() * 1., 
    }, 
    "Branch C": {
        "experimental_delta": sampled_based_function.std() * 1.25, 
    }, 
    "Branch D": {
        "experimental_delta": sampled_based_function.std() * 1.5,
    }, 
    "Branch E": {
        "experimental_delta": sampled_based_function.std() * 1.75, 
    }, 
}

sampled_functions_underlying = []
sampled_functions_effect = []
for arm in ["Branch A", "Branch B", "Branch C", "Branch D", "Branch E"]:

    kernel_function = partial(
        white_noise_kernel,
        variance=.1,
    )
    exponential_decay_mean = exponential_decay_function(
        initial_value=models_parameters[arm]["experimental_delta"],
        rate_decay=0.05,
        max_t=sampled_based_function.shape[1],
    )
    
    _, _, sampled_experimental_effect_function = simulate_kernel(
        X=TIME_IDX,
        X_prime=TIME_IDX,
        kernel_function=kernel_function,
        mean=exponential_decay_mean,
        samples=1
    )
    
    sampled_functions_effect.append(
         sampled_experimental_effect_function.flatten()
    )
    sampled_functions_underlying.append(
        (
            sampled_based_function + sampled_experimental_effect_function
        ).flatten()
    )

sampled_functions_effect = np.array(sampled_functions_effect)
sampled_functions_underlying = np.array(sampled_functions_underlying)
```

```{python}
fig, ax = plt.subplots(1, 1, figsize=(8, 4))
for idx, series in enumerate(range(sampled_functions_underlying.shape[0])):

    ax = plot_univariate_series(
        series_data={
            "x": {
                "data": TIME_IDX,
                "label": "x"
            },
            "y": {
                "data":  sampled_functions_underlying[series, :],
                "label": "y"
            },

        },
        ax=ax,
        alpha=0.25 if idx != 0 else 1.
    )
plt.show()
```

```{python}
fig, ax = plt.subplots(1, 1, figsize=(8, 4))
for idx, series in enumerate(range(sampled_functions_effect.shape[0])):

    ax = plot_univariate_series(
        series_data={
            "x": {
                "data": TIME_IDX,
                "label": "x"
            },
            "y": {
                "data":  sampled_functions_effect[series, :],
                "label": "y"
            },

        },
        ax=ax,
        alpha=0.25 if idx != 0 else 1.
    )
plt.axhline(0, linestyle=":", c="k")
plt.show()
```

## Adding WeekDay Effects

```{python}
weekday_effect = np.array(
    [
        0.,
        0.,
        0.,
        0.,
        0.15,
        -0.1,
        -0.05
    ]
)
weekday_effect = np.hstack([weekday_effect for _ in range(TIME_IDX.shape[0] // 6)])
weekday_effect = weekday_effect[:TIME_IDX.shape[0]]

fig, ax = plt.subplots(1, 1, figsize=(8, 4))
ax = plot_univariate_series(
    series_data={
        "x": {
            "data": TIME_IDX,
            "label": "x"
        },
        "y": {
            "data":  np.ones(shape=(TIME_IDX.shape[0])) * weekday_effect,
            "label": "y"
        },

    },
    ax=ax,
    alpha=0.5
)
plt.show()
```

```{python}
weekday_deltas = (sampled_functions_underlying) * weekday_effect
simulated_series = sampled_functions_underlying + weekday_deltas

fig, ax = plt.subplots(1, 1, figsize=(8, 4))
for idx, series in enumerate(range(simulated_series.shape[0])):

    ax = plot_univariate_series(
        series_data={
            "x": {
                "data": TIME_IDX,
                "label": "x"
            },
            "y": {
                "data":  simulated_series[series, :],
                "label": "y"
            },

        },
        ax=ax,
        alpha=0.25 if idx != 0 else 1.
    )
plt.show()
```

# Fitting Gaussian Process Models to A/B test data

```{python}
df = pd.DataFrame(
    simulated_series.T,
    # the names here indicates the 5 branches of an AB test.
    columns=MODELS
)
df["date"] = DATES
df["day_week"] = df["date"].dt.day_name()
df.head()
```

```{python}
#| code-fold: true
#| code-summary: Show supplementary code
def _set_x_axis_grid(ax):
    ax.tick_params(
        direction="in",
        top=True,
        axis="x"
    )
    ax.grid(
        visible=True,
        which="major",
        axis="x",
        color="k",
        alpha=0.25,
        linestyle="--"
    )
    return ax

def plot_arms_time_series(time, arm_a, arm_b, df, ax, y_label, title):
    ax.scatter(
        df[time].values,
        df[arm_a].values,
        marker="x",
        c="blue",
        label=arm_a
    )
    ax.scatter(
        df[time].values,
        df[arm_b].values,
        marker="*",
        c="red",
        label=arm_b
    )
    ax = _set_x_axis_grid(ax=ax)
    ax.set_title(title)
    ax.set_xlabel("Time")
    ax.set_ylabel(y_label)
    return ax
```

```{python}
plt.figure(figsize=(15, 5))
sns.violinplot(
    data=df.melt(
        id_vars=("date"),
        value_vars=MODELS,
        var_name="Arm",
        value_name="Dependent Variable"
    ),
    x="Arm",
    y="Dependent Variable",
    hue="Arm",
)
plt.xlabel("AB Test Arms")
plt.grid(alpha=0.5)
plt.show()
```

```{python}
plt.figure(figsize=(8, 4))
sns.violinplot(
    data=df.melt(
        id_vars=("date", "day_week"),
        value_vars=MODELS,
        var_name="Arm",
        value_name="Dependent Variable"
    ),
    x="day_week",
    y="Dependent Variable",
    order=[
        "Monday",
        "Tuesday",
        "Wednesday",
        "Thursday",
        "Friday",
        "Saturday",
        "Sunday"
    ],
    hue="day_week"
)
plt.xlabel("Day of the Week")
plt.grid(alpha=0.5)
plt.show()
```

```{python}
fig = plt.figure(figsize=(8, 4))
ax = sns.violinplot(
    data=df.melt(
        id_vars=("date", "day_week"),
        value_vars=MODELS,
        var_name="Arm",
        value_name="Dependent Variable"
    ),
    x="day_week",
    y="Dependent Variable",
    hue="Arm",
    order=[
        "Monday",
        "Tuesday",
        "Wednesday",
        "Thursday",
        "Friday",
        "Saturday",
        "Sunday"
    ]
)
sns.move_legend(ax, "upper left", bbox_to_anchor=(1, 1))
plt.xlabel("Day of the Week")
plt.grid(alpha=0.5)
plt.show()
```

```{python}
fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 6), sharex=True, sharey=True)
arms = MODELS.copy()
arms.remove("Branch A")

for arm_b, ax in zip(arms, axs.flatten()):

    ax = plot_arms_time_series(
        time="date",
        arm_a="Branch A",
        arm_b=arm_b,
        df=df,
        ax=ax,
        y_label="Dependent Variable",
        title="AB Test"
    )
    ax.legend()

plt.tight_layout()
plt.show()
```

```{python}
arms = MODELS.copy()
arms.remove("Branch A")

fig, axs = plt.subplots(
    nrows=2,
    ncols=2,
    figsize=(12, 6),
    sharex=True,
    sharey=True
)
for arm_b, ax in zip(arms, axs.flatten()):

    delta = df[arm_b] - df["Branch A"]
    time = df[TIME]

    semed = median_abs_deviation(delta) / np.sqrt(len(time))

    ax.scatter(
        time,
        delta,
        marker="o",
        facecolors='none',
        edgecolors='r',
        linestyle=":"
    )
    ax.set_title(arm_b)
    ax.plot(
        time,
        [np.median(delta)] * len(time),
        c='r',
    )
    ax.fill_between(
        time,
        [np.median(delta) - (1.96 * semed)] * len(time),
        [np.median(delta) + (1.96 * semed)] * len(time),
        color='r',
        alpha=0.1
    )
    ax.axhline(
        0,
        linestyle=":",
        c="k"
    )
    ax.set_xlabel("Date")
    ax.set_ylabel("$ \Delta $\nDependent Variable")

plt.tight_layout()
plt.show()
```

## The Gaussian Process Model

```{python}
def gaussian_process_model(
        trend_kernel_variance_dist,
        trend_kernel_noise_dist,
        trend_kernel_length_dist,

        seasonal_kernel_variance_dist,
        seasonal_kernel_noise_dist,
        seasonal_kernel_length_dist,
        seasonal_kernel_period_dist,

        effect_initial_dist,
        effect_decay_rate_dist,

        model_noise_dist,
        coefficients_days_dist,
    ):

    def build_trend_covariance(
            X, 
            trend_kernel_variance_dist, 
            trend_kernel_noise_dist, 
            trend_kernel_length_dist,
        ):
        trend_kernel_variance = numpyro.sample(
            "trend_kernel_variance",
            trend_kernel_variance_dist,
        )
        trend_kernel_noise = numpyro.sample(
            "trend_kernel_noise",
            trend_kernel_noise_dist,
        )
        trend_kernel_length = numpyro.sample(
            "trend_kernel_length",
            trend_kernel_length_dist,
        )
        covariance_matrix_trend = RBF_kernel(
            X=X,
            X_prime=X,
            variance=trend_kernel_variance,
            length=trend_kernel_length,
            noise=trend_kernel_noise,
        )
        return covariance_matrix_trend
    
    def build_seasonal_covariance(
            X, 
            seasonal_kernel_variance_dist, 
            seasonal_kernel_noise_dist,
            seasonal_kernel_length_dist,
            seasonal_kernel_period_dist,
        ):
        seasonal_kernel_variance = numpyro.sample(
            "seasonal_kernel_variance",
            seasonal_kernel_variance_dist,
        )
        seasonal_kernel_noise = numpyro.sample(
            "seasonal_kernel_noise",
            seasonal_kernel_noise_dist,
        )
        seasonal_kernel_length = numpyro.sample(
            "seasonal_kernel_length",
            seasonal_kernel_length_dist,
        )
        seasonal_kernel_period = numpyro.sample(
            "seasonal_kernel_length_periodic",
            seasonal_kernel_period_dist,
        )
        covariance_matrix_seasonal = periodic_kernel(
            X=X,
            X_prime=X,
            variance=seasonal_kernel_variance,
            length=seasonal_kernel_length,
            period=seasonal_kernel_period,
            noise=seasonal_kernel_noise,
        )
        return covariance_matrix_seasonal
    
    def build_effect_component(
            X, 
            effect_initial_dist, 
            effect_decay_rate_dist,
        ):
        effect_initial = numpyro.sample(
            "effect_initial",
            effect_initial_dist,
        )
        effect_decay_rate = numpyro.sample(
            "effect_decay_rate",
            effect_decay_rate_dist,
        )
        effect_component = numpyro.deterministic(
            "effect_component",
            exponential_decay_function(
                initial_value=effect_initial,
                rate_decay=effect_decay_rate,
                max_t=X.shape[0],
            )
        )
        return effect_component


    def get_model(X, y, days):
        # The parameters of the kernel function are sampled from their
        # associated prior distributions

        trend_covariance = build_trend_covariance(
            X=X, 
            trend_kernel_variance_dist=trend_kernel_variance_dist, 
            trend_kernel_noise_dist=trend_kernel_noise_dist, 
            trend_kernel_length_dist=trend_kernel_length_dist,
        )
        seasonal_covariance = build_seasonal_covariance(
            X=X, 
            seasonal_kernel_variance_dist=seasonal_kernel_variance_dist, 
            seasonal_kernel_noise_dist=seasonal_kernel_noise_dist,
            seasonal_kernel_length_dist=seasonal_kernel_length_dist,
            seasonal_kernel_period_dist=seasonal_kernel_period_dist,
        )
        effect_component = build_effect_component(
            X=X, 
            effect_initial_dist=effect_initial_dist, 
            effect_decay_rate_dist=effect_decay_rate_dist,
        )
        intercept =  numpyro.sample(
            "intercept",
            dist.MultivariateNormal(
                loc=jnp.zeros(X.shape[0]),
                covariance_matrix=trend_covariance +  seasonal_covariance,
            )
        )

        coefficients_days = numpyro.sample(
            "coefficients_days",
            coefficients_days_dist,
        )
        days_effect = numpyro.deterministic(
            "days_effect", 
            jnp.dot(days, coefficients_days),
        )
        model_noise = numpyro.sample(
            "model_noise",
            model_noise_dist,
        )

        numpyro.sample(
            "y",
            dist.Normal(
                intercept + days_effect + effect_component,
                model_noise
            ),
            obs=y,
        )

    return build_trend_covariance, build_seasonal_covariance, get_model
```

```{python}
#| code-fold: true
#| code-summary: Show supplementary code
def standardize_targets_to_common_statistics(reference_X, targets_X):
    standardizing_mean, standardizing_std = _derive_mean_std(
        X=reference_X
    )
    standardized_reference_X = (reference_X - standardizing_mean) / standardizing_std
    standardized_tragets_X = {}
    for target_name, target_X in targets_X.items():

        standardized_tragets_X[target_name] = (target_X - standardizing_mean) / standardizing_std

    return standardizing_mean, standardizing_std, standardized_reference_X, standardized_tragets_X

def inverse_standardize(X, mean, std):
    return X * std + mean

def _derive_mean_std(X):
    return np.mean(X), np.std(X)
```

```{python}
from sklearn.preprocessing import OneHotEncoder

y_control = df["Branch A"].values.copy()
y_models = {
    model_name: df[model_name].values.copy() for model_name in arms
}

(
    standardizing_mean,
    standardizing_std,
    y_control_standardized,
    y_models_standardized
) = standardize_targets_to_common_statistics(
    reference_X=y_control,
    targets_X=y_models
)

# This can be read as the N of days over which the AB test was conducted
X = np.linspace(0, 1, len(y_control))

days = OneHotEncoder(sparse_output=False).fit_transform(df["day_week"].values.reshape(-1, 1))
```

```{python}
#| code-fold: true
#| code-summary: Show supplementary code
def plot_prior_marginal(ax, prior_frozen, prior_name, samples=1000):
    support = np.sort(prior_frozen.rvs(samples))
    density = prior_frozen.pdf(support)
    ax.plot(
        support,
        density,
        linewidth=2.5,
        c=BLUE
    )
    ax.set_title(f"Prior \n {prior_name}")
    ax.set_xlabel("Support")
    ax.set_ylabel("Density")
    return ax

def plot_sampled_functions(ax, X, X_prime, kernel, priors, n_samples=10):
    alphas = np.linspace(0, 1, n_samples * 2)
    for sample in range(n_samples):

        priors_sample = {}

        for prior_name, prior_frozen_dist in priors.items():

            priors_sample[prior_name] = prior_frozen_dist.rvs(1)

        k = kernel(X, X_prime, **priors_sample)
        sampled_f = np.random.multivariate_normal(np.zeros(len(X)), k)
        ax.plot(
            X,
            sampled_f,
            alpha=alphas[sample],
            c=RED
        )

    ax.set_title("Prior over $f$")
    ax.set_xlabel("$X$")
    ax.set_ylabel("$f(X)$")
    return ax
```

```{python}
def sample_posterior(model, sampler_kernel_algo, rng_key, X, y, days,
                     **mcmc_kwargs):
    # We obtain the kernel used by the MCMC sampler
    sampler_kernel = sampler_kernel_algo(model)
    mcmc = MCMC(
        sampler_kernel,
        **mcmc_kwargs
    )
    mcmc.run(
        rng_key=rng_key,
        X=X,
        y=y,
        days=days,
    )
    mcmc.print_summary()
    posterior_samples = mcmc.get_samples()
    return posterior_samples


def compute_posterior_predictive(rng_key, X, y, X_test, variance,
                                 length, noise):
    # In this case p means that the computations are executing using
    # the test data,
    covariance_xp_xp = RBF_kernel(
        X=X_test,
        X_prime=X_test,
        variance=variance,
        length=length,
        noise=noise,
        include_noise=True
    )
    covariance_xp_x = RBF_kernel(
        X=X_test,
        X_prime=X,
        variance=variance,
        length=length,
        noise=noise,
        include_noise=False
    )
    covariance_x_x = RBF_kernel(
        X=X,
        X_prime=X,
        variance=variance,
        length=length,
        noise=noise,
        include_noise=True
    )

    covariance_x_x_inverse = jnp.linalg.inv(covariance_x_x)
    full_covariance = covariance_xp_xp - jnp.matmul(
        covariance_xp_x,
        jnp.matmul(
            covariance_x_x_inverse,
            jnp.transpose(covariance_xp_x)
        )
    )

    predictive_noise = jnp.sqrt(
        jnp.clip(jnp.diag(full_covariance), a_min=0.0)
    ) * jax.random.normal(rng_key, X_test.shape[:1])

    predictive_mean = jnp.matmul(
        covariance_xp_x,
        jnp.matmul(covariance_x_x_inverse, y)
    )

    return predictive_mean, predictive_mean + predictive_noise

def sample_posterior_predictive(rng_key, X, X_test, y, samples,
                                compute_posterior_predictive):
    vmap_args = (
        random.split(rng_key, samples["trend_kernel_variance"].shape[0]),
        samples["trend_kernel_variance"],
        samples["trend_kernel_length"],
        samples["trend_kernel_noise"],
    )
    predictive_mean, predictive_distribution = vmap(
    lambda rng_key, variance, length, noise: compute_posterior_predictive(
        rng_key, X, y, X_test, variance, length, noise
    )
    )(*vmap_args)
    return predictive_mean, predictive_distribution
```

```{python}
rng_key, rng_key_predictive = random.split(random.PRNGKey(0))

posteriors_models = {}
build_trend_component, build_seasonal_component, get_model = gaussian_process_model(
    trend_kernel_variance_dist=dist.LogNormal(0., .05),
    trend_kernel_noise_dist=dist.LogNormal(0., .05),
    trend_kernel_length_dist=dist.LogNormal(0., .05),

    seasonal_kernel_variance_dist=dist.LogNormal(0., .05),
    seasonal_kernel_noise_dist=dist.LogNormal(0., .05),
    seasonal_kernel_length_dist=dist.LogNormal(0., .05),
    seasonal_kernel_period_dist=dist.LogNormal(0., .05),

    effect_initial_dist=dist.Normal(0., 1.),
    effect_decay_rate_dist=dist.Beta(1., 4.),

    coefficients_days_dist=dist.Normal(0., jnp.ones(days.shape[1]) * 1.),

    model_noise_dist=dist.LogNormal(0., .05),
)

posterior_samples = sample_posterior(
    model=get_model,
    sampler_kernel_algo=NUTS,
    rng_key=rng_key,
    X=X,
    y=y_control_standardized,
    days=days,
    num_warmup=2000,
    num_samples=2000,
    num_chains=4,
    chain_method='parallel',
)

for model_name in arms:

    print(f"Fitting for model {model_name}")

    posterior_samples_model = sample_posterior(
        model=get_model,
        sampler_kernel_algo=NUTS,
        rng_key=rng_key,
        X=X,
        days=days,
        y=y_models_standardized[model_name],
        num_warmup=2000,
        num_samples=2000,
        num_chains=NUM_CHAINS
    )
    posteriors_models[model_name] = posterior_samples_model
```

```{python}
samples_mean_control, _ = sample_posterior_predictive(
    rng_key_predictive,
    X,
    X,
    y_control,
    posterior_samples,
    compute_posterior_predictive
)
samples_mean_control = inverse_standardize(
    X=samples_mean_control,
    mean=standardizing_mean,
    std=standardizing_std
)

samples_mean_models = {}
samples_mean_differences = {}
for model_name in arms:

    samples_mean_model, _ = sample_posterior_predictive(
        rng_key_predictive,
        X,
        X,
        y_models_standardized[model_name],
        posteriors_models[model_name],
        compute_posterior_predictive
    )
    samples_mean_model = inverse_standardize(
        X=samples_mean_model,
        mean=standardizing_mean,
        std=standardizing_std
    )

    samples_mean_models[model_name] = samples_mean_model
    samples_mean_differences[model_name] = samples_mean_model - samples_mean_control
```

# Visualize the results

```{python}
#| code-fold: true
#| code-summary: Show supplementary code
def compute_mean_percentiles(time_series_samples):
    expected_time_series = np.mean(time_series_samples, axis=0)
    percentiles_time_series = np.percentile(
        time_series_samples,
        [2.5, 97.5],
        axis=0
    )
    return expected_time_series, percentiles_time_series

def plot_estimated_time_series(ax, color, time, expected_values,
                               percentiles_values, **plot_kwargs):
    ax.plot(
        time,
        expected_values,
        color=color,
        **plot_kwargs
    )
    ax.fill_between(
        time,
        percentiles_values[0, :],
        percentiles_values[1, :],
        color=color,
        alpha=0.1
    )
    return ax
```

```{python}
expected_mean_control, percentiles_mean_control = compute_mean_percentiles(
    time_series_samples=samples_mean_control
)
expected_residuals_control, percentiles_residuals_control = compute_mean_percentiles(
    time_series_samples=samples_mean_control - y_control
)

means_models = {}
residuals_models = {}
deltas_models = {}
statistics_differences_models = {}
for model_name in arms:

    expected_mean_model, percentiles_mean_model = compute_mean_percentiles(
        time_series_samples=samples_mean_models[model_name]
    )
    expected_residuals_model, percentiles_residuals_model = compute_mean_percentiles(
        time_series_samples=samples_mean_models[model_name] - y_models[model_name]
    )
    expected_mean_difference, percentiles_mean_difference = compute_mean_percentiles(
        time_series_samples=samples_mean_differences[model_name]
    )

    samples_expected_difference = np.mean(samples_mean_differences[model_name], axis=1)
    percentiles_difference = np.percentile(samples_expected_difference, [2.5, 97.5], axis=0)

    means_models[model_name] = {
        "mean": expected_mean_model,
        "percentiles": percentiles_mean_model
    }
    residuals_models[model_name] = {
        "mean": expected_residuals_model,
        "percentiles": percentiles_residuals_model
    }
    deltas_models[model_name] = {
        "mean": expected_mean_difference,
        "percentiles": percentiles_mean_difference
    }
    statistics_differences_models[model_name] = {
        "expected_difference": samples_expected_difference,
        "percentiles_difference": percentiles_difference
    }
```

```{python}
fig, axs = plt.subplots(
    2,
    2,
    tight_layout=True,
    figsize=(12, 8),
    sharey=True
)

for model_name, ax in zip(arms, axs.flatten()):

    expeceted_residuals = round(np.mean(residuals_models[model_name]["mean"]), 2)
    ax_residuals= plot_estimated_time_series(
        ax=ax,
        time=df["date"].values,
        expected_values=residuals_models[model_name]["mean"],
        percentiles_values=residuals_models[model_name]["percentiles"],
        color="k",
        label=f"Mean Residuals {expeceted_residuals}"
    )
    ax_residuals.set_title(model_name)
    ax_residuals.set_ylabel("Residuals")
    ax.legend()

for ax in fig.axes:

    ax.axhline(0, linestyle=":", c="k")
    ax = _set_x_axis_grid(ax=ax)

plt.tight_layout()
plt.show()
```

```{python}
for model in arms:

    fig = plt.figure(tight_layout=True, figsize=(10, 7))
    gs = gridspec.GridSpec(2, 3, figure=fig)

    ax_time_series = fig.add_subplot(gs[0, :])
    ax_difference = fig.add_subplot(gs[1, :2])
    ax_distribution = fig.add_subplot(gs[1, -1])

    ax_time_series = plot_arms_time_series(
        time="date",
        arm_a="Branch A",
        arm_b=model,
        df=df,
        y_label="Dependent Variable",
        title="Estimated Dependent Variable",
        ax=ax_time_series
    )
    _set_x_axis_grid(ax=ax_time_series)

    ax_time_series = plot_estimated_time_series(
        ax=ax_time_series,
        color=BLUE,
        time=df["date"].values,
        expected_values=expected_mean_control,
        percentiles_values=percentiles_mean_control,
    )
    ax_time_series = plot_estimated_time_series(
        ax=ax_time_series,
        color=RED,
        time=df["date"].values,
        expected_values=means_models[model]["mean"],
        percentiles_values=means_models[model]["percentiles"]
    )

    ax_difference = plot_estimated_time_series(
        ax=ax_difference,
        color="k",
        time=df["date"].values,
        expected_values=deltas_models[model]["mean"],
        percentiles_values=deltas_models[model]["percentiles"],
        linestyle="--"
    )

    ax_difference.set_title(
        "Series of Differences in Dependent Variable"
    )
    ax_difference.axhline(0, linestyle=":", c="r")
    ax_difference.set_xlabel("Date")
    ax_difference.set_ylabel("Difference Dependent Variable")

    ax_time_series.axvline(
        df["date"].max(),
        c="k",
        linestyle=":",
        alpha=0.5
    )

    ax_distribution.hist(
        statistics_differences_models[model]["expected_difference"],
        bins=400,
        color="k",
        alpha=0.25,
        density=True
    )
    ax_distribution.set_title("Distribution of\nExpected Difference")
    ax_distribution.set_xlabel("Expected Difference\nin Dependent Variable")
    ax_distribution.axvline(
        np.median(statistics_differences_models[model]["expected_difference"]),
        c="red",
        linestyle="--"
    )
    ax_distribution.axvline(
        statistics_differences_models[model]["percentiles_difference"][0],
        c="red",
        linestyle=":"
    )
    ax_distribution.axvline(
        statistics_differences_models[model]["percentiles_difference"][1],
        c="red",
        linestyle=":"
    )

    ax_time_series.legend()

    ax_time_series = _set_x_axis_grid(ax=ax_time_series)
    ax_difference = _set_x_axis_grid(ax=ax_difference)

    plt.suptitle("AB Test")
    plt.tight_layout()
    plt.show()
```

# Conclusion

# Hardware and Requirements
Here you can find the hardware and python requirements used for building this post.

```{python}
%watermark
```

```{python}
%watermark --iversions
```


