[
  {
    "objectID": "posts/generic_posts/lin_ucb_jax.html",
    "href": "posts/generic_posts/lin_ucb_jax.html",
    "title": "LinUCB for Contextual Multi-Armed Bandit",
    "section": "",
    "text": "Show supplementary code\nfrom typing import Tuple, List, Any\n\nfrom functools import partial\n\nimport numpy as np\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.manifold import TSNE\n\nfrom jax.typing import ArrayLike\nfrom jax.lax import scan\nfrom jax import vmap\nfrom jax import numpy as jnp\nfrom jax import random, jit\nfrom jax.scipy.linalg import inv\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n\n@jit\ndef tempered_softmax(logits, temperature=1):\n    \"\"\"Produce a tempered softmax given logits.\n\n    Args:\n        logits (ArrayLike): logits to be turned into probability.\n        temperature (int, optional): parameter controlling the softness\n        of the function, the higher the value the more soft is the function. \n        Defaults to 1.\n\n    Returns:\n        ArrayLike: simplex derived from the input logits.\n    \"\"\"\n    nominator = jnp.exp(logits / temperature)\n    denominator = jnp.sum(nominator, axis=1).reshape(-1, 1)\n    return nominator / denominator"
  },
  {
    "objectID": "posts/generic_posts/lin_ucb_jax.html#what-we-will-cover",
    "href": "posts/generic_posts/lin_ucb_jax.html#what-we-will-cover",
    "title": "LinUCB for Contextual Multi-Armed Bandit",
    "section": "1.1 What we will cover",
    "text": "1.1 What we will cover\n\nVery brief introduction to multi armed and contextual multi armed bandit problems.\nVery brief introduction to the LinUCB algorithm.\nSimulating a disjoint contentual multi armed bandit problem.\nImplementing the LinUCB algorithm in JAX.\nTesting the algorithm on simulated data.\nAccelerating testing and simulation using a GPU.\nEvaluating the performance of the algorithm."
  },
  {
    "objectID": "posts/generic_posts/lin_ucb_jax.html#what-we-will-not-cover",
    "href": "posts/generic_posts/lin_ucb_jax.html#what-we-will-not-cover",
    "title": "LinUCB for Contextual Multi-Armed Bandit",
    "section": "1.2 What we will not cover",
    "text": "1.2 What we will not cover\n\nJAX fundamentals.\nIn depth expalantion of multi-armed and contextual multi-armed bandit problems.\nIn depth expalantion of the LinUCB algorithm."
  },
  {
    "objectID": "posts/generic_posts/lin_ucb_jax.html#multi-armed-bandit-problem",
    "href": "posts/generic_posts/lin_ucb_jax.html#multi-armed-bandit-problem",
    "title": "LinUCB for Contextual Multi-Armed Bandit",
    "section": "2.1 Multi-Armed Bandit Problem",
    "text": "2.1 Multi-Armed Bandit Problem\nThe multi-armed bandit problem describes a situation where an agent is faced with \\(K = (k_0, k_1, \\dots, k_n)\\) different options (or arms), each one with an associated unknown payoff (or reward) \\(r_k\\)1 (Sutton and Barto 2018).\nThe goal of the agent is to select, over a finite sequence of interactions \\(T=(t_0, t_1, \\dots, t_n)\\), the set of options that will maximize the expected total payoff over \\(T\\) (Sutton and Barto 2018). What our agent is interested in then is the true value \\(q*\\) of taking an action \\(a\\) and selecting a given arm \\(k\\), the action associated with highest value should be the go-to strategy for maximizing the cumulative payoff\nSince the true value is not known, our agent often has to rely on an estimate of such value which comes with an associated level of uncertainity. We can think of this in terms of the relationship between the mean of the distribution from which the rewards of a given arm are sampled and its empirical estimate with associated standard error.\nSelecting the best set of actions over a finite number of interactions then requires a balance between and exploitative and explorative behaviour\n\nExploit the options with the highest associated estimated reward\nAllow the exploration of other options in case our exploitative behaviour has been biased by noisy estimates."
  },
  {
    "objectID": "posts/generic_posts/lin_ucb_jax.html#contextual-multi-armed-bandit-problem",
    "href": "posts/generic_posts/lin_ucb_jax.html#contextual-multi-armed-bandit-problem",
    "title": "LinUCB for Contextual Multi-Armed Bandit",
    "section": "2.2 Contextual Multi-Armed Bandit Problem",
    "text": "2.2 Contextual Multi-Armed Bandit Problem\nThe conventional multi-armed bandit scenario attempts to solve what is called a nonassociative task, meaning that the payoff of a given action (e.g. selecting one of the \\(k\\) available arms) doesn’t depend on any context. This means that as a measure of value for a given action, we are interested in\n\\(\\mathbb{E}[r | K=k]\\)\nIn a contextual multi-amred bandit scenario instead, the payoff of a given action is dependent on the context in which the action is performed. This implies that given a matrix of context vectors \\(X_{K\\times h}\\) we try to estimate the value of a given action as\n\\(\\mathbb{E}[r | K=k, X=x_k]\\)\nIn order to get a better understanding of what we mean here, let’s simulate a potential generating process that could give rise to data suitable for a contextual multi-armed bandit problem.\n\n2.2.1 Create the Simulation Dataset\nWe will approximate the data generating process using sklearn’s make_classification function. The idea is to re-formulate this as a multi-class classification problem where the context are features able to influece the probability to pick one of n_arms classes.\nIn order to simulate some of the challenges we could face in a real world setting we will add the following hurdles:\n\nOnly a small portion of the fetaures will actually have predictive power on which arm is the most promising.\nWe will generate features that have a ceertain degree of overlap (immagine them being drawn from relatively spread-out distributions)\nWe will enforce sparsity on the reward generated by each arm. This is like saying that the reward generated by pulling a given arm comes from a zero inflated distribution of the form\n\n\\[\n\\begin{gather}\nhurdle \\sim Bernoulli(p_{hurdle}) \\\\\nP(r | K=k, X=x_k) = \\alpha x_k \\\\\nr = \\begin{cases} 0,& \\text{if hurdle} = 0 \\\\\nBernoulli(p_{r}),& \\text{otherwise} \\end{cases}\n\\end{gather}\n\\]\n\nUNIT_CONTEXT_SIZE = 1\nUNIT_ARMS = 2\n\nINFORMATIVE = 2\nREPEATED = 0\nREDUNDANT  = 0\nRANDOM  = 8\n\nCONTEXT_SIZE = INFORMATIVE + REPEATED + REDUNDANT + RANDOM\n\nREWARD_SPARSITY = 0.99\nREWARD_WEIGHTING = 1 / (1 - REWARD_SPARSITY)\n\nN_USERS = 1000\n\nN_ARMS = UNIT_ARMS ** 2 # We ensure we can always plot the arms in a squared grid\nCLASS_SEP = .9\n\n\n\nShow supplementary code\ndef generate_context(n_arms, n_users, context_size, redundant, repeated, informative, clusters_per_class, class_sep, with_intercept=True):\n    context, groups = make_classification(\n        n_classes=n_arms, \n        n_samples=n_users, \n        n_features=context_size,\n        n_redundant=redundant,\n        n_repeated=repeated,\n        n_informative=informative,\n        n_clusters_per_class=clusters_per_class,\n        class_sep=class_sep\n    )\n    context = (context - context.mean(0)) / context.std(0)\n    if with_intercept:\n\n        context = np.hstack([context, np.ones(shape=(context.shape[0], 1))])\n    \n    return context, groups\n\ndef compute_arms_probabilities(context, groups, with_intercept=True, temperature=5):\n    model = LogisticRegression(fit_intercept=not with_intercept).fit(context, groups)\n    weights = model.coef_.T\n\n    logits = context @ weights\n    arms_probabilities = tempered_softmax(logits=logits, temperature=temperature)\n    return arms_probabilities, model\n\ndef expand_context_to_arms(context, n_arms):\n    context = np.array([context for _ in range(n_arms)])\n    context = np.swapaxes(context, 0, 1)\n    return context\n\n\n\nCONTEXT, GROUPS = generate_context(\n    n_arms=N_ARMS, \n    n_users=N_USERS, \n    context_size=CONTEXT_SIZE, \n    redundant=REDUNDANT, \n    repeated=REPEATED, \n    informative=INFORMATIVE, \n    clusters_per_class=1, \n    class_sep=CLASS_SEP, \n    with_intercept=True\n)\nARMS_PROBAILITIES, MODE = compute_arms_probabilities(\n    context=CONTEXT,\n    groups=GROUPS,\n    with_intercept=True,\n)\n\nCONTEXT = expand_context_to_arms(\n    context=CONTEXT,\n    n_arms=N_ARMS,\n)\n\nHere we will use TSNE for projecting the multidimensional context space on a 2D plane, this should allow us to get a better intuition of what is going on. What we expect to see are separate spheres or regions (this depends on how much noise we encode in our context space) with different colouring depening on which arms they are associated with\n\nembedding = TSNE().fit_transform(CONTEXT[:, 0, :])\n\nfig, axs = plt.subplots(UNIT_ARMS, UNIT_ARMS, figsize=(8, 8), sharex=True, sharey=True)\naxs = axs.flatten()\nfor arm in range(ARMS_PROBAILITIES.shape[1]):\n\n    axs[arm].scatter(\n        embedding[:, 0],\n        embedding[:, 1],\n        c=ARMS_PROBAILITIES[:, arm],\n        s=1,\n        cmap=\"RdBu_r\"\n    )\n    axs[arm].set_title(f\"Arm {arm}\")\n\nfig.supylabel(\"First Context Dimension\")\nfig.supxlabel(\"Second Context Dimension\")\nfig.suptitle(\"Arm Reward Probability \\n Visualized in Context Space\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/generic_posts/lin_ucb_jax.html#linucb-algorithm-for-a-multi-armed-bandit-problem",
    "href": "posts/generic_posts/lin_ucb_jax.html#linucb-algorithm-for-a-multi-armed-bandit-problem",
    "title": "LinUCB for Contextual Multi-Armed Bandit",
    "section": "2.3 LinUCB Algorithm for a Multi-Armed Bandit Problem",
    "text": "2.3 LinUCB Algorithm for a Multi-Armed Bandit Problem\nTo solve a multi-armed bandit problem with context we can leverage an algorithm called Linear Upper Confidence Bound [li2010contextual] (i.e. LinUCB).\nThe aim of the algorithm is to progressively obtain a reliable estimate of all the options provided by the multi-armed bandit and to do so efficiently (i.e. with the smallest number of interactions.)\nFor doing so LinUCB simply fit a multinomial regression to the context (i.e. the covariates) provided by each arm of the bandit in order to estimate the return value associated with each arm, more formally\n\\[\n\\mathbb{E}[r_{t,k} | x_k] = x^\\intercal \\theta^*_k\n\\tag{1}\\]\nwhere \\(\\theta^*_k\\) are the set of parameters associated with a given arm \\(k\\) for which the algorithm is trying to find the optimal estimate. Here we assume \\(x_k\\) to be invariant across all \\(t \\in T\\) but that is not always the case.\nAs the number of covariates can be large we cannot know a-priori if all of them are informative or if they are collinear. For this reason LinUCB rely on a form of regularized regression called Ridge Regression. Moreover since we don’t have a datset to fit this model on but rather we refine the estimate for \\(\\theta^*_k\\) as we interact with the arms \\(K\\), LinUCB utilizes what is called online learning for obtaining estimates of each interaction as given by\n\\[\n\\begin{gather}\n\\mathbb{A}_k = \\mathbb{X}_k^\\intercal \\mathbb{X_k} + \\mathbb{I}_d\\\\\n\\hat{\\theta_k} = \\mathbb{A}^{-1}\\mathbb{X}_k^\\intercal r_k\n\\end{gather}\n\\]\nwhere \\(\\mathbb{X}_k\\) is the design matrix \\(m \\times d\\) where \\(m\\) is the number of contexts (i.e., training inputs) and \\(d\\) the dimensionality of each context (i.e., the number of covariates considered). Here \\(\\mathbb{I}\\) is the \\(d\\times d\\) identity matrix and \\(r_k\\) is the corresponding \\(m\\)-dimensional reward (or response) vector. The identity matrix (which is usually expressed with a scaling factor \\(\\lambda\\) here implicitly set to 1.) act as a constrain on the parameter \\(\\theta_k\\).\nThe adavntage of relying on the Ridge regression is that we can interpret \\(\\hat{\\theta_k}\\) a the mean of the Bayesian posterior of the parameter estimate and \\(\\mathbb{A}_k^-1\\) its covariance. In this way we can compute the expectation for \\(r_{t, k}\\) as in Equation 1 and with it its associated standard deviation \\(\\sqrt{x_k^\\intercal \\mathbb{A_k}^{-1}x_k}\\) which is proven to be a reasoble tight bound.\nBased on this assumption then at any step \\(t\\) we can select the appropriate arm \\(k\\) to be\n\\(k_t = \\arg \\max_{k_t \\in K_t}(x^\\intercal \\hat{\\theta}_k + \\alpha \\sqrt{x_k^\\intercal \\mathbb{A_k}^{-1}x_k})\\)\nwhere \\(\\alpha\\) becomes a constant that controls the exploration vs exploitation behaviour of the algorithm. Indeed, we can think of this as taking the value sitting at \\(\\alpha\\) standard deviation at the right of the mean of a gaussian as most optimistic retun value when picking a certain arm. Larrger values of \\(\\alpha\\) will therefore encourage to select arms with highest UCB even if the return is much more uncertain (as the UCB lies far away from the expected value).\nThe positive aspect of this is that the more an arm get selected the tighter it’s confidence bound become, up to a point in which it will become more promising to explore arms that provides larger UCB by the fact that they have simply been explored less."
  },
  {
    "objectID": "posts/generic_posts/lin_ucb_jax.html#parameters-initialisation",
    "href": "posts/generic_posts/lin_ucb_jax.html#parameters-initialisation",
    "title": "LinUCB for Contextual Multi-Armed Bandit",
    "section": "3.1 Parameters Initialisation",
    "text": "3.1 Parameters Initialisation\nAs a first step we will need to initialize the matrices for \\(X\\) and \\(r\\) from this point onward we will call them \\(A\\) and \\(b\\) as mentioned in [li2010contextual].\n\ndef init_matrices(context_size: int) -&gt; Tuple[ArrayLike, ...]:\n    A = jnp.eye(N=context_size)\n    b = jnp.zeros(shape=(context_size, 1))\n    return A, b\n\ndef init_matrices_for_all_arms(\n        number_of_arms: int, \n        context_size: int,\n    )  -&gt; Tuple[ArrayLike, ...]:\n\n    arms_A = []\n    arms_b = []\n    for _ in range(number_of_arms):\n\n        A, b = init_matrices(context_size=context_size)\n        arms_A.append(A)\n        arms_b.append(b)\n\n    arms_A = jnp.array(arms_A)\n    arms_b = jnp.array(arms_b)\n\n    return arms_A, arms_b"
  },
  {
    "objectID": "posts/generic_posts/lin_ucb_jax.html#computations",
    "href": "posts/generic_posts/lin_ucb_jax.html#computations",
    "title": "LinUCB for Contextual Multi-Armed Bandit",
    "section": "3.2 Computations",
    "text": "3.2 Computations\nWe will now define the code for the various computations, namely deriving the \\(\\theta\\) and \\(\\sigma\\) parameters. And subsequently estimating the expection for the return as well as the UCB.\nIn order to speed up the computation, we will leverage the Just in Time Compilation functionality in JAX as well as its automatic vectorisation.\nThe first one will allow us to cache computations that are used multiple times by the LinUCB algorithm, while the second one will allow us in this case to vectorize the computation across all the arms in our bandit problem (instead of slowly iterating through them).\n\n@jit\ndef compute_theta(\n        A_inverse: ArrayLike, \n        b: ArrayLike,\n    ) -&gt; ArrayLike:\n\n    theta = jnp.dot(A_inverse, b)\n    return theta\n\n@jit\ndef compute_sigma(\n        context: ArrayLike, \n        A_inverse: ArrayLike,\n    ) -&gt; ArrayLike:\n\n    sigma = jnp.sqrt(\n        jnp.dot(\n            jnp.transpose(context), \n            jnp.dot(A_inverse, context)\n        )\n    )\n    return sigma\n\n@jit\ndef compute_mu(\n        theta: ArrayLike, \n        context: ArrayLike,\n    )-&gt; ArrayLike:\n\n    mu = jnp.dot(\n        jnp.transpose(theta),\n        context\n    )\n    return mu\n\n@jit\ndef compute_upper_bound(\n        A: ArrayLike, \n        b: ArrayLike, \n        context: ArrayLike, \n        alpha: float,\n    ) -&gt; ArrayLike:\n    A_inverse = inv(A)\n    context_column = jnp.reshape(a=context, newshape=(-1, 1))\n\n    theta = compute_theta(\n        A_inverse=A_inverse, \n        b=b\n    )\n    sigma = compute_sigma(\n        context=context_column, \n        A_inverse=A_inverse\n    )\n    mu = compute_mu(\n        theta=theta, \n        context=context_column\n    )\n    upper_bound = mu + (sigma * alpha)\n    return upper_bound\n\n@jit\ndef execute_linucb(\n        alpha: float, \n        arms_A: ArrayLike, \n        arms_b:ArrayLike, \n        context: ArrayLike, \n        noise: ArrayLike,\n    ) -&gt; ArrayLike:\n    partialized_compute_upper_bound = partial(\n        compute_upper_bound,\n        alpha=alpha\n    )\n    upper_bound = vmap(\n        fun=vmap( \n            fun=partialized_compute_upper_bound, \n            in_axes=(0, 0, 0)\n        ),\n        in_axes=(None, None, 0)\n    )(arms_A, arms_b, context).squeeze()\n    upper_bound += noise\n    return upper_bound"
  },
  {
    "objectID": "posts/generic_posts/lin_ucb_jax.html#update-parameters",
    "href": "posts/generic_posts/lin_ucb_jax.html#update-parameters",
    "title": "LinUCB for Contextual Multi-Armed Bandit",
    "section": "3.3 Update parameters",
    "text": "3.3 Update parameters\n\n@jit\ndef update_parameters(\n    arms_A: ArrayLike, \n    arms_b: ArrayLike, \n    arms_context: ArrayLike, \n    policy: ArrayLike, \n    reward: ArrayLike\n) -&gt; Tuple[ArrayLike, ArrayLike]:\n    new_A=arms_A[policy, :, :] \n    new_b=arms_b[policy, :, :]\n    context=arms_context[policy, :]\n\n    context_column = jnp.reshape(a=context, newshape=(-1, 1))\n    new_A += jnp.dot(context_column, jnp.transpose(context_column))\n    new_b += reward*context_column\n\n    arms_A = arms_A.at[policy, :, :].set(new_A)\n    arms_b = arms_b.at[policy, :, :].set(new_b)\n    return arms_A, arms_b"
  },
  {
    "objectID": "posts/generic_posts/lin_ucb_jax.html#footnotes",
    "href": "posts/generic_posts/lin_ucb_jax.html#footnotes",
    "title": "LinUCB for Contextual Multi-Armed Bandit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe assumption is that the payoff comes from a stationary distribution, meaning that at any point in time we can expect that \\(r_k \\sim \\mathcal{N}(\\mu_k, \\sigma_k)\\) (or any other suitable probability distribution).↩︎"
  },
  {
    "objectID": "posts/kalman_bayesian_filters/gh_filter/index.html",
    "href": "posts/kalman_bayesian_filters/gh_filter/index.html",
    "title": "Chapter 1 - The g-h Filter",
    "section": "",
    "text": "Show supplementary code\nimport numpy as np\n\nfrom itertools import product\n\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/kalman_bayesian_filters/gh_filter/index.html#combining-noisy-measurements",
    "href": "posts/kalman_bayesian_filters/gh_filter/index.html#combining-noisy-measurements",
    "title": "Chapter 1 - The g-h Filter",
    "section": "2.1 Combining noisy measurements",
    "text": "2.1 Combining noisy measurements\nLet’s imagine to be in a situation in which we want to evaluate a certain phenomena using two sensors (or any measuring tool). We know that both sensors provide measures with an error margin of \\(\\pm 5\\) units (expressed as standard deviations), so how can we obtain a reliable estimate of the state of the underlying phenomena?\nIn the absence of any other information, the most straightforward strategy here is to leverage the measurements provided by both sensors and compute the expected value.\n\nmeasurements_dict = {\n    \"Sensor 1\": np.random.normal(30, 5, 50),\n    \"Sensor 2\": np.random.normal(20, 5, 50)\n}\n\nfig, ax = sensors_plots(\n    measurements_dict=measurements_dict, \n    show_expectation=True\n)\nplt.show()\n\n\n\n\nThe computed expected value gives us a more reasonable and robust estimate of the true state of the phenomena we are trying to measure. This because it is a value compatible with the measurements provided by both sensors.\nThis assumes both sensors to be equally trustworthy or in other words to have the same margin of error. In case we knew that one of the two sensors is sensibly more accurate than the other (let’s say a margin of error of \\(\\pm 2.5\\) units) we would probably expect it to contribute more to our estimate. However, surprisingly including some of the information provided by the less accurate sensor can give us an even better estimate!\n\nmeasurements_dict = {\n    \"Accurate Sensor\": np.random.normal(30, 2.5, 50),\n    \"Noisy Sensor\": np.random.normal(20, 5, 50)\n}\n\nfig, ax = sensors_plots(\n    measurements_dict=measurements_dict, \n    show_expectation=True\n)\nplt.show()\n\n\n\n\nIf we look at the overlap between the measurements provided by both sensors we can see that the region of acceptable values compatible with the two different measurements regimes is narrower than the one provided by the more accurate sensor alone!\nInterestingly, this region doesn’t include the expected value from the accurate sensor and compute a naive average between the two provide an estimate that is incompatible with the observed measurements.\n\nTake Home Message\nTwo measurements sources, even if one is less accurate, are better than one. Information is always used and never thrown away."
  },
  {
    "objectID": "posts/kalman_bayesian_filters/gh_filter/index.html#measurements-and-predictions",
    "href": "posts/kalman_bayesian_filters/gh_filter/index.html#measurements-and-predictions",
    "title": "Chapter 1 - The g-h Filter",
    "section": "2.2 Measurements and Predictions",
    "text": "2.2 Measurements and Predictions\nA more common situation however is observed sequential measurements from a single, potentially noisy sensor. Of course the more measurements we take the closer we should get to the actual state of the phenomena we are analyzing.\nUnfortunately in many situations we don’t have this luxury. As we mentioned at the beginning of this post, in some situation we can only observe a handful of measurements (potentially only one!) like in the case of temporal series.\nIn a temporal series if our measurements happen to be at a frequency higher than the dynamics of the phenomena under investigation we are in a relatively comfortable position but otherwise we are going to be in a situation where information is sparse and vulnerable to noise.\n\nmeasurements = np.array([50, 60, 48, 49, 46, 42, 40])\n\nfig, ax = time_series_plots(\n    measurements=measurements,\n    expected_noise=5\n)\nplt.show()\n\n\n\n\nIf we were to use the same heuristics illustrated before, a jump of ~10 units between Time 0, Time 1 and Time 2 would be compatible with the observed measurements and their associated error bounds.\nDepending on the nature of the phenomena under investigation this might or might not be plausible. In order to verify this we need to “blend” the measurements provided by our noisy sensor with a model of the dynamics controlling our phenomena. Let’s start with a very naive model.\n\nfig, ax = time_series_plots(\n    measurements=measurements,\n    expected_noise=5\n)\nax.plot(\n    np.arange(len(measurements)),\n    [50] * len(measurements),\n    linestyle=\":\",\n    color=\"k\",\n    label=\"Constant Dynamics\"\n)\nax.legend()\nplt.show()\n\n\n\n\nIn this case we assume that the state underlying our phenomena stays constant over time, hence producing virtually no variations in the observed measurements.\nWe can already see how this, despite its simplicity, is already informing us that, under the assumptions of our model, the measurement at Time 1 is likely to be driven by noise.\n\nHeads Up\nIt is important to remember that the word noise doesn’t necessary indicate the presence of an error. It rather represents variations that are not consistent with our model or that cannot be explained by the information we have at hand.\n\nThat said, we can also clearly see how the assumptions of a constant dynamics model do not get along well with the available measurements, which seems to be characterized by a downward trend. So maybe let’s try to draw the gradient line connecting the first to the last measurement\n\ndef slope_model(beta, alpha, time):\n    \"\"\"Generate predictions according to a \n    simple linear model.\n\n    Args:\n        - beta(float): intercept value.\n        - alpha(float): slope value.\n        - time(np.array): time points.\n    \n    Returns:\n        - predictions (np.array): predictions generated by\n        the model.\n    \"\"\"\n    return beta + alpha * time\n\nnegative_slope_model = slope_model(\n    beta=50, \n    alpha=-1.7, \n    time=np.arange(len(measurements))\n)\n\nfig, ax = time_series_plots(\n    measurements=measurements,\n    expected_noise=5\n)\nax.plot(\n    np.arange(len(measurements)),\n    negative_slope_model,\n    linestyle=\":\",\n    color=\"k\",\n    label=\"Negative Slope Dynamics\"\n)\nax.legend()\nplt.show()\n\n\n\n\nAlthough far from being perfect, this new model appears to provide a much better fit to our measurements. It is compatible with most of them or at least lies within the acceptable region defined by their error bounds. On top of that, having a model allows us to perform predictions about future measurements that we can then confront with actual observed values.\nHowever, we can see that the predictions from our negative slope model are not incompatible with the measurement provided at Time 1, what should we do in this case then? Should we simply disregard it? At the end of the day the model is just an artificial device plucked out of thin air (well, not exactly as it should be informed by our knowledge of the dynamics at play) while the measurement is the manifestation of real events. If we can exclude a completely faulty record, it still holds some information that we can use for improving our estimate.\nIf both measurements and model’s predictions are identical one of the two becomes redundant. However, as soon as we observe discrepancies between them we know that there is information that can be gained. So what should we trust in this situation, the predictions or the measurements?\n\nTake Home Message\nNo source of information must be ignored, a better estimate can be achieved by blending model predictions and measurements. Two measurements are better than one."
  },
  {
    "objectID": "posts/kalman_bayesian_filters/gh_filter/index.html#blending-measurements-and-predictions",
    "href": "posts/kalman_bayesian_filters/gh_filter/index.html#blending-measurements-and-predictions",
    "title": "Chapter 1 - The g-h Filter",
    "section": "2.3 Blending Measurements and predictions",
    "text": "2.3 Blending Measurements and predictions\nSo how should this blending happen? A reasonable strategy would be to weight more the source of information that we believe to be more reliable. Let’s look at how we can formalize this in python code. From here on we will occasionally rely on JAX for making our code more efficient (we won’t spend time describing what JAX is, we recommend checking out the documentation page).\n\nfrom jax import jit\nfrom jax.lax import scan\n\nimport jax.numpy as jnp\n\nfrom jax import random\n\n\n@jit\ndef custom_gain_model(measurements, starting_estimate, gain_rate, scale_factor, gain_update_frequency):\n    \"\"\"Create a custom gain model. It will iteratively estimate \n    the state underlying the observed measurements. The function will be JIT compiled for increasing execution time.\n\n    Args:\n        - measurements (np.array): observed measurements\n        - starting_estimate (float): the starting point \n        for the estimate.\n        - gain_rate (float): the time dependent increase in estimate, it correspond to the slope of the slope_model.\n        - gain_update_frequency (float): time steps required for having a full update \n        of the gain.\n        - scale_factor (float): scale applied to weight the measurements.\n    \n    Returns:\n        - estimated_states(np.array): estimated states underlying the measurements.\n    \"\"\"\n    @jit\n    def step(carry, measurement):\n        \"\"\"Step function for computing the estimate, this will be iterated\n        over all the measurements. The arguments required for estimating the \n        state will be provided by the carry.\n        \"\"\"\n        previous_estimate, gain_update_frequency, gain_rate, scale_factor = carry\n        prediction = previous_estimate + gain_rate * gain_update_frequency\n\n        new_estimate = prediction + scale_factor * (measurement - prediction)\n        \n        new_carry = (new_estimate, gain_update_frequency, gain_rate, scale_factor)\n        output = jnp.array([new_estimate, prediction])\n\n        return new_carry, output\n\n    _, output = scan(\n        step,\n        (starting_estimate, gain_update_frequency, gain_rate, scale_factor),\n        measurements\n    )\n\n    estimated_states = output[:, 0]\n    predictions = output[:, 1]\n\n    return estimated_states, predictions\n\nmeasurements = 50 + np.arange(10) * -1.7\nmeasurements += random.normal(\n    key=random.PRNGKey(666), \n    shape=(len(measurements),)\n) * 5\n\nestimated_states, predictions = custom_gain_model(\n    measurements=measurements, \n    gain_update_frequency=1.,\n    starting_estimate=50., \n    gain_rate=-1.7, \n    scale_factor=.3\n)\nestimated_states = np.array(estimated_states)\npredictions = np.array(predictions)\n\nIn this case we used as gain rate the slope of our previous slope_model and expressed a belief that the predictions of our model are to be trusted more than the measurements. Precisely that the discrepancy between prediction and measurement should contribute only by 30% to the overall estimate.\n\n\nShow supplementary code\nfig, ax = time_series_plots(\n    measurements=measurements,\n    expected_noise=5\n)\nax.plot(\n    np.arange(len(measurements)),\n    estimated_states,\n    linestyle=\"--\",\n    color=\"r\",\n    label=\"Estimated States\"\n)\nax.plot(\n    np.arange(len(measurements)),\n    predictions,\n    linestyle=\":\",\n    color=\"r\",\n    marker=\"*\",\n    label=\"Predictions\"\n)\nax.legend()\nplt.show()\n\n\n\n\n\nBy varying our belief, expressed through the gain_rate we can obtain estimates that are more or less influenced by either the dynamics’ model or by the measurements.\n\n\nShow supplementary code\nimport matplotlib\n\ncolor_mapper = matplotlib.cm.get_cmap('Dark2')\n\nfig, ax = time_series_plots(\n    measurements=measurements,\n    expected_noise=5\n)\n\nfor index, scale_factor in enumerate([.4, .6, .8,]):\n\n    estimated_states, predictions = custom_gain_model(\n        measurements=measurements, \n        starting_estimate=52., \n        gain_rate=-1.7, \n        gain_update_frequency=1.,\n        scale_factor=scale_factor\n    )\n    estimated_states = np.array(estimated_states)\n    ax.plot(\n        np.arange(len(measurements)),\n        estimated_states,\n        c=color_mapper(index),\n        linestyle=\"--\",\n        label=f\"Scale Factor {scale_factor}\"\n    )\n    ax.plot(\n        np.arange(len(measurements)),\n        predictions,\n        c=color_mapper(index),\n        linestyle=\":\",\n        marker=\"*\",\n        label=f\"Predictions {scale_factor}\"\n    )\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\n\n\n/var/folders/5w/nz0br6dn399_04hd3q55ljth0000gq/T/ipykernel_39848/1561832762.py:3: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n  color_mapper = matplotlib.cm.get_cmap('Dark2')\n\n\n\n\n\nhere we used as gain_rate a fixed value of -1.7 as it was the slope of the linear model we used for simulating our measurements. However, having a fixed gain rate is quite an inflexible strategy which becomes particular ineffective when the rate of change varies dynamically. To circumvent this issue, we can let the gain_rate parameter varies based on how far are the predictions of our model dynamic from the observed measurement.\n\n@jit\ndef adaptive_gain_model(measurements, starting_estimate, starting_gain, gain_factor, gain_update_frequency, scale_factor):\n    \"\"\"Create an adaptive gain model. It will iteratively estimate \n    the state underlying the observed measurements and compute the gain adaptively based on the mismatch between prediction and measurement. \n    The function will be JIT compiled for increasing execution time.\n\n    Args:\n        - measurements (np.array): observed measurements\n        - starting_estimate (float): the starting point \n        for the estimate.\n         - starting_gain (float): the starting point \n        for the gain.       \n        - gain_factor (float): factor used for adaptively update the gain.\n        - gain_update_frequency (float): time steps required for having a full update \n        of the gain.\n        - scale_factor (float): scale applied to weight the measurements.\n    \n    Returns:\n        - estimated_states(np.array): estimated states underlying the measurements.\n    \"\"\"\n    @jit\n    def step(carry, measurement):\n        \"\"\"Step function for computing the estimate, this will be iterated\n        over all the measurements. The arguments required for estimating the \n        state will be provided by the carry.\n        \"\"\"\n        (\n            previous_estimate, \n            previous_gain, \n            gain_update_frequency, \n            gain_factor, \n            scale_factor\n         ) = carry\n        \n        prediction = previous_estimate + previous_gain * gain_update_frequency\n        residual = measurement - prediction\n\n        new_estimate = prediction + scale_factor * (residual)\n        new_gain = previous_gain + gain_factor * (residual / gain_update_frequency)\n        \n        new_carry = (\n            new_estimate, \n            new_gain, \n            gain_update_frequency, \n            gain_factor, \n            scale_factor\n        )\n        output = jnp.array([new_gain, new_estimate, prediction])\n\n        return new_carry, output\n\n    _, output = scan(\n        step,\n        (starting_estimate, starting_gain, gain_update_frequency, gain_factor, scale_factor),\n        measurements\n    )\n\n    gains = output[:, 0]\n    estimated_states = output[:, 1]\n    predictions = output[:, 2]\n\n    return gains, estimated_states, predictions\n\ngains, estimated_states, predictions = adaptive_gain_model(\n    measurements=measurements, \n    gain_update_frequency=1.,\n    starting_estimate=50., \n    starting_gain=-1.7, \n    scale_factor=.6,\n    gain_factor=.1\n)\n\ngains = np.array(gains)\nestimated_states = np.array(estimated_states)\npredictions = np.array(predictions)\n\nIn this new version of custom_gain_model we let the gain (which can grossly be compared with the slope of our original slope_model) be a fraction (controlled by gain_factor) of the residuals derived from predictions and observed measurements. As before this new_gain parameter controls how much belief we put in either the model or the observed measurements but instead of setting its value statically we let it adapt based on the performance of the model so that if our initial guess results to be inadequate at a certain point in time (e.g. due to abrupt changes in the state dynamics) the model can correct itself.\n\n\nShow supplementary code\nfrom matplotlib.gridspec import GridSpec\n\nfig = plt.figure(figsize=(8, 5))\nspec = GridSpec(2, 2, figure=fig)\n\nax_state = fig.add_subplot(spec[0, :])\nax_predictions = fig.add_subplot(spec[1, 0])\nax_gains = fig.add_subplot(spec[1, 1])\n\nfor ax in [ax_state, ax_predictions]:\n\n    ax = time_series_plots(\n        measurements=measurements,\n        expected_noise=5,\n        current_ax=ax\n    )\n\nax_state.plot(\n    np.arange(len(measurements)),\n    estimated_states,\n    linestyle=\"--\",\n    color=\"r\",\n    label=\"Estimated States\"\n)\nax_state.legend()\nax_predictions.plot(\n    np.arange(len(measurements)),\n    predictions,\n    linestyle=\"--\",\n    color=\"r\",\n    label=\"Predictions\"\n)\nax_predictions.legend()\n\nax_gains.plot(\n    np.arange(len(measurements)),\n    gains,\n    linestyle=\"--\",\n    color=\"r\",\n    label=\"Gains\"\n)\nax_gains.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/kalman_bayesian_filters/gh_filter/index.html#finally-to-the-g-h-filter",
    "href": "posts/kalman_bayesian_filters/gh_filter/index.html#finally-to-the-g-h-filter",
    "title": "Chapter 1 - The g-h Filter",
    "section": "2.4 Finally to the g-h Filter",
    "text": "2.4 Finally to the g-h Filter\nThe adaptive_gain_model is basically equivalent to the g-h filter we introduced at the beginning of this post. The g and h are the scaling factors that we introduced in our function, namely the scale_factor (weighting the contribution of the the observed measurements) and the gain_factor (scaling the change in state estimate over time).\n\nTake Home Message\nThe key points underlying the algorithm summarize what we highlighted until here\n\nMore data points are better than less\nNo data must be thrown away, no information must be discarded.\nThe middle point between two value can give a more accurate estimate.\nMeasurement will be predicted along with rate of change based on current state estimate and how much change we will expect in the future.\nThe new estimate will be generated as a middle point between measurement and estimated state, giving more weight to the more accurate of the two.\n\n\nWhat is the target of our estimation efforts is what is called the system. We can think of it as the data generating process, for example given a time series of GDP for a nation the system might be the economy of that nation. The state is the current configuration of the system at a given point in time, for example the estimated GDP for the year 2008, it is usually hidden to us. The measurements are quantitative description of the state of the system, they are often noisy so they might not coincide with the state, we can think of them as observable manifestations of the state.\n\nTake Home Message\nAs in eny estimation problem, the aim of a filtering algorithm is to form estimates of the hidden state of the system given some observable information.\n\nIn order to generate our estimate we need to alternate between a prediction step and an update step.\nIn the prediction step we use a process model (in our case was the slope_model) for mathematically describing how the system evolves, this model is used for propagating the state of the system into the future. The model has an intrinsic amount of unknown process error that might be caused by imperfect measurements or inadequate process model (we must remember that all models are wrong!).\n\nTake Home Message\nA filtering algorithm is as good as the mathematical model used for expressing our beliefs about the system dynamics.\n\nIn the update step we update the propagated state with the information derived from the measurement.\nWe can have a better understanding of the various steps by looking at a graphical representation of the algorithm\n\n\n\n\nflowchart LR    \n   predict(((Predict Step)))\n   update(((Update Step)))\n   estimate((State Estimate\\nt1))\n   prediction((State Prediction\\nt1))\n   init((Initialisation\\nt0))\n   measurement((Measurement\\nt1))\n\n   init --&gt; predict;\n   predict ----&gt; prediction;\n   prediction ----&gt; update;\n   measurement --&gt; update;\n   update ----&gt; estimate;\n   estimate --&gt; predict\n\n\n\n\n\nWe will now re-write the adaptive_gain_model into the gh_filter by making the steps outlined in the diagram above a little bit more explicit\n\n@jit\ndef gh_filter(measurements, starting_state, starting_gain, gain_factor, gain_update_frequency, state_factor):\n    \"\"\"Create the gh filter model. It will iteratively estimate \n    the state underlying the observed measurements and compute the gain adaptively based on the mismatch between prediction and measurement. \n    The function will be JIT compiled for increasing execution time.\n\n    Args:\n        - measurements (np.array): observed measurements\n        - starting_estimate (float): the starting point \n        for the estimate.\n         - starting_gain (float): the starting point \n        for the gain.       \n        - gain_factor (float): factor used for adaptively update the gain.\n        - gain_update_frequency (float): time steps required for having a full update \n        of the gain.\n        - state_factor (float): scale applied to weight the measurements.\n    \n    Returns:\n        - estimated_states(np.array): estimated states underlying the measurements.\n    \"\"\"\n    @jit\n    def predict(state_estimate, gain, gain_update_frequency):\n        \"\"\"Perform the predict step of the algorithm. Gain will not really be\n        predicted.\n        \"\"\"\n        state_predicted = state_estimate + gain * gain_update_frequency\n        gain_predicted = gain\n        return state_predicted, gain_predicted\n\n    def update(state_predicted, gain_predicted, state_factor, gain_factor, gain_update_frequency, residual):\n        \"\"\"Perform the update step of the algorithm.\n        \"\"\"\n        state_updated = state_predicted + state_factor * (residual)\n        gain_updated = gain_predicted + gain_factor * (residual / gain_update_frequency)\n        return state_updated, gain_updated\n\n    @jit\n    def step(carry, measurement):\n        \"\"\"Step function for computing the estimate, this will be iterated\n        over all the measurements. The arguments required for estimating the \n        state will be provided by the carry.\n        \"\"\"\n        (\n            previous_state, \n            previous_gain, \n            gain_update_frequency, \n            gain_factor, \n            state_factor\n         ) = carry\n        \n        state_predicted, gain_predicted = predict(\n            state_estimate=previous_state, \n            gain=previous_gain, \n            gain_update_frequency=gain_update_frequency\n        )\n\n        residual = measurement - state_predicted\n\n        state_updated, gain_updated = update(\n            state_predicted=state_predicted, \n            gain_predicted=gain_predicted, \n            state_factor=state_factor, \n            gain_factor=gain_factor, \n            gain_update_frequency=gain_update_frequency, \n            residual=residual\n        )\n \n        new_carry = (\n            state_updated, \n            gain_updated, \n            gain_update_frequency, \n            gain_factor, \n            state_factor\n        )\n        output = jnp.array([state_updated, state_predicted, gain_updated])\n\n        return new_carry, output\n\n    _, output = scan(\n        step,\n        (\n            starting_state, \n            starting_gain, \n            gain_update_frequency, \n            gain_factor, \n            state_factor\n        ),\n        measurements\n    )\n\n    states_estimated = output[:, 0]\n    states_predicted = output[:, 1]\n    gains_estimated = output[:, 2]\n\n    return gains_estimated, states_estimated, states_predicted\n\nas we can the the algorithm is exactly the same, we just made each step be represented by an appropriate closure (this is needed for making JAX happy). So how do we pick and optimal value of state_factor and gain_factor? Well this is problem dependent but it can be informed by the effect that these two parameters have on the estimate\n\nstate_factor (g): as this parameter gets larger we will tend to more closely follow (i.e. we will weight more) the measurement. A value too high will lead us to over-fit to the noise while a value too low will force us to ignore important information about changes in the system dynamics.\ngain_factor (h): as this parameter gets larger we will tend to react more quickly to sudden changes in the dynamics underlying our measurements. A value too high might cause to over-react producing spurious high-frequency spikes in the estimate while a value too low might cause the signal to adapt very slowly to sudden changes in the measurements (i.e., the estimate will take a long time to correct itself)."
  },
  {
    "objectID": "posts/kalman_bayesian_filters/gh_filter/index.html#examples",
    "href": "posts/kalman_bayesian_filters/gh_filter/index.html#examples",
    "title": "Chapter 1 - The g-h Filter",
    "section": "2.5 Examples",
    "text": "2.5 Examples\nLet’s try to get a visual intuition of the effect of varying state_factor and gain_factor in different situations.\n\n\nShow supplementary code\ndef plot_estimated_states(measurements, measurement_noise, estimated_states_dict, ax):\n    color_mapper = matplotlib.cm.get_cmap('Dark2')\n    ax = time_series_plots(\n        measurements=measurements,\n        expected_noise=measurement_noise,\n        dense_grid=False,\n        current_ax=ax,\n        scatter_kwargs={\n            \"alpha\": 0.2,\n            \"s\": 2\n        },\n        error_kwargs={\n            \"alpha\": 0.2\n        },       \n    )\n    for index, (model_name, estimated_states) in enumerate(estimated_states_dict.items()):\n\n        ax.plot(\n            np.arange(len(measurements)),\n            estimated_states,\n            c=color_mapper(index),\n            linestyle=\"--\",\n            label=f\"Estimated States {model_name}\",\n        )\n    ax.legend()\n    return ax\n\ndef run_example(measurements, gain_factors, state_factors, measurement_noise=5):\n    estimated_states_dict = {}\n    for (gain_factor, state_factor) in list(product(gain_factors, state_factors)):\n\n        gains, estimated_states, predictions = gh_filter(\n            measurements=measurements, \n            starting_state=5., \n            starting_gain=1., \n            gain_factor=gain_factor, \n            state_factor=state_factor,\n            gain_update_frequency=1\n\n        )\n        estimated_states_dict[f\"g: {state_factor} h: {gain_factor}\"] = estimated_states\n\n    fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n    ax = plot_estimated_states(\n        measurements=measurements, \n        measurement_noise=measurement_noise, \n        estimated_states_dict=estimated_states_dict, \n        ax=ax\n    )\n\n    return fig, ax\n\n\n\n2.5.1 Example 1 - Derivative Change\n\nmeasurements = (0 + jnp.arange(100) * 1.3) + random.normal(shape=(100,), key=random.PRNGKey(666)) * 5.\nmeasurements = jnp.clip(measurements, None, 100)\n\nfig, ax = run_example(\n    measurements=measurements, \n    gain_factors=[.3, .6], \n    state_factors=[.3, .6]\n)\n\n/var/folders/5w/nz0br6dn399_04hd3q55ljth0000gq/T/ipykernel_39848/3276693041.py:2: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n  color_mapper = matplotlib.cm.get_cmap('Dark2')\n\n\n\n\n\n\n\n2.5.2 Example 2 - Level Shift\n\nmeasurements = (0 + jnp.arange(50) * 1.03) + random.normal(shape=(50,), key=random.PRNGKey(666)) * 5.\nmeasurements = jnp.hstack(\n    (\n        measurements, \n        jnp.repeat(measurements[-1] * 3, 25)  + random.normal(shape=(25,), key=random.PRNGKey(666)) * 5.\n    )\n)\nmeasurements = jnp.hstack(\n    (\n        measurements, \n        jnp.repeat(measurements[-1] / 3.5, 25) + random.normal(shape=(25,), key=random.PRNGKey(666)) * 5.\n    )\n) \n\nfig, ax = run_example(\n    measurements=measurements, \n    gain_factors=[.3, .6], \n    state_factors=[.3, .6]\n)\n\n/var/folders/5w/nz0br6dn399_04hd3q55ljth0000gq/T/ipykernel_39848/3276693041.py:2: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n  color_mapper = matplotlib.cm.get_cmap('Dark2')\n\n\n\n\n\n\n\n2.5.3 Example 3 - Inadequate System Model (Dynamics with acceleration)\n\ntime = jnp.arange(5)\nmeasurements = 0 + (time * 1.03) + (jnp.power(time, 2) * 6) + random.normal(shape=(5,), key=random.PRNGKey(666)) * 5.\n\n\nfig, ax = run_example(\n    measurements=measurements, \n    gain_factors=[.3, .6], \n    state_factors=[.3, .6]\n)\n\n/var/folders/5w/nz0br6dn399_04hd3q55ljth0000gq/T/ipykernel_39848/3276693041.py:2: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n  color_mapper = matplotlib.cm.get_cmap('Dark2')\n\n\n\n\n\n\n\n2.5.4 Example 4 - Extreme Noise\n\nmeasurements = (0 + jnp.arange(100) * 1.3) + random.normal(shape=(100,), key=random.PRNGKey(666)) * 50.\n\nfig, ax = run_example(\n    measurements=measurements, \n    gain_factors=[.3, .6], \n    state_factors=[.3, .6],\n    measurement_noise=50\n)\n\n/var/folders/5w/nz0br6dn399_04hd3q55ljth0000gq/T/ipykernel_39848/3276693041.py:2: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n  color_mapper = matplotlib.cm.get_cmap('Dark2')"
  },
  {
    "objectID": "posts/jax_tutorial/1_jax_basics/index.html",
    "href": "posts/jax_tutorial/1_jax_basics/index.html",
    "title": "1 - The Basic Building Blocks",
    "section": "",
    "text": "Show supplementary code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef visualize_univariate_time_series(\n    time_series, ax=None, figsize=(8, 4), **plot_kwargs\n):\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    ax.plot(\n        np.arange(len(time_series)), \n        time_series, \n        **plot_kwargs\n    )\n\n    ax.tick_params(direction=\"in\", top=True, axis=\"x\", rotation=45)\n    ax.grid(\n        visible=True, \n        which=\"major\", \n        axis=\"x\", \n        color=\"k\", \n        alpha=0.25, \n        linestyle=\"--\"\n    )\n    return ax\n\n\n\n1 JAX functions unraveled\nIn order to leverage the speedup granted by the XLA compiler JAX first needs to transform python code in a set of lower-lever and strictier set of instructions: a jax expression. Let’ s see how this expression would look like\n\nimport jax \nimport jax.numpy as jnp\n\ndef my_foo(x):\n    x_1 = x\n    x_2 = jnp.square(x)\n    x_3 = jnp.power(x, 3)\n    out = x_1 + x_2 + x_3\n    return out\n\nprint(jax.make_jaxpr(my_foo)(3.0))\n\n{ lambda ; a:f32[]. let\n    b:f32[] = integer_pow[y=2] a\n    c:f32[] = integer_pow[y=3] a\n    d:f32[] = add a b\n    e:f32[] = add d c\n  in (e,) }\n\n\nas we can see our variables have now an explicit type (f32) and the functions jnp.square and jnp.power have been replaced by the lax equivalent integer_pow. lax can be thought as a sort of JAX low-level back-end. This expression will be then sent to the XLA compiler for being transformed in efficient machine code.\nBut what happens to our jax expression if we try to poke one of JAX sharp edges? Let’s introduce a side effect in our function and see\n\nimport jax \nimport jax.numpy as jnp\n\naccumulator = [] # global variable\n\ndef my_foo_with_side_effect(x):\n    x_1 = x\n    x_2 = jnp.square(x)\n    x_3 = jnp.power(x, 3)\n    out = x_1 + x_2 + x_3\n    accumulator.append(jnp.power(out, 2)) # side effect\n    return out\n\nprint(jax.make_jaxpr(my_foo_with_side_effect)(3.0))\n\n{ lambda ; a:f32[]. let\n    b:f32[] = integer_pow[y=2] a\n    c:f32[] = integer_pow[y=3] a\n    d:f32[] = add a b\n    e:f32[] = add d c\n    _:f32[] = integer_pow[y=2] e\n  in (e,) }\n\n\nas we can see our accumulator variable is not tracked in the jax expression neither are its associated computations! They will not be tracked by the compiler nor retrieved when the cached version of foo is executed a second time.\nWe know that most JAX functionalities (e.g. computing gradients) are achieved by applying the appropriated transformations to the functions of interest. These transformed functions will go down the same route of being translated into the relevant jax expression and compiled by XLA. Let’s unravel what the gradient of foo would look like\n\nfrom jax import grad\n\ngrad_my_foo = grad(my_foo) # we derive the gradient function of my_foo\n\nprint(jax.make_jaxpr(grad_my_foo)(3.0))\n\n{ lambda ; a:f32[]. let\n    b:f32[] = integer_pow[y=2] a\n    c:f32[] = integer_pow[y=1] a\n    d:f32[] = mul 2.0 c\n    e:f32[] = integer_pow[y=3] a\n    f:f32[] = integer_pow[y=2] a\n    g:f32[] = mul 3.0 f\n    h:f32[] = add a b\n    _:f32[] = add h e\n    i:f32[] = mul 1.0 g\n    j:f32[] = add_any i 1.0\n    k:f32[] = mul 1.0 d\n    l:f32[] = add_any j k\n  in (l,) }\n\n\nWe can see how the derived jax expression now tracks all the required computations for computing the gradient pf my_foo with respect to its input x. We will expand more on the grad function later on this post. Let’s move now to another very important feature of JAX: random number generation.\n\n\n2 ARRRRGH!!!! explicit PRNG states\nGenerate random numbers using numpy it is a relatively straightforward matter:\n\nx_1 = np.random.normal() # numpy is both easy...\nprint(x_1)\n\nx_2 = np.random.normal() # ...and intuitive!\nprint(x_1 == x_2)\n\n1.1358003380123887\nFalse\n\n\nunder the hood numpy will do alot of wizardry for us implicitly setting the state of the Pseudo-Random Number Generator (PRNG) every time we ask to simulate sampling from a given distribution (e.g. uniform, normal etc…). With JAX we must do a bit more work as the library requires to explicitly pass a state whenever we call the PRNG\n\nfrom jax import random\n\n# jax is less easy\nmaster_key = random.PRNGKey(666) # set the state with a seed\nprint(f\"Key {master_key}\")\nrandom.normal(key=master_key) # sample from the standard normal\n\nKey [  0 666]\n\n\nArray(0.19690295, dtype=float32)\n\n\nthis implies that the numbers are deterministically generated at random given a certain state. Hence if we do not make sure to generate fresh new states whenever we require a new random behaviour we might incur in some rather nasty side effects\n\naccumulator = 0\nfor _ in range(100):\n\n    x_1 = random.normal(key=master_key) # generate two numbers using the same state\n    x_2 = random.normal(key=master_key)\n    accumulator += int(x_1 == x_2)\n\nprint(accumulator)\n\n100\n\n\nWithout modifying the state, calling random.normal will always generate the exact same sequence of random numbers! What we need to do in this case is to leverage the split function in the random module for splitting the original state (or key) in one or more sub-states (or sub-keys)\n\naccumulator = 0\nseed_key = master_key\nfor _ in range(100):\n\n    seed_key, consume_key_1, consume_key_2 = random.split(seed_key, 3) # one key always left for generation\n    x_1 = random.normal(key=consume_key_1) # generate two numbers using different states\n    x_2 = random.normal(key=consume_key_2)\n    accumulator += int(x_1 == x_2)\n\nprint(accumulator)\n\n0\n\n\ndespite this behaviour might look as a big annoyance at first, it offers us a greater degree on control of when and where we want to see randomness.\n\n\n3 Just In Time Compilation\nOne of the advantages of JAX is its ability to Just In Time (JIT) compile python code to different types of accelerating devices, be them CPU, GPU or TPU. By compiling and caching slow python code to optimized machine code. So let’s see a simple example\n\ndef silly_expensive_loopy_function(x):\n    \"\"\"A silly function, it does many completely useless computations.\n    However it is very loopy and expensive.\n    \n    Args:\n        x  (float): starting point of the silly function\n        \n    Returns:\n        x (float): output of the silly function\n    \"\"\"\n    for i in range(10):\n        for j in range(10):    \n            for k in range(10): \n                x += i + j + k + i*j + j*k + j**2 + k**2\n    return x\n\nprint(\"Pure Python\")\n%timeit silly_expensive_loopy_function(10.)\n\nPure Python\n238 µs ± 9.18 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nas we can see we have a quite hefty execution time, but what happens if we JIT compile our function through the relevant JAX transformation?\n\nfrom jax import jit\n\njitted_silly_expensive_loopy_function = jit(silly_expensive_loopy_function)\n\nprint(\"Jitted Python with Compilation Time\")\n%timeit jitted_silly_expensive_loopy_function(10.).block_until_ready()\n\nJitted Python with Compilation Time\n135 µs ± 48.8 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nas we can see execution time is almost 2 orders of magnitude lower for the JIT compiled function. The function of .block_until_ready() is to time not just compilation but also computation.\nIf compilation time can be avoided, through caching for example, we can achieve even further speed-up. This because once a piece of potentially slow python code is compiled and cached by JAX, it can be skipped altogether for subsequent computations.\n\njitted_silly_expensive_loopy_function(10.).block_until_ready()\n\nprint(\"Jitted Python without Compilation Time\")\n%timeit jitted_silly_expensive_loopy_function(666.).block_until_ready()\n\nJitted Python without Compilation Time\n168 µs ± 9.28 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\n4 Looping vs Scanning\nJIT compilation is a great way for optimizing our potentially slow python code, however it comes with few gotchas and looping is one of them. If we have a function executing some relatively demanding computations over a long sequence\n\nstart_x_est=160.\ndx=1.\nh=1./10.\ng=.5/3.\ndt=1.\n\nX = 160. + (jnp.arange(1, 51) * 1.) + random.normal(shape=(50, ), key=master_key)\n\n@jit\ndef step(carry, x):\n    \"\"\" Step function for the g-h filter\n\n    Args:\n        carry (tupler): values to be carried over\n        x (float): data\n\n    Returns:\n        carry (tuple): updated components\n        x_est_update (float): updated estimate for the state\n    \"\"\"\n    previous_x_est, dx, h, g, dt = carry # h, g, and dt are fixed parameters\n    \n    x_pred = previous_x_est + (dx * dt) # system dynamics\n\n    residual = x - x_pred\n    dx = dx + h * (residual) / dt\n    updated_x_est = x_pred + g * residual\n    \n    return (updated_x_est, dx, h, g, dt ), updated_x_est\n\nfor example, the above function illustrates the computations used by a gh-filter. This function is supposed to\n\nStep over a signal\nGenerate an estimate of the state underlying the signal. This is given by a clever combination of the previous estimate and the current observed signal.\nFinally, provide the current estimate along with other relative parameters needed in the next step.\n\nIn this case, the most straightforward way to move the step function over the signal would be using a for loop\n\n@jit\ndef loopy_function(X, start_x_est, dx, h, g, dt=1.):\n    \"\"\"Gh filter logic implmented with for loop\n\n    Args:\n        X (Device Array): Input data\n        start_x_est (float): Start values for the estimated state\n        dx (float): Rate of change in the system dynamics\n        h (float): Update value\n        g (float): Gain value\n        dt (float): Frequency \n\n    Returns:\n        output (Device Array): Estimate state value\n    \"\"\"\n    output = []\n    carry = (start_x_est, dx, h, g, dt)\n    for x in X:\n        \n        carry, yhat = step(carry=carry, x=x)\n        output.append(yhat)\n    \n    return jnp.array(output)\n\nprint(\"Jitted for loop\")\n%timeit loopy_function(X=X, start_x_est=start_x_est, dx=dx, h=h, g=g, dt=dt).block_until_ready()\n\nJitted for loop\n295 µs ± 27.4 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nJIT compiling both our function gives us a convenient speedup as we have seen before. But what happens if we increase the length of the signal over which we want to step?\n\nX = 160. + (jnp.arange(1, 101) * 1.) + random.normal(shape=(100, ), key=master_key)\n\nprint(\"Jitted for loop over a long sequence\")\n%timeit loopy_function(X=X, start_x_est=start_x_est, dx=dx, h=h, g=g, dt=dt).block_until_ready()\n\nJitted for loop over a long sequence\n469 µs ± 82.7 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nthat is a considerable increase in computational time which doesn’t seem to increase linearly with the number of steps in our signal. What is happening under the hood is that XLA has to unroll all the computations included in our for loop in order to compile them, would our signal be much longer we would wait until the end of times for the compiler to do its job.\nThankfully, JAX offers a solution to this through its the lower-level API lax using scan.\n\nfrom jax.lax import scan\n\n@jit\ndef scan_function(X, start_x_est, dx, h, g, dt=1.):\n    \"\"\"Gh filter logic implemented with lax scan\n\n    Args:\n        X (Device Array): Input data\n        start_x_est (float): Start values for the estimated state\n        dx (float): Rate of change in the system dynamics\n        h (float): Update value\n        g (float): Gain value\n        dt (float): Frequency \n\n    Returns:\n        output (Device Array): Estimate state value\n    \"\"\"\n    carry, output= scan(\n        step, # this function is going to be moved along the input series,\n        (start_x_est, dx, h, g, dt), # these are the initial values of the carry,\n        X # this is the series over which step is moved\n    )\n    return output\n\nthe syntax of scan might looks a bit un-intuitive if you are used to for loops but it is actually quite simple. It will iterate our step function over all the values of X and compute both the output relative to the current value of X and a carry. As the name suggests the carry will carry over any information that might be required by step in the future, be those parameters or computed values. In our example the carry is made of fixed parameters and state variables computed inside step. But let’s look at a quick perfromance benchmark now\n\n\nShow supplementary code\nscan_timings = []\nloop_timings = []\nfor length in [10, 20, 40, 80, 100]:\n\n    X = (\n        160. + \n        (jnp.arange(1, length+1) * 1.) + \n        (random.normal(shape=(length, ), key=master_key) * 10)\n    )\n\n    kwargs = {\n        \"X\": X,\n        \"start_x_est\": start_x_est, \n        \"dx\": dx, \n        \"h\": h, \n        \"g\": g, \n        \"dt\": dt\n    }\n\n    loopy_result = %timeit -o -n300 loopy_function(**kwargs).block_until_ready()\n    scan_result = %timeit -o -n300 scan_function(**kwargs).block_until_ready()\n\n    scan_timings.append(\n        scan_result.best\n    )\n    loop_timings.append(\n        loopy_result.best\n    )\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 3))\nax.plot(\n    [10, 20, 40, 80, 100],\n    loop_timings,\n    label=\"For Loop\"\n)\nax.plot(\n    [10, 20, 40, 80, 100],\n    scan_timings,\n    label=\"Scan\"\n)\nax.legend()\nax.set_xlabel(\"Input Legth\")\nax.set_ylabel(\"Compilation + Execution Time\\nSeconds\")\nplt.show()\n\n\n392 µs ± 60.2 µs per loop (mean ± std. dev. of 7 runs, 300 loops each)\n576 µs ± 184 µs per loop (mean ± std. dev. of 7 runs, 300 loops each)\n569 µs ± 296 µs per loop (mean ± std. dev. of 7 runs, 300 loops each)\n624 µs ± 94 µs per loop (mean ± std. dev. of 7 runs, 300 loops each)\nThe slowest run took 19.54 times longer than the fastest. This could mean that an intermediate result is being cached.\n1.27 ms ± 2.12 ms per loop (mean ± std. dev. of 7 runs, 300 loops each)\n729 µs ± 164 µs per loop (mean ± std. dev. of 7 runs, 300 loops each)\nThe slowest run took 43.21 times longer than the fastest. This could mean that an intermediate result is being cached.\n2.52 ms ± 5.06 ms per loop (mean ± std. dev. of 7 runs, 300 loops each)\n922 µs ± 132 µs per loop (mean ± std. dev. of 7 runs, 300 loops each)\n516 µs ± 31.1 µs per loop (mean ± std. dev. of 7 runs, 300 loops each)\n978 µs ± 111 µs per loop (mean ± std. dev. of 7 runs, 300 loops each)\n\n\n\n\n\nas you can see the difference is negligible for short sequences (scan might even require more time!) but increases massively for longer sequences. Lets’s look at the result of our scanned function now\n\n\nShow supplementary code\nyhat = scan_function(\n    X=X, \n    start_x_est=start_x_est, \n    dx=dx, \n    h=h, \n    g=g, \n    dt=dt\n)\n\nax = visualize_univariate_time_series(\n    time_series=X,\n    label=\"Data\"\n)\n\nax = visualize_univariate_time_series(\n    time_series=yhat,\n    ax=ax,\n    label=\"State Estimate\"\n)\nax.plot(\n    np.arange(1, 101),\n    160. + (jnp.arange(1, 101) * 1.),\n    linestyle=\"--\",\n    c=\"k\",\n    alpha=0.5,\n    label=\"System\"\n)\nax.legend()\nax.set_xlabel(\"Time\")\nax.set_ylabel(\"Value\")\nplt.show()\n\n\n\n\n\nall seems to be in order and executed in record time!\n\n\n5 Computing Gradients\nLet’s come now to the other central transformation offered by JAX: grad. With grad we can transform python functions in gradient functions, what do we mean by this? Let’s take the square function as an example\n\ndef square(x):\n    \"\"\"Return the square of x\n    \n    Args:\n        x  (float): values to be squared\n        \n    Returns:\n        ssq_ (float): square of x\n    \"\"\"\n    sq_ = x ** 2\n    return sq_\n\ndx = grad(square)\n\nby passing this function to the grad transformation, we can obtain a new function that will evaluate the gradient of x with respect to sq_ for us, pretty convenient.\n\n\nShow supplementary code\nfig, axs = plt.subplots(1, 2, figsize=(8, 4), sharex=True)\nfor x in np.linspace(-10, 10, 100):\n\n    axs[0].scatter(\n        x, \n        square(x),\n        s=1,\n        c=\"k\"\n    )\n    axs[1].scatter(\n        x, \n        dx(x),\n        s=1,\n        c=\"r\"\n    )\n\nfor ax in axs:\n    ax.set_xlabel(\"x\")\n\naxs[0].set_ylabel(\"$x^2$\")\naxs[1].set_ylabel(\"$\\dfrac{\\partial f(x)}{\\partial x}$\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nObtaining the derivative of a single variable function however is not that impressive, scipy.optimize.approx_fprime can easily achieve the same result although with a slightly more laborious approach. So is the advantage of grad only to provide some syntactic sugar? Well, not really.\n\nDifferently from scipy.optimize.approx_fprime which relies on finite difference for approximating gradients, grad leverages automatic differentiation for obtaining more numerically stable results.\nThe use of automatic differentiation allows us to compute gradients of very complex and composite functions.\nThe syntactic sugar capabilities of grad allow us to customize which gradients we are interested to and to compute them with respect to many different data structures.\n\nLet’s look at the two variables function sum_of_squares\n\ndef sum_of_squares(x_1, x_2):\n    \"\"\"Return the sum of squares of x_1 and x_2\n    \n    Args:\n        x_1  (float): first variable to be squared\n        x_2  (float): second variable to be squared\n        \n    Returns:\n        ssq_ (float): square of x\n    \"\"\"\n    ssq_ = jnp.square(x_1) + jnp.square(x_2)\n    return ssq_\n\ndxx = grad(sum_of_squares, argnums=[0, 1])\n\nIn this case sum_of_squares takes two variables as inputs so we have to specify for which one we want to compute the partial derivative, we do that using the argnums argument. Let’s see at the results produced by dxx\n\n\nShow supplementary code\nssq_results = []\ngrads_x_1 = []\ngrads_x_2 = []\nspace = np.linspace(-10, 10, 10)\nfig, axs = plt.subplots(1, 2, figsize=(8, 4))\n\nfor x_1 in space:\n\n    for x_2 in space:\n\n        grads = dxx(x_1, x_2) # gradients come as a tuple of device arrays\n\n        grads_x_1.append(grads[0])\n        grads_x_2.append(grads[1])\n\n        ssq_results.append(sum_of_squares(x_1, x_2))\n\nx, y = np.meshgrid(\n    space, \n    space\n)\n\naxs[0].scatter(\n    x.flatten(),\n    y.flatten(), \n    c=ssq_results,\n    cmap=\"viridis\"\n)\naxs[1].scatter(\n    grads_x_1, \n    grads_x_2, \n    c=ssq_results,\n    cmap=\"viridis\"\n)\n\naxs[0].set_xlabel(\"$x_1$\")\naxs[0].set_ylabel(\"$x_2$\")\n\naxs[1].set_xlabel(\"$\\dfrac{\\partial f(x_1, x_2)}{\\partial x_1}$\")\naxs[1].set_ylabel(\"$\\dfrac{\\partial f(x_1, x_2)}{\\partial x_2}$\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nthe best way to describe grad is to consider it as a tranformation able to compute gradients with respect to almost any type of data structure. The only requirement is that such data structure is in the form of a Pytree. A Pytree is a “…a tree-like structure built out of container-like Python objects…”, it usually have the following form\npytree\n|\n|_node_1\n|      |_leaf_1.1\n|      |_leaf_1.2\n|\n|_node_2\n|      |_leaf_2.1\n|      |_leaf_2.2\n|      |_node_2.1\n|               |_leaf_2.1.1\n|               |_...\n|\n|_...                \neach node and leaf can by default be any python data structures among lists, tuples and dicts however JAX allows to register others as valid pytree. The tree-like structure offers a great deal of flexibility for specifying things like the parameters of a model. Let’s see a concrete example with a linear regression\n\nfrom jax import value_and_grad\n\nX = random.normal(key=master_key, shape=(1000, 40))\ny = random.normal(key=master_key, shape=(1000,))\n\n# a dictionary as a pytree with 2 nodes and 41 leaves\nmy_parameters = {\n    \"alpha\": random.normal(key=master_key, shape=(1,)), \n    \"beta\": random.normal(key=master_key, shape=(X.shape[1],))\n}\n\nonce we have defined our parameters, we can pass them to an appropriate function and let grad do its magic for deriving the gradient.\n\n@jit\ndef sum_of_squared_errors(y, yhat):\n    \"\"\"Return the square of x\n    \n    Args:\n        y  (DeviceArray): ground truth values\n        yhat  (DeviceArray): model predictions\n        \n    Returns:\n        ssq (float): sum of the squares of the difference between y and yhat.\n    \"\"\"\n    return jnp.sum(jnp.square(y - yhat))\n\n@jit\ndef linear_regression_loss(X, y, parameters):\n    \"\"\"Compute the loss,  sum_of_squared_errors, for a linear regression model.\n\n    Args:\n        X  (DeviceArray): model covariates.\n        y  (DeviceArray): ground truth values.\n        parameters (dictionary): model's parameters.\n        \n    Returns:\n        loss (float): loss for the linear regression model.\n\n    \"\"\"\n    yhat = jnp.dot(X, parameters[\"beta\"]) + parameters[\"alpha\"]\n    loss = sum_of_squared_errors(y=y, yhat=yhat)\n    return loss\n\nlinear_regression_grad = value_and_grad(\n    fun=linear_regression_loss,\n    argnums=2 # w.r.t. parameters\n)\n\nin this case we used a variation of grad named value_and_grad that returns not just the gradient but also the output of the function, which in this case is whatever comes out of sum_of_squared_errors.\nThe covenience of grad (and its variations) is that it will return the partial derivates of our parameters with respect to the output of sum_of_squared_errors keeping the same pytree structure!\n\nsse, gradients = linear_regression_grad(X, y, my_parameters)\n\ngrad_alpha = gradients['alpha']\ngrad_beta = gradients['beta']\n\nprint(f\"SSE: {sse}\")\nprint(f\"Partial Derivative alpha: {grad_alpha}\")\nprint(f\"Partial Derivatives beta: {grad_beta}\")\n\nSSE: 32192.728515625\nPartial Derivative alpha: [241.85338]\nPartial Derivatives beta: [  219.33125    763.7705     -37.764587  1821.604    -2178.1812\n  1445.4122   -1254.5923    -502.70135   -186.74942    906.9597\n  2448.1733    -184.66875   1934.4185   -2894.3225    1366.3875\n   184.45464   2376.773    -1922.4895    -804.0646   -1066.2643\n -2347.2046   -1965.3784    1555.4048   -2364.5405   -1658.9954\n  2206.7283   -1709.1989    1556.511     1021.581     1998.0963\n  3664.0103    -440.48767   3889.8447     807.2294    -665.2969\n   133.03035  -4179.5854     915.6873    -391.67392  -1247.0728  ]\n\n\nThis behaviour is extensible to virtually any python object as far as it is registered as a pytree. So how can we register a python object as a pytree? Let’s take a NamedTuple as an example\n\nfrom collections import namedtuple\n\nLinearRegressionParameters = namedtuple(\n    \"LinearRegressionParameters\", \n    [\"alpha\", \"beta\"]\n)\n\nwhen we register a pytree we have to tell JAX how to unpack the leaves into an iterable and pack them back in the original tree structure\n\nfrom jax.tree_util import register_pytree_node\n\nregister_pytree_node(\n    LinearRegressionParameters,\n    lambda xs: (tuple(xs), None),  # tell JAX how to unpack to an iterable\n    lambda _, xs: LinearRegressionParameters(*xs)       # tell JAX how to pack back into LinearRegressionPArameters\n)\n\nwe now just need to modify our linear_regression_loss function slightly in order to use the LinearRegressionParameters instead of a dictionary.\n\n@jit\ndef linear_regression_loss(X, y, parameters):\n    \"\"\"Compute the loss,  sum_of_squared_errors, for a linear regression model.\n\n    Args:\n        X  (DeviceArray): model covariates.\n        y  (DeviceArray): ground truth values.\n        parameters (NamedTuple): model's parameters.\n        \n    Returns:\n        loass (float): loss for the linear regression model.\n    \"\"\"\n    yhat = jnp.dot(X, parameters.beta) + parameters.alpha\n    loss = sum_of_squared_errors(y=y, yhat=yhat)\n    return loss\n\nlinear_regression_grad = value_and_grad(\n    fun=linear_regression_loss,\n    argnums=2\n)\n\nmy_parameters = LinearRegressionParameters(\n    random.normal(key=master_key, shape=(1,)),\n    random.normal(key=master_key, shape=(X.shape[1],))\n)\n\nsse, gradients = linear_regression_grad(X, y, my_parameters)\n\ngrad_alpha = gradients.alpha\ngrad_beta = gradients.beta\n\nprint(f\"SSE: {sse}\")\nprint(f\"Partial Derivative alpha: {grad_alpha}\")\nprint(f\"Partial Derivatives beta: {grad_beta}\")\n\nSSE: 32192.728515625\nPartial Derivative alpha: [241.85338]\nPartial Derivatives beta: [  219.33125    763.7705     -37.764587  1821.604    -2178.1812\n  1445.4122   -1254.5923    -502.70135   -186.74942    906.9597\n  2448.1733    -184.66875   1934.4185   -2894.3225    1366.3875\n   184.45464   2376.773    -1922.4895    -804.0646   -1066.2643\n -2347.2046   -1965.3784    1555.4048   -2364.5405   -1658.9954\n  2206.7283   -1709.1989    1556.511     1021.581     1998.0963\n  3664.0103    -440.48767   3889.8447     807.2294    -665.2969\n   133.03035  -4179.5854     915.6873    -391.67392  -1247.0728  ]\n\n\nHere we conclude this first introductory post on the basics of JAX. We want to stress that this is just a small selection of the features offered by JAX.\nWe can think of it as a distillation of some of the contents reported in the JAX online documentation\nThat said, what we outlined so far should equip us with enough knowledge to develop some simple models in the next posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bazaar",
    "section": "",
    "text": "About me\n\nI’m Valerio Bonometti, a senior data scientist with experience in the retail Tesco and renewable energy sectors at LightsourceBP.\nI completed an industrial PhD in computer science at the University of York while being based full-time with Square Enix data science team for 4 years. My PhD thesis attempted to bridge neuroscintific theories of motivation with machine learning approaches for its estimation. The focus was on designing, developing and prototyping artificial neural network models for estimating the motivational state of players and predicting their engagement simulatenously across a wide range of games.\nMy educational background is in (clinical) experimental psychology and neuroscience and I have a keen interest in applied statistics, machine learning and science in general.\n\n\n\nAbout this website\n\nThis website is an attempt to organize the small and heterogeneous projects I develop in my free time as part of my self development. It is a way for guiding and self-disciplining my learning journey."
  },
  {
    "objectID": "jax_index.html",
    "href": "jax_index.html",
    "title": "Basic Introductory Tutorial to JAX",
    "section": "",
    "text": "1 - The Basic Building Blocks\n\n\n\nJAX\n\n\nTutorial\n\n\nbasics\n\n\n\nThis post introduces some of JAX basic building blocks. It is not an exhaustive collection but will include most components used in this series of blog posts.\n\n\n\nValerio Bonometti\n\n\nAug 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n2 - Model specification and fitting\n\n\n\nJAX\n\n\nTutorial\n\n\nmodel fitting\n\n\nmodel building\n\n\n\nThis post introduces the general set-up that we will use in this tutorial for specifying models and fitting them to the data.\n\n\n\nValerio Bonometti\n\n\nAug 24, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "kalman_index.html",
    "href": "kalman_index.html",
    "title": "Kalman and Bayesian Filters",
    "section": "",
    "text": "Chapter 1 - The g-h Filter\n\n\n\nJAX\n\n\nTutorial\n\n\nKalman and Bayesian Filters\n\n\ng-h Filter\n\n\n\nThis post introduces the rudimentary of filtering with a particular focus on the g-h filter.\n\n\n\nValerio Bonometti\n\n\nAug 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2 - Discrete Bayes Filter\n\n\n\nJAX\n\n\nTutorial\n\n\nKalman and Bayesian Filters\n\n\nDiscrete Bayes Filter\n\n\nBayesian Statistics\n\n\n\nThis post illustrates how to incorporate concepts borrowed from bayesian statistics into filtering algorithms.\n\n\n\nValerio Bonometti\n\n\nAug 24, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/jax_tutorial/2_set_up/index.html",
    "href": "posts/jax_tutorial/2_set_up/index.html",
    "title": "2 - Model specification and fitting",
    "section": "",
    "text": "Show supplementary code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom jax.debug import print as jprint\nIn order to specify models in JAX we first need to figure out what are the core functionalities that we need to implement. We will focus on specific set of models that given an input \\(X\\), a target \\(y\\) and parameters \\(\\theta\\) aim to approximate functions of the form \\(f(X; \\theta) \\mapsto y\\).\nWhat we need to specify are:\nWe also need to make sure that while developing these functionalities we leverage the optimisations provided by JAX while avoiding its sharp edges."
  },
  {
    "objectID": "posts/jax_tutorial/2_set_up/index.html#parameters-container",
    "href": "posts/jax_tutorial/2_set_up/index.html#parameters-container",
    "title": "2 - Model specification and fitting",
    "section": "1.1 Parameters Container",
    "text": "1.1 Parameters Container\nThe ideal way for storing parameters would be to create an immutable data structure (e.g., the named tuple example presented in our first post), registered as a pytree node, every time we need to update our parameters.\nIn this and future posts we will adopt a much simpler although more intuitive strategy and store our parameters in a dictionary.\n\nfrom jax import numpy as jnp\n\nmy_parameters = {}\n\nThe nice thing about dictionaries is that they are:\n\nNatively supported as a pytree.\nEasy to inspect.\nNaturally support nested structures.\nEasy to update using native JAX functionalities\n\n\nmy_parameters[\"alpha\"] = 1.\n\n# more complicated structure\nmy_parameters[\"beta_1\"] = {\n    0: jnp.arange(5),\n    1: jnp.arange(5, 10),\n    2: jnp.arange(10, 15)\n}\nmy_parameters[\"beta_2\"] = {\n    0: {\n        \"beta_21\": jnp.arange(5),\n        \"beta_22\": jnp.arange(5, 10),\n        \"beta_23\": jnp.arange(10, 15)\n    },\n    1: 4.,\n    2: 5.\n}"
  },
  {
    "objectID": "posts/jax_tutorial/2_set_up/index.html#parameters-initialisation",
    "href": "posts/jax_tutorial/2_set_up/index.html#parameters-initialisation",
    "title": "2 - Model specification and fitting",
    "section": "1.2 Parameters Initialisation",
    "text": "1.2 Parameters Initialisation\nOne of the first step when fitting a model to the data is to set the starting point for the optimization process for each of the considered parameters.\nWe can achieve this by defining functions that implement specific initialisation strategies\n\ndef ones(init_state, random_key):\n    \"\"\"Initialise parameters with a vector of ones.\n\n    Args:\n        init_state (Tuple): state required to\n        initialise the parameters. For this initializer only the parameters shape is required\n\n        random_key (PRNG Key): random state used for generate the random numbers. Not used for this type of initialisation.\n\n    Returns:\n        param(DeviceArray): generated parameters\n    \"\"\"\n    params_shape, _ = init_state\n    params = jnp.ones(shape=params_shape)\n    return params\n\none of the most straightforward strategies is to initialize all the parameters with the same constant value (a one in this case).\nIn this case we our function requires an init_state tuple containing all the information necessaries for initializing the parameters and a random_key used for setting the state of the random number generator. In this case we really do not need any random behaviour but we keep the signature for keeping compatibility with other initialisation strategies.\nAnother alternative is to generate starting values according to some statistical distribution, like a gaussian for instance\n\nfrom jax.random import PRNGKey\nfrom jax import random\n\nimport seaborn as sns\n\ndef random_gaussian(init_state, random_key, sigma=0.1):\n    \"\"\"Initialize parameters with a vector of random numbers drawn from a normal distribution with mean 0 and std sigma.\n\n    Args:\n        init_state (Tuple): state required to\n        initialize the parameters. For this initializer only the parameters shape is required\n\n        random_key (PRNG Key): random state used for generate the random numbers.\n\n    Returns:\n        param(DeviceArray): generated parameters\n    \"\"\"\n    params_shape, _ = init_state\n    params = random.normal(\n        key=random_key,\n        shape=params_shape\n    ) * sigma\n    return params\n\nmaster_key = PRNGKey(666)\n\nmy_parameters = random_gaussian(\n    init_state=((100, 2), None),\n    random_key=master_key,\n    sigma=0.1\n    )\n\ngrid = sns.jointplot(\n    x=my_parameters[:, 0],\n    y=my_parameters[:, 1],\n    kind=\"kde\",\n    height=4\n)\ngrid.ax_joint.set_ylabel(\"Parameter 2\")\ngrid.ax_joint.set_xlabel(\"Parameter 1\")\nplt.show()"
  },
  {
    "objectID": "posts/jax_tutorial/2_set_up/index.html#parameters-sharing",
    "href": "posts/jax_tutorial/2_set_up/index.html#parameters-sharing",
    "title": "2 - Model specification and fitting",
    "section": "1.3 Parameters Sharing",
    "text": "1.3 Parameters Sharing\nSharing the parameters at this point is better understood as part of a state manipulation process. What do we mean by this? If we were to perform parameters update within a an object oriented framework we might do something among these lines\n\nclass Model:\n    def __init__(self):\n        self._parameters = np.array([0, 0, 0])\n\n    def add(self, x):\n        self._parameters += x\n\n    def subtract(self, x):\n        self._parameters -= x\n\n    def get_parameters(self):\n        return self._parameters\n\nmodel = Model()\nmodel.add(10)\nmodel.subtract(5)\n\nprint(f\"Updated Parameters {model.get_parameters()}\")\n\nUpdated Parameters [5 5 5]\n\n\nthe parameters are part of the state of Model an get updated according to the behavior of add and subtract.\nSince in JAX we have to stick to pure functions as much as we can, a viable option is to consider parameters as a state that is passed through a chain of transformation\n\nfrom jax import jit\n\ndef parameters_init():\n    return jnp.array([0., 0., 0.])\n\n@jit\ndef add(parameters, x):\n    return parameters + x\n\n@jit\ndef subtract(parameters, x):\n    return parameters - x\n\nparameters = parameters_init()\n# parameters are passed to transformations\n# and returned modified\nparameters = add(parameters=parameters, x=10.)\nparameters = subtract(parameters=parameters, x=5.)\n\nprint(f\"Updated Parameters {parameters}\")\n\nUpdated Parameters [5. 5. 5.]\n\n\ndifferently from the previous example, here the state (i.e., parameters) is made explicit and passed as argument to the functions in charge of doing the transformations."
  },
  {
    "objectID": "posts/jax_tutorial/2_set_up/index.html#forward-computations",
    "href": "posts/jax_tutorial/2_set_up/index.html#forward-computations",
    "title": "2 - Model specification and fitting",
    "section": "2.1 Forward Computations",
    "text": "2.1 Forward Computations\nThe forward function is in charge of performing all the the “forward computations” in the model. This naming, that we borrow from the various deep learning framework around, might not be the optimal one as we won’t necessarely need or have a complementary “backward” function but it is functional to the needs of this tutorial.\nLet’s look at how the forward functiona for a simple linear model might look like\n\n@jit\ndef forward(X, current_state):\n    \"\"\"Perform the forward computations for a linear model\n\n    Args:\n        x: covaruates of the model.\n        current_state: current state of the model, containing \n        parameters and state for the pseudo random number generator.\n\n    Returns:\n        yhat: estimate of the target variable.\n    \"\"\"\n    current_params, random_state = current_state\n    beta = current_params[\"beta\"]\n    alphas = current_params[\"alphas\"]\n    yhat =  beta + jnp.dot(X, alphas)\n    return yhat\n\nHere, forward receive as input some data x and a variable we call current_state, as we see this last one it then gets unpacked in some current_parameters and a random_state. The idea here is that we are passing around the current state of the model (parameters or random number generator seed) instead of accessing it as we would with the attributes of a class.\nOnce unpacked all the necessary variables (here random state is spurious, but we could use it in case we need some stochastic behaviour from our function), the forward function simply execute all the logic necessary for mapping the input x to an estimate of our target variable yhat.\nOnce we obtain an estimate of our target variable, we should evaluate it with rispect to a given objective function, let’s see how we can do that with the compute_loss function."
  },
  {
    "objectID": "posts/jax_tutorial/2_set_up/index.html#objective-computations",
    "href": "posts/jax_tutorial/2_set_up/index.html#objective-computations",
    "title": "2 - Model specification and fitting",
    "section": "2.2 Objective Computations",
    "text": "2.2 Objective Computations\nThe computation of the objective function will require in our case four components: the parameters, the forward function, the loss function (along with any function used for computing any regularizing term) ad the input to the model.\nWe’ll see that we will pass all the components except the paramenters to the objective function using a closure and that the actual loss function need to be wrapped inside a compute_loss function.\n\nfrom jax import tree_leaves\n\nreg_strength = 0.001\n\nX = np.random.normal(size=(100, 10))\ny = np.random.normal(size=(100,))\n\nparams = {\n    \"alphas\": np.random.normal(size=(10, )),\n    \"beta\": np.random.normal(),\n}\n\n@jit\ndef root_mean_squared_error(y, yhat):\n    \"\"\"Compute the the root mean squared error between two vectors.\n\n    Args:\n        - y: ground truth value\n        - yhat: estimate of ground truth\n    \n    Returns:\n        squared_error: the squared mean error between y and y hat\n    \"\"\"\n    squared_error = jnp.square(y - yhat)\n    mean_squared_error = jnp.mean(squared_error)\n    return jnp.sqrt(mean_squared_error)\n\n@jit\ndef l1_reg_loss(params):\n    \"\"\"Compute the l1 norm of the parameters.\n\n    Args:\n        - y: ground truth value\n        - yhat: estimate of ground truth\n    \n    Returns:\n        squared_error: the squared mean error between y and y hat\n    \"\"\"\n    loss = sum([jnp.sum(jnp.abs(leave)) for leave in tree_leaves(params)])\n    return loss\n\n\ndef model(X, y, reg_strength):\n\n    @jit\n    def compute_loss(current_params):\n        \"\"\"Perform the computations required for deriving the loss given the current parameters.\n\n        This inlcudes a call to the forward function, the loss computation and the regularisation\n        loss.\n\n        Args:\n            current_params: the current state of the parameters of the model\n        \"\"\"\n        yhat = forward(X=X, current_state=(current_params, None))\n        raw_loss = root_mean_squared_error(y=y, yhat=yhat)\n        reg_loss = l1_reg_loss(params=current_params) * reg_strength\n        return raw_loss + reg_loss\n    \n    return compute_loss\n\n/tmp/ipykernel_21206/2913352642.py:1: DeprecationWarning: jax.tree_leaves is deprecated: use jax.tree_util.tree_leaves.\n  from jax import tree_leaves\n\n\nOf course there are are more intuitive ways for doing this but in this case we require this convoluted game of chinese boxes because when we apply the grad transformation we want it to give us the derivative of the objective function (root_mean_squared_error plus l1_reg_loss) with respect to the current_parameters. So in this case in order to obtain the derivative we can simple apply the relevant transformation value_and_grad to the compute_loss function\n\nfrom jax import value_and_grad\n\ncompute_loss = model(X=X, y=y, reg_strength=reg_strength)\ncompute_loss_derivative = value_and_grad(compute_loss)\n\ncompute_loss_derivative(params)\n\n(Array(2.1123009, dtype=float32),\n {'alphas': Array([-0.4701293 , -0.29670703,  0.43866345, -0.18365346,  0.36222848,\n         -0.07263832, -0.1072477 ,  0.12307326,  0.08663466,  0.2632417 ],      dtype=float32),\n  'beta': Array(0.325346, dtype=float32, weak_type=True)})\n\n\nWhat is particularly convinient in the functions defined above is the tree_leaves we use we computing the regularisation term. This utility function provided by jax allows us to traverse an antire PyTree in order to get all its leaves values, let’s see an example in action\n\ncomplex_nested_pytree = {\n    \"a\": (1, 2, 3),\n    \"b\": 4,\n    \"c\": {\n        \"c_1\": 5,\n        \"c_2\": 6,\n        \"c_3\": {\n            \"c_3_1\": 7,\n            \"c_3_2\": 8,\n            \"c_3_3\": 9\n        }\n    }\n}\n\ntree_leaves(complex_nested_pytree)\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nAs you can see, this can become a convenient way for flattenting even the most complicated tree structure (such as those we have when specifying the parameters of some complex model)."
  },
  {
    "objectID": "posts/jax_tutorial/2_set_up/index.html#backward-computations",
    "href": "posts/jax_tutorial/2_set_up/index.html#backward-computations",
    "title": "2 - Model specification and fitting",
    "section": "2.3 Backward Computations",
    "text": "2.3 Backward Computations"
  },
  {
    "objectID": "posts/kalman_bayesian_filters/discrete_bayes_filter/index.html",
    "href": "posts/kalman_bayesian_filters/discrete_bayes_filter/index.html",
    "title": "Chapter 2 - Discrete Bayes Filter",
    "section": "",
    "text": "Show supplementary code\nfrom jax import jit\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/kalman_bayesian_filters/discrete_bayes_filter/index.html#updating-beliefs",
    "href": "posts/kalman_bayesian_filters/discrete_bayes_filter/index.html#updating-beliefs",
    "title": "Chapter 2 - Discrete Bayes Filter",
    "section": "1.1 Updating Beliefs",
    "text": "1.1 Updating Beliefs\nAs we have seen before the gh filter relied consistently on the idea of generating predictions given our knowledge of the state of a system and adjusting these as soon as we had observable measurements. It turns out that bayesian statistics offers a convenient and formal framework for doing this type of belief updates. Let’s look at a concrete example\nLet’s say that we are trying to estimate the position of an agent within an environment with a certain number of obstacles.\n\nfrom jax import random\nimport jax.numpy as jnp\n\nmaster_key = random.PRNGKey(666)\n\nobservations = random.choice(key=master_key, a=jnp.array([0., 1.]), shape=(10,), p=jnp.array([.4, .6]))\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\nax.bar(\n    x=np.arange(len(observations)),\n    height=observations,\n    color=\"r\"\n)\nax.set_ylabel(\"Observation\")\nax.set_xlabel(\"Position\")\nplt.show()\n\n\n\n\nWithout receiving any measurement from the agent our belief, we will call it prior from now on, on its position is distributed equally on all the available locations\n\npriors = jnp.array([1. / len(observations)] * len(observations))\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 4))\nax.bar(\n    x=np.arange(len(observations)),\n    height=priors\n)\nax.set_title(\"Prior\")\nax.set_ylabel(\"Probability\")\nax.set_xlabel(\"Position\")\nplt.show()\n\n\n\n\nThe measurements we receive from the agent will tell us if, in its current state, it is facing an obstacle or not. We can quantify our knowledge in light of a measurement though a likelihood function\n\n@jit\ndef custom_likelihood(measurement, observations, p):\n    \"\"\"Compute the likelihood of the agent being in\n    each potion of the observations space given a measurement\n    and the probability of an obstacle\n\n    Args:\n        - measurement(int): measurement from the agent.\n        - observations(DeviceArray): possible locations of the agent.\n        - p (float): probability that the measurement is providing correct information\n\n    Returns:\n        - likelihood(DeviceArray): likelihood of the agent \n        position in the observation space.\n    \"\"\"\n    likelihood = jnp.ones(shape=(len(observations), ))\n    likelihood = jnp.where(\n        observations == measurement,\n        likelihood * (p / (1-p)),\n        likelihood\n    )\n    return likelihood\n\nlikelihood_obstacle = custom_likelihood(\n    measurement=1,\n    observations=observations,\n    p=.75\n)\nlikelihood_path = custom_likelihood(\n    measurement=0,\n    observations=observations,\n    p=.75\n)\n\nfig, axs = plt.subplots(1, 2, figsize=(6, 3), sharex=True, sharey=True)\n\naxs[0].bar(\n    x=np.arange(len(observations)),\n    height=likelihood_obstacle\n)\naxs[0].set_title(\"Likelihood Measurement\\nObstacle\")\n\naxs[1].bar(\n    x=np.arange(len(observations)),\n    height=likelihood_path\n)\naxs[1].set_title(\"Likelihood Measurement\\nPath\")\n\naxs[0].set_ylabel(\"Density\")\naxs[0].set_xlabel(\"Position\")\naxs[1].set_xlabel(\"Position\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nand subsequently update our priors in view of the likelihood derived from the measurements using bayes theorem \\(posterior \\propto likelihood \\times prior\\)\n\n\nShow supplementary code\ndef plot_posterior_derivation(priors, likelihood, posterior):\n\n    fig, axs = plt.subplots(1, 3, figsize=(9, 3), sharex=True, sharey=True)\n\n    axs[0].bar(\n        x=np.arange(len(priors)),\n        height=priors \n    )\n    axs[0].set_title(\"Priors Position\")\n\n    axs[1].bar(\n        x=np.arange(len(likelihood)),\n        height=likelihood / likelihood.sum()\n    )\n    axs[1].set_title(\"Likelihood Measurement\\nObstacle\")\n\n    axs[2].bar(\n        x=np.arange(len(posterior)),\n        height=posterior\n    )\n    axs[2].set_title(\"Posterior Position\")\n\n    axs[0].set_ylabel(\"Probability\")\n    for ax in axs:\n\n        ax.set_xlabel(\"Position\")\n\n    plt.tight_layout()\n    plt.show()\n\n\nSince our priors were not providing any information the posterior we obtained was driven exclusively byt the likelihood\n\n@jit\ndef custom_update(priors, likelihood):\n    \"\"\"Update priors about agent position given likelihood of the measurement\n\n    Args:\n        - priors(DeviceArray): priors on the agent position.\n        - likelihood(DeviceArray): likelihood of the agent position after observing a measurement.\n\n    Returns:\n        - posterior(DeviceArray): posterior probability of the agent \n        position in the observation space.\n    \"\"\"\n    posterior = priors * likelihood\n    return posterior / jnp.sum(posterior)\n\nlikelihood = custom_likelihood(\n    measurement=1,\n    observations=observations,\n    p=.75\n)\nposterior = custom_update(\n    priors=priors,\n    likelihood=likelihood\n)\n\nplot_posterior_derivation(\n    priors=priors, \n    likelihood=likelihood, \n    posterior=posterior\n)\n\n\n\n\nHowever it is easy enough to see how things change if we provide priors that are slightly more informative\n\npriors = jnp.array(\n    [\n        0.2,\n        0.4,\n        0.2,\n        0.05,\n        0.05,\n        0.02,\n        0.02,\n        0.02,\n        0.02,\n        0.02\n    ]\n)\nposterior = custom_update(\n    priors=priors,\n    likelihood=likelihood\n)\n\nplot_posterior_derivation(\n    priors=priors, \n    likelihood=likelihood, \n    posterior=posterior\n)"
  }
]